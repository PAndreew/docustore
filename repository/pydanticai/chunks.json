{
  "1c951793a24274424ece1060689a04279b2b281a16a91ebf807924f59db155c5": {
    "text": "*Agent Framework / shim to use Pydantic with LLMs*\n\nPydantic AI is a Python agent framework designed to make it less painful to build production grade\napplications with Generative AI.\n\n🎉 Pydantic AI V1 Beta Released!\n\nWe'd love your feedback as we prepare for V1:\n\n```\npip install pydantic-ai==1.0.0b1\n```\n\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev \"https://docs.pydantic.dev\").\n\nSimilarly, virtually every agent framework and LLM library in Python uses Pydantic Validation, yet when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\"), we couldn't find anything that gave us the same feeling.\n\nWe built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app development.",
    "source_url": "https://ai.pydantic.dev/",
    "header": "Introduction"
  },
  "392505fcc9a9ea61eddbac53e6054fa81323fccd1a6dc7fe2fefbeecfa46bab9": {
    "text": "* **Built by the Pydantic Team**:\n  Built by the team behind [Pydantic Validation](https://docs.pydantic.dev/latest/ \"https://docs.pydantic.dev/latest/\") (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more).\n* **Model-agnostic**:\n  Supports OpenAI, Anthropic, Gemini, Deepseek, Ollama, Groq, Cohere, and Mistral, and there is a simple interface to implement support for [other models](models/ \"models/\").\n* **Pydantic Logfire Integration**:\n  Seamlessly [integrates](logfire/ \"logfire/\") with [Pydantic Logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\") for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\n* **Type-safe**:\n  Designed to make [type checking](agents/#static-type-checking \"agents/#static-type-checking\") as powerful and informative as possible for you.\n* **Python-centric Design**:\n  Leverages Python's familiar control flow and agent composition to build your AI-driven projects, making it easy to apply standard Python best practices you'd use in any other (non-AI) project.\n* **Structured Responses**:\n  Harnesses the power of [Pydantic Validation](https://docs.pydantic.dev/latest/ \"https://docs.pydantic.dev/latest/\") to [validate and structure](output/#structured-output \"output/#structured-output\") model outputs, ensuring responses are consistent across runs.\n* **Dependency Injection System**:\n  Offers an optional [dependency injection](dependencies/ \"dependencies/\") system to provide data and services to your agent's [system prompts](agents/#system-prompts \"agents/#system-prompts\"), [tools](tools/ \"tools/\") and [output validators](output/#output-validator-functions \"output/#output-validator-functions\").\n  This is useful for testing and eval-driven iterative development.\n* **Streamed Responses**:\n  Provides the ability to [stream](output/#streamed-results \"output/#streamed-results\") LLM responses continuously, with immediate validation, ensuring real time access to validated outputs.\n* **Graph Support**:\n  [Pydantic Graph](graph/ \"graph/\") provides a powerful way to define graphs using typing hints, this is useful in complex applications where standard control flow can degrade to spaghetti code.\n\n**Sign up for our newsletter, *The Pydantic Stack*, with updates & tutorials on Pydantic AI, Logfire, and Pydantic:**",
    "source_url": "https://ai.pydantic.dev/",
    "header": "Why use Pydantic AI"
  },
  "8ef985c56d8f1d919a78f9e5325c3782496c2420b42991a7a83dd213d1f6bce3": {
    "text": "Here's a minimal example of Pydantic AI:\n\nhello\\_world.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent(  # (1)!\n    'google-gla:gemini-1.5-flash',\n    system_prompt='Be concise, reply with one sentence.',  # (2)!\n)\n\nresult = agent.run_sync('Where does \"hello world\" come from?')  # (3)!\nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n```\n\n1. We configure the agent to use [Gemini 1.5's Flash](api/models/google/ \"api/models/google/\") model, but you can also set the model when running the agent.\n2. Register a static [system prompt](agents/#system-prompts \"agents/#system-prompts\") using a keyword argument to the agent.\n3. [Run the agent](agents/#running-agents \"agents/#running-agents\") synchronously, conducting a conversation with the LLM.\n\n*(This example is complete, it can be run \"as is\")*\n\nThe exchange should be very short: Pydantic AI will send the system prompt and the user query to the LLM, the model will return a text response.\n\nNot very interesting yet, but we can easily add \"tools\", dynamic system prompts, and structured responses to build more powerful agents.\n\nHere is a concise example using Pydantic AI to build a support agent for a bank:\n\nbank\\_support.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\n\n@dataclass\nclass SupportDependencies:  # (3)!\n    customer_id: int\n    db: DatabaseConn  # (12)!\n\n\nclass SupportOutput(BaseModel):  # (13)!\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\nsupport_agent = Agent(  # (1)!\n    'openai:gpt-4o',  # (2)!\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,  # (9)!\n    system_prompt=(  # (4)!\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n\n@support_agent.system_prompt  # (5)!\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n@support_agent.tool  # (6)!\nasync def customer_balance(\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n    \"\"\"Returns the customer's current account balance.\"\"\"  # (7)!\n    return await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n\n\n...  # (11)!\n\n\nasync def main():\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = await support_agent.run('What is my balance?', deps=deps)  # (8)!\n    print(result.output)  # (10)!\n    \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```\n\n1. This [agent](agents/ \"agents/\") will act as first-tier support in a bank. Agents are generic in the type of dependencies they accept and the type of output they return. In this case, the support agent has type `Agent[SupportDependencies, SupportOutput]`.\n2. Here we configure the agent to use [OpenAI's GPT-4o model](api/models/openai/ \"api/models/openai/\"), you can also set the model when running the agent.\n3. The `SupportDependencies` dataclass is used to pass data, connections, and logic into the model that will be needed when running [system prompt](agents/#system-prompts \"agents/#system-prompts\") and [tool](tools/ \"tools/\") functions. Pydantic AI's system of dependency injection provides a [type-safe](agents/#static-type-checking \"agents/#static-type-checking\") way to customise the behavior of your agents, and can be especially useful when running [unit tests](testing/ \"testing/\") and evals.\n4. Static [system prompts](agents/#system-prompts \"agents/#system-prompts\") can be registered with the [`system_prompt` keyword argument](api/agent/#pydantic_ai.agent.Agent.__init__ \"api/agent/#pydantic_ai.agent.Agent.__init__\") to the agent.\n5. Dynamic [system prompts](agents/#system-prompts \"agents/#system-prompts\") can be registered with the [`@agent.system_prompt`](api/agent/#pydantic_ai.agent.Agent.system_prompt \"api/agent/#pydantic_ai.agent.Agent.system_prompt\") decorator, and can make use of dependency injection. Dependencies are carried via the [`RunContext`](api/tools/#pydantic_ai.tools.RunContext \"api/tools/#pydantic_ai.tools.RunContext\") argument, which is parameterized with the `deps_type` from above. If the type annotation here is wrong, static type checkers will catch it.\n6. [`tool`](tools/ \"tools/\") let you register functions which the LLM may call while responding to a user. Again, dependencies are carried via [`RunContext`](api/tools/#pydantic_ai.tools.RunContext \"api/tools/#pydantic_ai.tools.RunContext\"), any other arguments become the tool schema passed to the LLM. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\n7. The docstring of a tool is also passed to the LLM as the description of the tool. Parameter descriptions are [extracted](tools/#function-tools-and-schema \"tools/#function-tools-and-schema\") from the docstring and added to the parameter schema sent to the LLM.\n8. [Run the agent](agents/#running-agents \"agents/#running-agents\") asynchronously, conducting a conversation with the LLM until a final response is reached. Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.\n9. The response from the agent will, be guaranteed to be a `SupportOutput`, if validation fails [reflection](agents/#reflection-and-self-correction \"agents/#reflection-and-self-correction\") will mean the agent is prompted to try again.\n10. The output will be validated with Pydantic to guarantee it is a `SupportOutput`, since the agent is generic, it'll also be typed as a `SupportOutput` to aid with static type checking.\n11. In a real use case, you'd add more tools and a longer system prompt to the agent to extend the context it's equipped with and support it can provide.\n12. This is a simple sketch of a database connection, used to keep the example short and readable. In reality, you'd be connecting to an external database (e.g. PostgreSQL) to get information about customers.\n13. This [Pydantic](https://docs.pydantic.dev \"https://docs.pydantic.dev\") model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run.\n\nComplete `bank_support.py` example\n\nThe code included here is incomplete for the sake of brevity (the definition of `DatabaseConn` is missing); you can find the complete `bank_support.py` example [here](examples/bank-support/ \"examples/bank-support/\").",
    "source_url": "https://ai.pydantic.dev/",
    "header": "Hello World Example"
  },
  "f0404ee3b526b452c2dc3c05fbca025345b699ad33b27885c94b07df508e7315": {
    "text": "To understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire.\n\nTo do this, we need to set up logfire, and add the following to our code:\n\nbank\\_support\\_with\\_logfire.py\n\n```\n...\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\nimport logfire\n\nlogfire.configure()  # (1)!\nlogfire.instrument_asyncpg()  # (2)!\n\n...\n\nsupport_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n    instrument=True,\n)\n```\n\n1. Configure logfire, this will fail if project is not set up.\n2. In our demo, `DatabaseConn` uses `asyncpg` to connect to a PostgreSQL database, so [`logfire.instrument_asyncpg()`](https://magicstack.github.io/asyncpg/current/ \"https://magicstack.github.io/asyncpg/current/\") is used to log the database queries.\n\nThat's enough to get the following view of your agent in action:\n\nSee [Monitoring and Performance](logfire/ \"logfire/\") to learn more.",
    "source_url": "https://ai.pydantic.dev/",
    "header": "Instrumentation with Pydantic Logfire"
  },
  "862fbb556ced89dd42fe6d26ab15323a7cfd1843841a99562cdd65eac816cd48": {
    "text": "The Pydantic AI documentation is available in the [llms.txt](https://llmstxt.org/ \"https://llmstxt.org/\") format.\nThis format is defined in Markdown and suited for large language models.\n\nTwo formats are available:\n\n* [llms.txt](https://ai.pydantic.dev/llms.txt \"https://ai.pydantic.dev/llms.txt\"): a file containing a brief description\n  of the project, along with links to the different sections of the documentation. The structure\n  of this file is described in details [here](https://llmstxt.org/#format \"https://llmstxt.org/#format\").\n* [llms-full.txt](https://ai.pydantic.dev/llms-full.txt \"https://ai.pydantic.dev/llms-full.txt\"): Similar to the `llms.txt` file,\n  but every link content is included. Note that this file may be too large for some LLMs.\n\nAs of today, these files *cannot* be natively leveraged by LLM frameworks or IDEs. Alternatively,\nan [MCP server](https://modelcontextprotocol.io/ \"https://modelcontextprotocol.io/\") can be implemented to properly parse the `llms.txt`\nfile.",
    "source_url": "https://ai.pydantic.dev/",
    "header": "llms.txt"
  },
  "2763164f3ffbe6429ae9d06bf9696d0b44e2e44dfddfb0ef69f46a0d463a75f8": {
    "text": "To try Pydantic AI yourself, follow the instructions [in the examples](examples/ \"examples/\").\n\nRead the [docs](agents/ \"agents/\") to learn more about building applications with Pydantic AI.\n\nRead the [API Reference](api/agent/ \"api/agent/\") to understand Pydantic AI's interface.",
    "source_url": "https://ai.pydantic.dev/",
    "header": "Next Steps"
  },
  "5e173fffe3ed25a5a37422320cb1d3f86d3d4b5672dd9505bb67a8c0912a0efe": {
    "text": "Pydantic AI is available on PyPI as [`pydantic-ai`](https://pypi.org/project/pydantic-ai/ \"https://pypi.org/project/pydantic-ai/\") so installation is as simple as:\n\n(Requires Python 3.10+)\n\nThis installs the `pydantic_ai` package, core dependencies, and libraries required to use all the models included in Pydantic AI.\nIf you want to install only those dependencies required to use a specific model, you can install the [\"slim\"](#slim-install \"#slim-install\") version of Pydantic AI.",
    "source_url": "https://ai.pydantic.dev/install/",
    "header": "Installation"
  },
  "e68dc51a55d6813647855804da5fec6aa2a2a4282159c98ecd81fd7317c9a1c7": {
    "text": "Pydantic AI has an excellent (but completely optional) integration with [Pydantic Logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\") to help you view and understand agent runs.\n\nLogfire comes included with `pydantic-ai` (but not the [\"slim\" version](#slim-install \"#slim-install\")), so you can typically start using it immediately by following the [Logfire setup docs](../logfire/#using-logfire \"../logfire/#using-logfire\").",
    "source_url": "https://ai.pydantic.dev/install/",
    "header": "Use with Pydantic Logfire"
  },
  "30b4d7a09c6e4a643bbc60ba4d5b7bf2701eb231f792b77b22f9b4134a40190a": {
    "text": "We distribute the [`pydantic_ai_examples`](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples \"https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples\") directory as a separate PyPI package ([`pydantic-ai-examples`](https://pypi.org/project/pydantic-ai-examples/ \"https://pypi.org/project/pydantic-ai-examples/\")) to make examples extremely easy to customize and run.\n\nTo install examples, use the `examples` optional group:\n\nTo run the examples, follow instructions in the [examples docs](../examples/ \"../examples/\").",
    "source_url": "https://ai.pydantic.dev/install/",
    "header": "Running Examples"
  },
  "2ed5af70330b44e31c44895e8ed4c55ea72ab7c7b11ea4c3219ad532744709d2": {
    "text": "If you know which model you're going to use and want to avoid installing superfluous packages, you can use the [`pydantic-ai-slim`](https://pypi.org/project/pydantic-ai-slim/ \"https://pypi.org/project/pydantic-ai-slim/\") package.\nFor example, if you're using just [`OpenAIChatModel`](../api/models/openai/#pydantic_ai.models.openai.OpenAIChatModel \"../api/models/openai/#pydantic_ai.models.openai.OpenAIChatModel\"), you would run:\n\n`pydantic-ai-slim` has the following optional groups:\n\nSee the [models](../models/ \"../models/\") documentation for information on which optional dependencies are required for each model.\n\nYou can also install dependencies for multiple models and use cases, for example:",
    "source_url": "https://ai.pydantic.dev/install/",
    "header": "Slim Install"
  },
  "96c122bffb7d58217164c7bd38655048f271770a96d6fb2e1054928c75a95f2e": {
    "text": "If you need help getting started with Pydantic AI or with advanced usage, the following sources may be useful.",
    "source_url": "https://ai.pydantic.dev/help/",
    "header": "Getting Help"
  },
  "71ce340f8305a1105186f1abe48ae3e87328df4cc176a193c88d8220a8e07187": {
    "text": "Join the `#pydantic-ai` channel in the [Pydantic Slack](https://logfire.pydantic.dev/docs/join-slack/ \"https://logfire.pydantic.dev/docs/join-slack/\") to ask questions, get help, and chat about Pydantic AI. There's also channels for Pydantic, Logfire, and FastUI.\n\nIf you're on a [Logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\") Pro plan, you can also get a dedicated private slack collab channel with us.",
    "source_url": "https://ai.pydantic.dev/help/",
    "header": "Slack"
  },
  "4a97cf051040902cde8b48bf484605fd45b04a9f7212c7d39394b9fdd68dd5d0": {
    "text": "The [Pydantic AI GitHub Issues](https://github.com/pydantic/pydantic-ai/issues \"https://github.com/pydantic/pydantic-ai/issues\") are a great place to ask questions and give us feedback.",
    "source_url": "https://ai.pydantic.dev/help/",
    "header": "GitHub Issues"
  },
  "f8a5166633ed416c2bbce0a052d7d26ddea1cae75e71d6b25d40ab57d399999f": {
    "text": "We'd love you to contribute to Pydantic AI!",
    "source_url": "https://ai.pydantic.dev/contributing/",
    "header": "Contributing"
  },
  "9667d2bc6c70f4723b5ee8200958f01aef1bbc716ea4acdd0ce7c3aed7a91018": {
    "text": "Clone your fork and cd into the repo directory\n\n```\ngit clone git@github.com:<your username>/pydantic-ai.git\ncd pydantic-ai\n```\n\nInstall `uv` (version 0.4.30 or later), `pre-commit` and `deno`:\n\nTo install `pre-commit` you can run the following command:\n\n```\nuv tool install pre-commit\n```\n\nFor `deno`, you can run the following, or check\n[their documentation](https://docs.deno.com/runtime/getting_started/installation/ \"https://docs.deno.com/runtime/getting_started/installation/\") for alternative\ninstallation methods:\n\n```\ncurl -fsSL https://deno.land/install.sh | sh\n```\n\nInstall `pydantic-ai`, all dependencies and pre-commit hooks",
    "source_url": "https://ai.pydantic.dev/contributing/",
    "header": "Installation and Setup"
  },
  "aed3c1f165d957e6036b3f3edceefef30087d1d3d7ac91e1e1a2486e468ef189": {
    "text": "We use `make` to manage most commands you'll need to run.\n\nFor details on available commands, run:\n\nTo run code formatting, linting, static type checks, and tests with coverage report generation, run:",
    "source_url": "https://ai.pydantic.dev/contributing/",
    "header": "Running Tests etc."
  },
  "e11d5e546b341cef96b1e06da77986819354382d0a91dd7b6c6559568d3288b0": {
    "text": "To run the documentation page locally, run:",
    "source_url": "https://ai.pydantic.dev/contributing/",
    "header": "Documentation Changes"
  },
  "5f91763e8841b8ef444fbe9a4452177de6dc4332eb740f09e2de4accade8ee08": {
    "text": "To avoid an excessive workload for the maintainers of Pydantic AI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't. This should hopefully reduce the chances of disappointment and wasted work.\n\n* To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\n* To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\n* For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\n* For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on [`pydantic-ai-slim`](../install/#slim-install \"../install/#slim-install\") and implements a model that inherits from our [`Model`](../api/models/base/#pydantic_ai.models.Model \"../api/models/base/#pydantic_ai.models.Model\") ABC\n\nIf you're unsure about adding a model, please [create an issue](https://github.com/pydantic/pydantic-ai/issues \"https://github.com/pydantic/pydantic-ai/issues\").",
    "source_url": "https://ai.pydantic.dev/contributing/",
    "header": "Rules for adding new models to Pydantic AI"
  },
  "e5ddc4cd022064809a98c7902729165a2139e9517bb48061e3faaa2f3434a85c": {
    "text": "Below are suggestions on how to fix some common errors you might encounter while using Pydantic AI. If the issue you're experiencing is not listed below or addressed in the documentation, please feel free to ask in the [Pydantic Slack](../help/ \"../help/\") or create an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues \"https://github.com/pydantic/pydantic-ai/issues\").",
    "source_url": "https://ai.pydantic.dev/troubleshooting/",
    "header": "Troubleshooting"
  },
  "18ea0adf5c87cadb8487fc30b9cc617ef15407b88ebe4c22449bb83e694eb145": {
    "text": "This error is caused by conflicts between the event loops in Jupyter notebook and Pydantic AI's. One way to manage these conflicts is by using [`nest-asyncio`](https://pypi.org/project/nest-asyncio/ \"https://pypi.org/project/nest-asyncio/\"). Namely, before you execute any agent runs, do the following:\n\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\nNote: This fix also applies to Google Colab and [Marimo](https://github.com/marimo-team/marimo \"https://github.com/marimo-team/marimo\").",
    "source_url": "https://ai.pydantic.dev/troubleshooting/",
    "header": "`RuntimeError: This event loop is already running`"
  },
  "8b5d3acc2100f36d9617c8c69a1a557cf1888e5a60e6387e77423cdfa4db81f2": {
    "text": "If you're running into issues with setting the API key for your model, visit the [Models](../models/ \"../models/\") page to learn more about how to set an environment variable and/or pass in an `api_key` argument.",
    "source_url": "https://ai.pydantic.dev/troubleshooting/",
    "header": "`UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable`"
  },
  "360ac6fd14648d2559617edbf719ce344e9e7faf34d9aef36053d4cc22b19a93": {
    "text": "You can use custom `httpx` clients in your models in order to access specific requests, responses, and headers at runtime.\n\nIt's particularly helpful to use `logfire`'s [HTTPX integration](../logfire/#monitoring-http-requests \"../logfire/#monitoring-http-requests\") to monitor the above.",
    "source_url": "https://ai.pydantic.dev/troubleshooting/",
    "header": "Monitoring HTTPX Requests"
  },
  "1d37d0f0a0946bac579677b0d52a739a09e7bb3ac9b1448503a9f06b85f6b109": {
    "text": "Pydantic AI is still pre-version 1, so breaking changes will occur, however:\n\n* We try to minimize them as much as possible.\n* We use minor version bumps to signify breaking changes.\n* Wherever possible we deprecate old features so code continues to work with deprecation warnings when changing the public API.\n* We intend to release V1 in summer 2025, and then follow strict semantic versioning, e.g. no intentional breaking changes except in minor or patch versions.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "Upgrade Guide"
  },
  "8d97f40f7a346a246fbafa67206f4e4badc5ed99e6d4e030cfd39a57e7b30575": {
    "text": "Note\n\nHere's a filtered list of the breaking changes for each version to help you upgrade Pydantic AI.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "Breaking Changes"
  },
  "5454cc820b87fc17fe68312886e5db46f8024b0c2f17f7666b0237aaa2868496": {
    "text": "See [#2689](https://github.com/pydantic/pydantic-ai/pull/2689 \"https://github.com/pydantic/pydantic-ai/pull/2689\") - `AgentStreamEvent` was expanded to be a union of `ModelResponseStreamEvent` and `HandleResponseEvent`, simplifying the `event_stream_handler` function signature. Existing code accepting `AgentStreamEvent | HandleResponseEvent` will continue to work.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.8.0 (2025-08-26)"
  },
  "c1fc86440a54e454dd1c5e8d2667e7e30965c15208d6527ad6d6a8250a0efba3": {
    "text": "The following breaking change was inadvertently released in a patch version rather than a minor version:\n\nSee [#2670](https://github.com/pydantic/pydantic-ai/pull/2670 \"https://github.com/pydantic/pydantic-ai/pull/2670\") - `TenacityTransport` and `AsyncTenacityTransport` now require the use of `pydantic_ai.retries.RetryConfig` (which is just a `TypedDict` containing the kwargs to `tenacity.retry`) instead of `tenacity.Retrying` or `tenacity.AsyncRetrying`.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.7.6 (2025-08-26)"
  },
  "eb3fdb13fe8b850fcc6773927be362c4a025f955a01d016ab898d1d24887ec66": {
    "text": "See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458 \"https://github.com/pydantic/pydantic-ai/pull/2458\") - `pydantic_ai.models.StreamedResponse` now yields a `FinalResultEvent` along with the existing `PartStartEvent` and `PartDeltaEvent`. If you're using `pydantic_ai.direct.model_request_stream` or `pydantic_ai.direct.model_request_stream_sync`, you may need to update your code to account for this.\n\nSee [#2458](https://github.com/pydantic/pydantic-ai/pull/2458 \"https://github.com/pydantic/pydantic-ai/pull/2458\") - `pydantic_ai.models.Model.request_stream` now receives a `run_context` argument. If you've implemented a custom `Model` subclass, you will need to account for this.\n\nSee [#2458](https://github.com/pydantic/pydantic-ai/pull/2458 \"https://github.com/pydantic/pydantic-ai/pull/2458\") - `pydantic_ai.models.StreamedResponse` now requires a `model_request_parameters` field and constructor argument. If you've implemented a custom `Model` subclass and implemented `request_stream`, you will need to account for this.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.7.0 (2025-08-12)"
  },
  "53cd5afaefce7a698d2ec2ea5bfebb81e0bb8af35d8acf69d21a2a05d76b9f44": {
    "text": "This release was meant to clean some old deprecated code, so we can get a step closer to V1.\n\nSee [#2440](https://github.com/pydantic/pydantic-ai/pull/2440 \"https://github.com/pydantic/pydantic-ai/pull/2440\") - The `next` method was removed from the `Graph` class. Use `async with graph.iter(...) as run: run.next()` instead.\n\nSee [#2441](https://github.com/pydantic/pydantic-ai/pull/2441 \"https://github.com/pydantic/pydantic-ai/pull/2441\") - The `result_type`, `result_tool_name` and `result_tool_description` arguments were removed from the `Agent` class. Use `output_type` instead.\n\nSee [#2441](https://github.com/pydantic/pydantic-ai/pull/2441 \"https://github.com/pydantic/pydantic-ai/pull/2441\") - The `result_retries` argument was also removed from the `Agent` class. Use `output_retries` instead.\n\nSee [#2443](https://github.com/pydantic/pydantic-ai/pull/2443 \"https://github.com/pydantic/pydantic-ai/pull/2443\") - The `data` property was removed from the `FinalResult` class. Use `output` instead.\n\nSee [#2445](https://github.com/pydantic/pydantic-ai/pull/2445 \"https://github.com/pydantic/pydantic-ai/pull/2445\") - The `get_data` and `validate_structured_result` methods were removed from the\n`StreamedRunResult` class. Use `get_output` and `validate_structured_output` instead.\n\nSee [#2446](https://github.com/pydantic/pydantic-ai/pull/2446 \"https://github.com/pydantic/pydantic-ai/pull/2446\") - The `format_as_xml` function was moved to the `pydantic_ai.format_as_xml` module.\nImport it via `from pydantic_ai import format_as_xml` instead.\n\nSee [#2451](https://github.com/pydantic/pydantic-ai/pull/2451 \"https://github.com/pydantic/pydantic-ai/pull/2451\") - Removed deprecated `Agent.result_validator` method, `Agent.last_run_messages` property, `AgentRunResult.data` property, and `result_tool_return_content` parameters from result classes.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.6.0 (2025-08-06)"
  },
  "d5569564e0e94a7700b473990b81d55589699c9cdb7ee054ba0886a0ef567bad": {
    "text": "See [#2388](https://github.com/pydantic/pydantic-ai/pull/2388 \"https://github.com/pydantic/pydantic-ai/pull/2388\") - The `source` field of an `EvaluationResult` is now of type `EvaluatorSpec` rather than the actual source `Evaluator` instance, to help with serialization/deserialization.\n\nSee [#2163](https://github.com/pydantic/pydantic-ai/pull/2163 \"https://github.com/pydantic/pydantic-ai/pull/2163\") - The `EvaluationReport.print` and `EvaluationReport.console_table` methods now require most arguments be passed by keyword.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.5.0 (2025-08-04)"
  },
  "406b8d035b5183fdc3a901e0d3be409c42abb90918c8f6dca28b6e059c851590": {
    "text": "See [#1799](https://github.com/pydantic/pydantic-ai/pull/1799 \"https://github.com/pydantic/pydantic-ai/pull/1799\") - Pydantic Evals `EvaluationReport` and `ReportCase` are now generic dataclasses instead of Pydantic models. If you were serializing them using `model_dump()`, you will now need to use the `EvaluationReportAdapter` and `ReportCaseAdapter` type adapters instead.\n\nSee [#1507](https://github.com/pydantic/pydantic-ai/pull/1507 \"https://github.com/pydantic/pydantic-ai/pull/1507\") - The `ToolDefinition` `description` argument is now optional and the order of positional arguments has changed from `name, description, parameters_json_schema, ...` to `name, parameters_json_schema, description, ...` to account for this.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.4.0 (2025-07-08)"
  },
  "5c0a5e186b6857a9f7e18914fb62557cc647635ab56c7db3dabc0130c2eac3bf": {
    "text": "See [#1142](https://github.com/pydantic/pydantic-ai/pull/1142 \"https://github.com/pydantic/pydantic-ai/pull/1142\") — Adds support for thinking parts.\n\nWe now convert the thinking blocks (`\"<think>...\"</think>\"`) in provider specific text parts to\nPydantic AI `ThinkingPart`s. Also, as part of this release, we made the choice to not send back the\n`ThinkingPart`s to the provider - the idea is to save costs on behalf of the user. In the future, we\nintend to add a setting to customize this behavior.",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.3.0 (2025-06-18)"
  },
  "8563b04c35a4d5f5d99b3981de6a0d081d8d08438b4c95d54cce547008da4d7f": {
    "text": "See [#1647](https://github.com/pydantic/pydantic-ai/pull/1647 \"https://github.com/pydantic/pydantic-ai/pull/1647\") — usage makes sense as part of `ModelResponse`, and could be really useful in \"messages\" (really a sequence of requests and response). In this PR:\n\n* Adds `usage` to `ModelResponse` (field has a default factory of `Usage()` so it'll work to load data that doesn't have usage)\n* changes the return type of `Model.request` to just `ModelResponse` instead of `tuple[ModelResponse, Usage]`",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.2.0 (2025-05-12)"
  },
  "fd73a410c873c5991b2160455dab8a1e5a517b4c332808f35d81898fc21767cb": {
    "text": "See [#1248](https://github.com/pydantic/pydantic-ai/pull/1248 \"https://github.com/pydantic/pydantic-ai/pull/1248\") — the attribute/parameter name `result` was renamed to `output` in many places. Hopefully all changes keep a deprecated attribute or parameter with the old name, so you should get many deprecation warnings.\n\nSee [#1484](https://github.com/pydantic/pydantic-ai/pull/1484 \"https://github.com/pydantic/pydantic-ai/pull/1484\") — `format_as_xml` was moved and made available to import from the package root, e.g. `from pydantic_ai import format_as_xml`.\n\n---",
    "source_url": "https://ai.pydantic.dev/changelog/",
    "header": "v0.1.0 (2025-04-15)"
  },
  "f637fcf7b6e892cacd2f681f4f3ca7d812f8b764c692323a5f16bbada5fd5c38": {
    "text": "Agents are Pydantic AI's primary interface for interacting with LLMs.\n\nIn some use cases a single Agent will control an entire application or component,\nbut multiple agents can also interact to embody more complex workflows.\n\nThe [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\") class has full API documentation, but conceptually you can think of an agent as a container for:\n\n| **Component** | **Description** |\n| --- | --- |\n| [System prompt(s)](#system-prompts \"#system-prompts\") | A set of instructions for the LLM written by the developer. |\n| [Function tool(s)](../tools/ \"../tools/\") and [toolsets](../toolsets/ \"../toolsets/\") | Functions that the LLM may call to get information while generating a response. |\n| [Structured output type](../output/ \"../output/\") | The structured datatype the LLM must return at the end of a run, if specified. |\n| [Dependency type constraint](../dependencies/ \"../dependencies/\") | System prompt functions, tools, and output validators may all use dependencies when they're run. |\n| [LLM model](../api/models/base/ \"../api/models/base/\") | Optional default LLM model associated with the agent. Can also be specified when running the agent. |\n| [Model Settings](#additional-configuration \"#additional-configuration\") | Optional default model settings to help fine tune requests. Can also be specified when running the agent. |\n\nIn typing terms, agents are generic in their dependency and output types, e.g., an agent which required dependencies of type `Foobar` and produced outputs of type `list[str]` would have type `Agent[Foobar, list[str]]`. In practice, you shouldn't need to care about this, it should just mean your IDE can tell you when you have the right type, and if you choose to use [static type checking](#static-type-checking \"#static-type-checking\") it should work well with Pydantic AI.\n\nHere's a toy example of an agent that simulates a roulette wheel:\n\nroulette\\_wheel.py\n\n```\nfrom pydantic_ai import Agent, RunContext\n\nroulette_agent = Agent(  # (1)!\n    'openai:gpt-4o',\n    deps_type=int,\n    output_type=bool,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n    ),\n)\n\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  # (2)!\n    \"\"\"check if the square is a winner\"\"\"\n    return 'winner' if square == ctx.deps else 'loser'",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Introduction"
  },
  "70517a9ef6d9b1bc40f9487fc774106b34df2150a25ed56dcdf33e406d781129": {
    "text": "success_number = 18  # (3)!\nresult = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)\nprint(result.output)  # (4)!\n#> True\n\nresult = roulette_agent.run_sync('I bet five is the winner', deps=success_number)\nprint(result.output)\n#> False\n```\n\n1. Create an agent, which expects an integer dependency and produces a boolean output. This agent will have type `Agent[int, bool]`.\n2. Define a tool that checks if the square is a winner. Here [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") is parameterized with the dependency type `int`; if you got the dependency type wrong you'd get a typing error.\n3. In reality, you might want to use a random number here e.g. `random.randint(0, 36)`.\n4. `result.output` will be a boolean indicating if the square is a winner. Pydantic performs the output validation, and it'll be typed as a `bool` since its type is derived from the `output_type` generic parameter of the agent.\n\nAgents are designed for reuse, like FastAPI Apps\n\nAgents are intended to be instantiated once (frequently as module globals) and reused throughout your application, similar to a small [FastAPI](https://fastapi.tiangolo.com/reference/fastapi/#fastapi.FastAPI \"https://fastapi.tiangolo.com/reference/fastapi/#fastapi.FastAPI\") app or an [APIRouter](https://fastapi.tiangolo.com/reference/apirouter/#fastapi.APIRouter \"https://fastapi.tiangolo.com/reference/apirouter/#fastapi.APIRouter\").",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Run the agent"
  },
  "6c999e513a21c66718e73cb03ebf35157ff3eab6c15f2caf78dccdfbe2f94d24": {
    "text": "There are four ways to run an agent:\n\n1. [`agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\") — an async function which returns a [`RunResult`](../api/agent/#pydantic_ai.agent.AgentRunResult \"../api/agent/#pydantic_ai.agent.AgentRunResult\") containing a completed response.\n2. [`agent.run_sync()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync\") — a plain, synchronous function which returns a [`RunResult`](../api/agent/#pydantic_ai.agent.AgentRunResult \"../api/agent/#pydantic_ai.agent.AgentRunResult\") containing a completed response (internally, this just calls `loop.run_until_complete(self.run())`).\n3. [`agent.run_stream()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\") — an async context manager which returns a [`StreamedRunResult`](../api/result/#pydantic_ai.result.StreamedRunResult \"../api/result/#pydantic_ai.result.StreamedRunResult\"), which contains methods to stream text and structured output as an async iterable.\n4. [`agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\") — a context manager which returns an [`AgentRun`](../api/agent/#pydantic_ai.agent.AgentRun \"../api/agent/#pydantic_ai.agent.AgentRun\"), an async-iterable over the nodes of the agent's underlying [`Graph`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph\").\n\nHere's a simple example demonstrating the first three:\n\nrun\\_agent.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n\nasync def main():\n    result = await agent.run('What is the capital of France?')\n    print(result.output)\n    #> The capital of France is Paris.\n\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        async for text in response.stream_text():\n            print(text)\n            #> The capital of\n            #> The capital of the UK is\n            #> The capital of the UK is London.\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nYou can also pass messages from previous runs to continue a conversation or provide context, as described in [Messages and Chat History](../message-history/ \"../message-history/\").",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Running Agents"
  },
  "0de3b91a96ea6160b5d7a521a659e292fb5f265ca394b8f8f63808c3a627dac8": {
    "text": "As shown in the example above, [`run_stream()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\") makes it easy to stream the agent's final output as it comes in.\nIt also takes an optional `event_stream_handler` argument that you can use to gain insight into what is happening during the run before the final output is produced.\n\nThe example below shows how to stream events and text output. You can also [stream structured output](../output/#streaming-structured-output \"../output/#streaming-structured-output\").\n\nNote\n\nAs the `run_stream()` method will consider the first output matching the `output_type` to be the final output,\nit will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\n\nIf you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools,\nuse [`agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\") with an `event_stream_handler` or [`agent.iter()`](../api/agent/#pydantic_ai.agent.AbstractAgent.iter \"../api/agent/#pydantic_ai.agent.AbstractAgent.iter\") instead, as described in the following sections.\n\nrun\\_stream\\_events.py\n\n```\nimport asyncio\nfrom collections.abc import AsyncIterable\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.messages import (\n    AgentStreamEvent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext,\n    location: str,\n    forecast_date: date,\n) -> str:\n    return f'The forecast in {location} on {forecast_date} is 24°C and sunny.'\n\n\noutput_messages: list[str] = []\n\n\nasync def event_stream_handler(\n    ctx: RunContext,\n    event_stream: AsyncIterable[AgentStreamEvent],\n):\n    async for event in event_stream:\n        if isinstance(event, PartStartEvent):\n            output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n        elif isinstance(event, PartDeltaEvent):\n            if isinstance(event.delta, TextPartDelta):\n                output_messages.append(f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}')\n            elif isinstance(event.delta, ThinkingPartDelta):\n                output_messages.append(f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}')\n            elif isinstance(event.delta, ToolCallPartDelta):\n                output_messages.append(f'[Request] Part {event.index} args delta: {event.delta.args_delta}')\n        elif isinstance(event, FunctionToolCallEvent):\n            output_messages.append(\n                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n            )\n        elif isinstance(event, FunctionToolResultEvent):\n            output_messages.append(f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}')\n        elif isinstance(event, FinalResultEvent):\n            output_messages.append(f'[Result] The model starting producing a final result (tool_name={event.tool_name})')\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async with weather_agent.run_stream(user_prompt, event_stream_handler=event_stream_handler) as run:\n        async for output in run.stream_text():\n            output_messages.append(f'[Output] {output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n```",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Streaming Events and Final Output"
  },
  "1b47815ba729c3f3ec970a23f21683faee5c5a38fa3e2470f4d9f142a50f8d0d": {
    "text": "Like `agent.run_stream()`, [`agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\") takes an optional `event_stream_handler`\nargument that lets you stream all events from the model's streaming response and the agent's execution of tools.\nUnlike `run_stream()`, it always runs the agent graph to completion even if text was received ahead of tool calls that looked like it could've been the final result.\n\nNote\n\nWhen used with an `event_stream_handler`, the `run()` method currently requires you to piece together the streamed text yourself from the `PartStartEvent` and subsequent `PartDeltaEvent`s instead of providing a `stream_text()` convenience method.\n\nTo get the best of both worlds, at the expense of some additional complexity, you can use [`agent.iter()`](../api/agent/#pydantic_ai.agent.AbstractAgent.iter \"../api/agent/#pydantic_ai.agent.AbstractAgent.iter\") as described in the next section, which lets you [iterate over the agent graph](#iterating-over-an-agents-graph \"#iterating-over-an-agents-graph\") and [stream both events and output](#streaming-all-events-and-output \"#streaming-all-events-and-output\") at every step.\n\nrun\\_events.py\n\n```\nimport asyncio\n\nfrom run_stream_events import event_stream_handler, output_messages, weather_agent\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    run = await weather_agent.run(user_prompt, event_stream_handler=event_stream_handler)\n\n    output_messages.append(f'[Final Output] {run.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        \"[Request] Part 0 text delta: 'warm and sunny '\",\n        \"[Request] Part 0 text delta: 'in Paris on '\",\n        \"[Request] Part 0 text delta: 'Tuesday.'\",\n        '[Final Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Streaming All Events"
  },
  "f29d68abd90acc1e01af14e1fbb217f333033c2517fd9c366fe252fb8dde61dd": {
    "text": "Under the hood, each `Agent` in Pydantic AI uses **pydantic-graph** to manage its execution flow. **pydantic-graph** is a generic, type-centric library for building and running finite state machines in Python. It doesn't actually depend on Pydantic AI — you can use it standalone for workflows that have nothing to do with GenAI — but Pydantic AI makes use of it to orchestrate the handling of model requests and model responses in an agent's run.\n\nIn many scenarios, you don't need to worry about pydantic-graph at all; calling `agent.run(...)` simply traverses the underlying graph from start to finish. However, if you need deeper insight or control — for example to inject your own logic at specific stages — Pydantic AI exposes the lower-level iteration process via [`Agent.iter`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\"). This method returns an [`AgentRun`](../api/agent/#pydantic_ai.agent.AgentRun \"../api/agent/#pydantic_ai.agent.AgentRun\"), which you can async-iterate over, or manually drive node-by-node via the [`next`](../api/agent/#pydantic_ai.agent.AgentRun.next \"../api/agent/#pydantic_ai.agent.AgentRun.next\") method. Once the agent's graph returns an [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\"), you have the final result along with a detailed history of all steps.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Iterating Over an Agent's Graph"
  },
  "03ea27958b26ba60d26abf74f263447aa6509dc31ef0264fbb3c1d776fb1623b": {
    "text": "Here's an example of using `async for` with `iter` to record each node the agent executes:\n\nagent\\_iter\\_async\\_for.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\n\nasync def main():\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    print(nodes)\n    \"\"\"\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions=None,\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    \"\"\"\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n* The `AgentRun` is an async iterator that yields each node (`BaseNode` or `End`) in the flow.\n* The run ends when an `End` node is returned.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "`async for` iteration"
  },
  "4562f41fdfbebe89fdb3485dd6c3dc1fd181dd377adb829280ad312ead2c4309": {
    "text": "You can also drive the iteration manually by passing the node you want to run next to the `AgentRun.next(...)` method. This allows you to inspect or modify the node before it executes or skip nodes based on your own logic, and to catch errors in `next()` more easily:\n\nagent\\_iter\\_next.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-4o')\n\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        node = agent_run.next_node  # (1)!\n\n        all_nodes = [node]\n\n        # Drive the iteration manually:\n        while not isinstance(node, End):  # (2)!\n            node = await agent_run.next(node)  # (3)!\n            all_nodes.append(node)  # (4)!\n\n        print(all_nodes)\n        \"\"\"\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions=None,\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        \"\"\"\n```\n\n1. We start by grabbing the first node that will be run in the agent's graph.\n2. The agent run is finished once an `End` node has been produced; instances of `End` cannot be passed to `next`.\n3. When you call `await agent_run.next(node)`, it executes that node in the agent's graph, updates the run's history, and returns the *next* node to run.\n4. You could also inspect or mutate the new `node` here as needed.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Using `.next(...)` manually"
  },
  "af69ed2954911232a98d4406f85a1234c0f42bf11b168ffcd4f7a243be50b419": {
    "text": "You can retrieve usage statistics (tokens, requests, etc.) at any time from the [`AgentRun`](../api/agent/#pydantic_ai.agent.AgentRun \"../api/agent/#pydantic_ai.agent.AgentRun\") object via `agent_run.usage()`. This method returns a [`RunUsage`](../api/usage/#pydantic_ai.usage.RunUsage \"../api/usage/#pydantic_ai.usage.RunUsage\") object containing the usage data.\n\nOnce the run finishes, `agent_run.result` becomes a [`AgentRunResult`](../api/agent/#pydantic_ai.agent.AgentRunResult \"../api/agent/#pydantic_ai.agent.AgentRunResult\") object containing the final output (and related metadata).",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Accessing usage and final output"
  },
  "b53eab6fb1d95a107a510ae9acb8a27a0b85debf84b59a429e08972d1922bce9": {
    "text": "Here is an example of streaming an agent run in combination with `async for` iteration:\n\nstreaming\\_iter.py\n\n```\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.messages import (\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\n\n@dataclass\nclass WeatherService:\n    async def get_forecast(self, location: str, forecast_date: date) -> str:\n        # In real code: call weather API, DB queries, etc.\n        return f'The forecast in {location} on {forecast_date} is 24°C and sunny.'\n\n    async def get_historic_weather(self, location: str, forecast_date: date) -> str:\n        # In real code: call a historical weather API or DB\n        return f'The weather in {location} on {forecast_date} was 18°C and partly cloudy.'\n\n\nweather_agent = Agent[WeatherService, str](\n    'openai:gpt-4o',\n    deps_type=WeatherService,\n    output_type=str,  # We'll produce a final answer as plain text\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext[WeatherService],\n    location: str,\n    forecast_date: date,\n) -> str:\n    if forecast_date >= date.today():\n        return await ctx.deps.get_forecast(location, forecast_date)\n    else:\n        return await ctx.deps.get_historic_weather(location, forecast_date)\n\n\noutput_messages: list[str] = []\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    # Begin a node-by-node, streaming iteration\n    async with weather_agent.iter(user_prompt, deps=WeatherService()) as run:\n        async for node in run:\n            if Agent.is_user_prompt_node(node):\n                # A user prompt node => The user has provided input\n                output_messages.append(f'=== UserPromptNode: {node.user_prompt} ===')\n            elif Agent.is_model_request_node(node):\n                # A model request node => We can stream tokens from the model's request\n                output_messages.append('=== ModelRequestNode: streaming partial request tokens ===')\n                async with node.stream(run.ctx) as request_stream:\n                    final_result_found = False\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ThinkingPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} args delta: {event.delta.args_delta}'\n                                )\n                        elif isinstance(event, FinalResultEvent):\n                            output_messages.append(\n                                f'[Result] The model started producing a final result (tool_name={event.tool_name})'\n                            )\n                            final_result_found = True\n                            break\n\n                    if final_result_found:\n                        # Once the final result is found, we can call `AgentStream.stream_text()` to stream the text.\n                        # A similar `AgentStream.stream_output()` method is available to stream structured output.\n                        async for output in request_stream.stream_text():\n                            output_messages.append(f'[Output] {output}')\n            elif Agent.is_call_tools_node(node):\n                # A handle-response node => The model returned some data, potentially calls a tool\n                output_messages.append('=== CallToolsNode: streaming partial response & tool usage ===')\n                async with node.stream(run.ctx) as handle_stream:\n                    async for event in handle_stream:\n                        if isinstance(event, FunctionToolCallEvent):\n                            output_messages.append(\n                                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n                            )\n                        elif isinstance(event, FunctionToolResultEvent):\n                            output_messages.append(\n                                f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}'\n                            )\n            elif Agent.is_end_node(node):\n                # Once an End node is reached, the agent run is complete\n                assert run.result is not None\n                assert run.result.output == node.data.output\n                output_messages.append(f'=== Final Agent Output: {run.result.output} ===')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        '=== UserPromptNode: What will the weather be like in Paris on Tuesday? ===',\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24°C and sunny.\",\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model started producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '=== Final Agent Output: It will be warm and sunny in Paris on Tuesday. ===',\n    ]\n    \"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Streaming All Events and Output"
  },
  "2223884cb62d7411b0b02dfbd14092eaeb01e1db22e391e27a77864e14409229": {
    "text": "Pydantic AI offers a [`UsageLimits`](../api/usage/#pydantic_ai.usage.UsageLimits \"../api/usage/#pydantic_ai.usage.UsageLimits\") structure to help you limit your\nusage (tokens and/or requests) on model runs.\n\nYou can apply these settings by passing the `usage_limits` argument to the `run{_sync,_stream}` functions.\n\nConsider the following example, where we limit the number of response tokens:\n\n```\nfrom pydantic_ai import Agent, UsageLimitExceeded, UsageLimits\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n\nresult_sync = agent.run_sync(\n    'What is the capital of Italy? Answer with just the city.',\n    usage_limits=UsageLimits(response_tokens_limit=10),\n)\nprint(result_sync.output)\n#> Rome\nprint(result_sync.usage())\n#> RunUsage(input_tokens=62, output_tokens=1, requests=1)\n\ntry:\n    result_sync = agent.run_sync(\n        'What is the capital of Italy? Answer with a paragraph.',\n        usage_limits=UsageLimits(response_tokens_limit=10),\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> Exceeded the output_tokens_limit of 10 (output_tokens=32)\n```\n\nRestricting the number of requests can be useful in preventing infinite loops or excessive tool calling:\n\n```\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent, ModelRetry, UsageLimitExceeded, UsageLimits\n\n\nclass NeverOutputType(TypedDict):\n    \"\"\"\n    Never ever coerce data to this type.\n    \"\"\"\n\n    never_use_this: str\n\n\nagent = Agent(\n    'anthropic:claude-3-5-sonnet-latest',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n)\n\n\n@agent.tool_plain(retries=5)  # (1)!\ndef infinite_retry_tool() -> int:\n    raise ModelRetry('Please try again.')\n\n\ntry:\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  # (2)!\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next request would exceed the request_limit of 3\n```\n\n1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop.\n2. This run will error after 3 requests, preventing the infinite tool calling.\n\nNote\n\n* Usage limits are especially relevant if you've registered many tools. The `request_limit` can be used to prevent the model from calling them in a loop too many times.\n* These limits are enforced at the final stage before the LLM is called. If your limits are stricter than your retry settings, the usage limit will be reached before all retries are attempted.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Usage Limits"
  },
  "75947514ebe251c38efd81d287274889819fb1ff541ff61b2a6d50379ab2ee97": {
    "text": "Pydantic AI offers a [`settings.ModelSettings`](../api/settings/#pydantic_ai.settings.ModelSettings \"../api/settings/#pydantic_ai.settings.ModelSettings\") structure to help you fine tune your requests.\nThis structure allows you to configure common parameters that influence the model's behavior, such as `temperature`, `max_tokens`,\n`timeout`, and more.\n\nThere are three ways to apply these settings, with a clear precedence order:\n\n1. **Model-level defaults** - Set when creating a model instance via the `settings` parameter. These serve as the base defaults for that model.\n2. **Agent-level defaults** - Set during [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\") initialization via the `model_settings` argument. These are merged with model defaults, with agent settings taking precedence.\n3. **Run-time overrides** - Passed to `run{_sync,_stream}` functions via the `model_settings` argument. These have the highest priority and are merged with the combined agent and model defaults.\n\nFor example, if you'd like to set the `temperature` setting to `0.0` to ensure less random behavior,\nyou can do the following:\n\n```\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.models.openai import OpenAIChatModel",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Model (Run) Settings"
  },
  "5742d278dc40b16e3c5cc92230cbef9d3d8cb69b8cd3174ebe17a3dc3fbba324": {
    "text": "model = OpenAIChatModel(\n    'gpt-4o',\n    settings=ModelSettings(temperature=0.8, max_tokens=500)  # Base defaults\n)",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "1. Model-level defaults"
  },
  "66f9516ce6e44178bff5b37bd6063794de52bbe748b42750cf35a704d7fd79ee": {
    "text": "agent = Agent(model, model_settings=ModelSettings(temperature=0.5))",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "2. Agent-level defaults (overrides model defaults by merging)"
  },
  "3aeb7a9039589e5c830d17e0b9fbdf91da3d43d788c9d44ecce2c17c335d2bbf": {
    "text": "result_sync = agent.run_sync(\n    'What is the capital of Italy?',\n    model_settings=ModelSettings(temperature=0.0)  # Final temperature: 0.0\n)\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nThe final request uses `temperature=0.0` (run-time), `max_tokens=500` (from model), demonstrating how settings merge with run-time taking precedence.\n\nModel Settings Support\n\nModel-level settings are supported by all concrete model implementations (OpenAI, Anthropic, Google, etc.). Wrapper models like `FallbackModel`, `WrapperModel`, and `InstrumentedModel` don't have their own settings - they use the settings of their underlying models.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "3. Run-time overrides (highest priority)"
  },
  "93ef24d3c273c262007a6a2d339025c5c21ceba8d2675def3aa9309c201dada8": {
    "text": "If you wish to further customize model behavior, you can use a subclass of [`ModelSettings`](../api/settings/#pydantic_ai.settings.ModelSettings \"../api/settings/#pydantic_ai.settings.ModelSettings\"), like\n[`GoogleModelSettings`](../api/models/google/#pydantic_ai.models.google.GoogleModelSettings \"../api/models/google/#pydantic_ai.models.google.GoogleModelSettings\"), associated with your model of choice.\n\nFor example:\n\n```\nfrom pydantic_ai import Agent, UnexpectedModelBehavior\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\ntry:\n    result = agent.run_sync(\n        'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:',\n        model_settings=GoogleModelSettings(\n            temperature=0.0,  # general model settings can also be specified\n            gemini_safety_settings=[\n                {\n                    'category': 'HARM_CATEGORY_HARASSMENT',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n                {\n                    'category': 'HARM_CATEGORY_HATE_SPEECH',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n            ],\n        ),\n    )\nexcept UnexpectedModelBehavior as e:\n    print(e)  # (1)!\n    \"\"\"\n    Safety settings triggered, body:\n    <safety settings details>\n    \"\"\"\n```\n\n1. This error is raised because the safety thresholds were exceeded.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Model specific settings"
  },
  "039a633613332b1ed584c538809b8cff927e3a6ad1f32611e4d1f5712bf67e9f": {
    "text": "An agent **run** might represent an entire conversation — there's no limit to how many messages can be exchanged in a single run. However, a **conversation** might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls.\n\nHere's an example of a conversation comprised of multiple runs:\n\nconversation\\_example.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Runs vs. Conversations"
  },
  "e1403647b5cc84dcd835a964b773375bc061bf22c19d06cb675efbfea8de3793": {
    "text": "result1 = agent.run_sync('Who was Albert Einstein?')\nprint(result1.output)\n#> Albert Einstein was a German-born theoretical physicist.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "First run"
  },
  "99b6b15cbf22b390d4d4d5d8f2b90270362cf16806ed62ded6f0a3bd715246b8": {
    "text": "result2 = agent.run_sync(\n    'What was his most famous equation?',\n    message_history=result1.new_messages(),  # (1)!\n)\nprint(result2.output)\n#> Albert Einstein's most famous equation is (E = mc^2).\n```\n\n1. Continue the conversation; without `message_history` the model would not know who \"his\" was referring to.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Second run, passing previous messages"
  },
  "d4c96bfda47707dc87580844eb9e81b09bba6c866647ce9b1b9035fcc6d66c09": {
    "text": "Pydantic AI is designed to work well with static type checkers, like mypy and pyright.\n\nTyping is (somewhat) optional\n\nPydantic AI is designed to make type checking as useful as possible for you if you choose to use it, but you don't have to use types everywhere all the time.\n\nThat said, because Pydantic AI uses Pydantic, and Pydantic uses type hints as the definition for schema and validation, some types (specifically type hints on parameters to tools, and the `output_type` arguments to [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\")) are used at runtime.\n\nWe (the library developers) have messed up if type hints are confusing you more than helping you, if you find this, please create an [issue](https://github.com/pydantic/pydantic-ai/issues \"https://github.com/pydantic/pydantic-ai/issues\") explaining what's annoying you!\n\nIn particular, agents are generic in both the type of their dependencies and the type of the outputs they return, so you can use the type hints to ensure you're using the right types.\n\nConsider the following script with type mistakes:\n\ntype\\_mistakes.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass User:\n    name: str\n\n\nagent = Agent(\n    'test',\n    deps_type=User,  # (1)!\n    output_type=bool,\n)\n\n\n@agent.system_prompt\ndef add_user_name(ctx: RunContext[str]) -> str:  # (2)!\n    return f\"The user's name is {ctx.deps}.\"\n\n\ndef foobar(x: bytes) -> None:\n    pass\n\n\nresult = agent.run_sync('Does their name start with \"A\"?', deps=User('Anne'))\nfoobar(result.output)  # (3)!\n```\n\n1. The agent is defined as expecting an instance of `User` as `deps`.\n2. But here `add_user_name` is defined as taking a `str` as the dependency, not a `User`.\n3. Since the agent is defined as returning a `bool`, this will raise a type error since `foobar` expects `bytes`.\n\nRunning `mypy` on this will give the following output:\n\n```\n➤ uv run mypy type_mistakes.py\ntype_mistakes.py:18: error: Argument 1 to \"system_prompt\" of \"Agent\" has incompatible type \"Callable[[RunContext[str]], str]\"; expected \"Callable[[RunContext[User]], str]\"  [arg-type]\ntype_mistakes.py:28: error: Argument 1 to \"foobar\" has incompatible type \"bool\"; expected \"bytes\"  [arg-type]\nFound 2 errors in 1 file (checked 1 source file)\n```\n\nRunning `pyright` would identify the same issues.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Type safe by design"
  },
  "a3621f65d5fee95b530274dca40aa1706101ef91f55c8b36a99b1729e9c84733": {
    "text": "System prompts might seem simple at first glance since they're just strings (or sequences of strings that are concatenated), but crafting the right system prompt is key to getting the model to behave as you want.\n\nTip\n\nFor most use cases, you should use `instructions` instead of \"system prompts\".\n\nIf you know what you are doing though and want to preserve system prompt messages in the message history sent to the\nLLM in subsequent completions requests, you can achieve this using the `system_prompt` argument/decorator.\n\nSee the section below on [Instructions](#instructions \"#instructions\") for more information.\n\nGenerally, system prompts fall into two categories:\n\n1. **Static system prompts**: These are known when writing the code and can be defined via the `system_prompt` parameter of the [`Agent` constructor](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\").\n2. **Dynamic system prompts**: These depend in some way on context that isn't known until runtime, and should be defined via functions decorated with [`@agent.system_prompt`](../api/agent/#pydantic_ai.agent.Agent.system_prompt \"../api/agent/#pydantic_ai.agent.Agent.system_prompt\").\n\nYou can add both to a single agent; they're appended in the order they're defined at runtime.\n\nHere's an example using both types of system prompts:\n\nsystem\\_prompts.py\n\n```\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=str,  # (1)!\n    system_prompt=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.system_prompt  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n```\n\n1. The agent expects a string dependency.\n2. Static system prompt defined at agent creation time.\n3. Dynamic system prompt defined via a decorator with [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\"), this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\n4. Another dynamic system prompt, system prompts don't have to have the `RunContext` parameter.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "System Prompts"
  },
  "8cb8a6cc4ae07082218bbe34914f8b3f3c48417005f55dc59eee435da4179b87": {
    "text": "Instructions are similar to system prompts. The main difference is that when an explicit `message_history` is provided\nin a call to `Agent.run` and similar methods, *instructions* from any existing messages in the history are not included\nin the request to the model — only the instructions of the *current* agent are included.\n\nYou should use:\n\n* `instructions` when you want your request to the model to only include system prompts for the *current* agent\n* `system_prompt` when you want your request to the model to *retain* the system prompts used in previous requests (possibly made using other agents)\n\nIn general, we recommend using `instructions` instead of `system_prompt` unless you have a specific reason to use `system_prompt`.\n\nInstructions, like system prompts, fall into two categories:\n\n1. **Static instructions**: These are known when writing the code and can be defined via the `instructions` parameter of the [`Agent` constructor](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\").\n2. **Dynamic instructions**: These rely on context that is only available at runtime and should be defined using functions decorated with [`@agent.instructions`](../api/agent/#pydantic_ai.agent.Agent.instructions \"../api/agent/#pydantic_ai.agent.Agent.instructions\"). Unlike dynamic system prompts, which may be reused when `message_history` is present, dynamic instructions are always reevaluated.\n\nBoth static and dynamic instructions can be added to a single agent, and they are appended in the order they are defined at runtime.\n\nHere's an example using both types of instructions:\n\ninstructions.py\n\n```\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=str,  # (1)!\n    instructions=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.instructions  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.instructions\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n```\n\n1. The agent expects a string dependency.\n2. Static instructions defined at agent creation time.\n3. Dynamic instructions defined via a decorator with [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\"),\n   this is called just after `run_sync`, not when the agent is created, so can benefit from runtime\n   information like the dependencies used on that run.\n4. Another dynamic instruction, instructions don't have to have the `RunContext` parameter.\n\n*(This example is complete, it can be run \"as is\")*\n\nNote that returning an empty string will result in no instruction message added.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Instructions"
  },
  "09ec2dad451dc6ae3ec3a5972620703231590d38536a15a5f0999b590bef50ba": {
    "text": "Validation errors from both function tool parameter validation and [structured output validation](../output/#structured-output \"../output/#structured-output\") can be passed back to the model with a request to retry.\n\nYou can also raise [`ModelRetry`](../api/exceptions/#pydantic_ai.exceptions.ModelRetry \"../api/exceptions/#pydantic_ai.exceptions.ModelRetry\") from within a [tool](../tools/ \"../tools/\") or [output validator function](../output/#output-validator-functions \"../output/#output-validator-functions\") to tell the model it should retry generating a response.\n\nHere's an example:\n\ntool\\_retry.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nfrom fake_database import DatabaseConn\n\n\nclass ChatResult(BaseModel):\n    user_id: int\n    message: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=DatabaseConn,\n    output_type=ChatResult,\n)\n\n\n@agent.tool(retries=2)\ndef get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n    \"\"\"Get a user's ID from their full name.\"\"\"\n    print(name)\n    #> John\n    #> John Doe\n    user_id = ctx.deps.users.get(name=name)\n    if user_id is None:\n        raise ModelRetry(\n            f'No user found with name {name!r}, remember to provide their full name'\n        )\n    return user_id\n\n\nresult = agent.run_sync(\n    'Send a message to John Doe asking for coffee next week', deps=DatabaseConn()\n)\nprint(result.output)\n\"\"\"\nuser_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\n\"\"\"\n```",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Reflection and self-correction"
  },
  "ddd6ca5aa96cb766133586a244f22f1b3aa333c75863c922ebc811aff5aa0ba1": {
    "text": "If models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns `503`), agent runs will raise [`UnexpectedModelBehavior`](../api/exceptions/#pydantic_ai.exceptions.UnexpectedModelBehavior \"../api/exceptions/#pydantic_ai.exceptions.UnexpectedModelBehavior\").\n\nIn these cases, [`capture_run_messages`](../api/agent/#pydantic_ai.agent.capture_run_messages \"../api/agent/#pydantic_ai.agent.capture_run_messages\") can be used to access the messages exchanged during the run to help diagnose the issue.\n\nagent\\_model\\_errors.py\n\n```\nfrom pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n\nagent = Agent('openai:gpt-4o')\n\n\n@agent.tool_plain\ndef calc_volume(size: int) -> int:  # (1)!\n    if size == 42:\n        return size**3\n    else:\n        raise ModelRetry('Please try again.')\n\n\nwith capture_run_messages() as messages:  # (2)!\n    try:\n        result = agent.run_sync('Please get me the volume of a box with size 6.')\n    except UnexpectedModelBehavior as e:\n        print('An error occurred:', e)\n        #> An error occurred: Tool 'calc_volume' exceeded max retries count of 1\n        print('cause:', repr(e.__cause__))\n        #> cause: ModelRetry('Please try again.')\n        print('messages:', messages)\n        \"\"\"\n        messages:\n        [\n            ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='Please get me the volume of a box with size 6.',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=62, output_tokens=4),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n            ModelRequest(\n                parts=[\n                    RetryPromptPart(\n                        content='Please try again.',\n                        tool_name='calc_volume',\n                        tool_call_id='pyd_ai_tool_call_id',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=72, output_tokens=8),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n    else:\n        print(result.output)\n```\n\n1. Define a tool that will raise `ModelRetry` repeatedly in this case.\n2. [`capture_run_messages`](../api/agent/#pydantic_ai.agent.capture_run_messages \"../api/agent/#pydantic_ai.agent.capture_run_messages\") is used to capture the messages exchanged during the run.\n\n*(This example is complete, it can be run \"as is\")*\n\nNote\n\nIf you call [`run`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\"), [`run_sync`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync\"), or [`run_stream`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\") more than once within a single `capture_run_messages` context, `messages` will represent the messages exchanged during the first call only.",
    "source_url": "https://ai.pydantic.dev/agents/",
    "header": "Model errors"
  },
  "786e1163365497cc76ec10407ac2fce65167fe50b009140e452369f38251152c": {
    "text": "Pydantic AI is model-agnostic and has built-in support for multiple model providers:",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "Model Providers"
  },
  "9c005325f948bb52cf6dccbb978b4ab3b73e7069c7dee48b57d5020fa2a7e848": {
    "text": "In addition, many providers are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI:\n\nPydantic AI also comes with [`TestModel`](../api/models/test/ \"../api/models/test/\") and [`FunctionModel`](../api/models/function/ \"../api/models/function/\")\nfor testing and development.\n\nTo use each model provider, you need to configure your local environment and make sure you have the right\npackages installed.",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "OpenAI-compatible Providers"
  },
  "4a88a53d093c5347d34e69530626f9947c25e450bd46885b8be72895250f6fd9": {
    "text": "Pydantic AI uses a few key terms to describe how it interacts with different LLMs:\n\n* **Model**: This refers to the Pydantic AI class used to make requests following a specific LLM API\n  (generally by wrapping a vendor-provided SDK, like the `openai` python SDK). These classes implement a\n  vendor-SDK-agnostic API, ensuring a single Pydantic AI agent is portable to different LLM vendors without\n  any other code changes just by swapping out the Model it uses. Model classes are named\n  roughly in the format `<VendorSdk>Model`, for example, we have `OpenAIChatModel`, `AnthropicModel`, `GoogleModel`,\n  etc. When using a Model class, you specify the actual LLM model name (e.g., `gpt-4o`,\n  `claude-3-5-sonnet-latest`, `gemini-1.5-flash`) as a parameter.\n* **Provider**: This refers to provider-specific classes which handle the authentication and connections\n  to an LLM vendor. Passing a non-default *Provider* as a parameter to a Model is how you can ensure\n  that your agent will make requests to a specific endpoint, or make use of a specific approach to\n  authentication (e.g., you can use Azure auth with the `OpenAIChatModel` by way of the `AzureProvider`).\n  In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility\n  with the vendor SDK used by an existing Model (such as `OpenAIChatModel`).\n* **Profile**: This refers to a description of how requests to a specific model or family of models need to be\n  constructed to get the best results, independent of the model and provider classes used.\n  For example, different models have different restrictions on the JSON schemas that can be used for tools,\n  and the same schema transformer needs to be used for Gemini models whether you're using `GoogleModel`\n  with model name `gemini-2.5-pro-preview`, or `OpenAIChatModel` with `OpenRouterProvider` and model name `google/gemini-2.5-pro-preview`.\n\nWhen you instantiate an [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\") with just a name formatted as `<provider>:<model>`, e.g. `openai:gpt-4o` or `openrouter:google/gemini-2.5-pro-preview`,\nPydantic AI will automatically select the appropriate model class, provider, and profile.\nIf you want to use a different provider or profile, you can instantiate a model class directly and pass in `provider` and/or `profile` arguments.",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "Models and Providers"
  },
  "0279dfd948949fffa83069ae106ca9ca3d2f0095392acc01b6e0a2fb9d71d817": {
    "text": "To implement support for a model API that's not already supported, you will need to subclass the [`Model`](../api/models/base/#pydantic_ai.models.Model \"../api/models/base/#pydantic_ai.models.Model\") abstract base class.\nFor streaming, you'll also need to implement the [`StreamedResponse`](../api/models/base/#pydantic_ai.models.StreamedResponse \"../api/models/base/#pydantic_ai.models.StreamedResponse\") abstract base class.\n\nThe best place to start is to review the source code for existing implementations, e.g. [`OpenAIChatModel`](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py \"https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py\").\n\nFor details on when we'll accept contributions adding new models to Pydantic AI, see the [contributing guidelines](../contributing/#new-model-rules \"../contributing/#new-model-rules\").\n\nIf a model API is compatible with the OpenAI API, you do not need a custom model class and can provide your own [custom provider](openai/#openai-compatible-models \"openai/#openai-compatible-models\") instead.",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "Custom Models"
  },
  "dde7b95bb9dee4a9ef03108f3bd2d8ec915952744454f09dc39379c9e3e19fcd": {
    "text": "You can use [`FallbackModel`](../api/models/fallback/#pydantic_ai.models.fallback.FallbackModel \"../api/models/fallback/#pydantic_ai.models.fallback.FallbackModel\") to attempt multiple models\nin sequence until one successfully returns a result. Under the hood, Pydantic AI automatically switches\nfrom one model to the next if the current model returns a 4xx or 5xx status code.\n\nIn the following example, the agent first makes a request to the OpenAI model (which fails due to an invalid API key),\nand then falls back to the Anthropic model.\n\nfallback\\_model.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nopenai_model = OpenAIChatModel('gpt-4o')\nanthropic_model = AnthropicModel('claude-3-5-sonnet-latest')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\nresponse = agent.run_sync('What is the capital of France?')\nprint(response.data)\n#> Paris\n\nprint(response.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='What is the capital of France?',\n                timestamp=datetime.datetime(...),\n                part_kind='user-prompt',\n            )\n        ],\n        kind='request',\n    ),\n    ModelResponse(\n        parts=[TextPart(content='Paris', part_kind='text')],\n        model_name='claude-3-5-sonnet-latest',\n        timestamp=datetime.datetime(...),\n        kind='response',\n        provider_response_id=None,\n    ),\n]\n\"\"\"\n```\n\nThe `ModelResponse` message above indicates in the `model_name` field that the output was returned by the Anthropic model, which is the second model specified in the `FallbackModel`.\n\nNote\n\nEach model's options should be configured individually. For example, `base_url`, `api_key`, and custom clients should be set on each model itself, not on the `FallbackModel`.",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "Fallback Model"
  },
  "59243a347be5dbc9d923a74ff02b24d660fd68de61252e57ba7f8e1e234c0e9a": {
    "text": "You can configure different [`ModelSettings`](../api/settings/#pydantic_ai.settings.ModelSettings \"../api/settings/#pydantic_ai.settings.ModelSettings\") for each model in a fallback chain by passing the `settings` parameter when creating each model. This is particularly useful when different providers have different optimal configurations:\n\nfallback\\_model\\_per\\_settings.py\n\n```\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "Per-Model Settings"
  },
  "fda55817adc2446950c19dd4454fb31059125413a3b0d6672552b370566a9f26": {
    "text": "openai_model = OpenAIChatModel(\n    'gpt-4o',\n    settings=ModelSettings(temperature=0.7, max_tokens=1000)  # Higher creativity for OpenAI\n)\nanthropic_model = AnthropicModel(\n    'claude-3-5-sonnet-latest',\n    settings=ModelSettings(temperature=0.2, max_tokens=1000)  # Lower temperature for consistency\n)\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\nagent = Agent(fallback_model)\n\nresult = agent.run_sync('Write a creative story about space exploration')\nprint(result.output)\n\"\"\"\nIn the year 2157, Captain Maya Chen piloted her spacecraft through the vast expanse of the Andromeda Galaxy. As she discovered a planet with crystalline mountains that sang in harmony with the cosmic winds, she realized that space exploration was not just about finding new worlds, but about finding new ways to understand the universe and our place within it.\n\"\"\"\n```\n\nIn this example, if the OpenAI model fails, the agent will automatically fall back to the Anthropic model with its own configured settings. The `FallbackModel` itself doesn't have settings - it uses the individual settings of whichever model successfully handles the request.\n\nIn this next example, we demonstrate the exception-handling capabilities of `FallbackModel`.\nIf all models fail, a [`FallbackExceptionGroup`](../api/exceptions/#pydantic_ai.exceptions.FallbackExceptionGroup \"../api/exceptions/#pydantic_ai.exceptions.FallbackExceptionGroup\") is raised, which\ncontains all the exceptions encountered during the `run` execution.\n\nBy default, the `FallbackModel` only moves on to the next model if the current model raises a\n[`ModelHTTPError`](../api/exceptions/#pydantic_ai.exceptions.ModelHTTPError \"../api/exceptions/#pydantic_ai.exceptions.ModelHTTPError\"). You can customize this behavior by\npassing a custom `fallback_on` argument to the `FallbackModel` constructor.",
    "source_url": "https://ai.pydantic.dev/models/",
    "header": "Configure each model with provider-specific optimal settings"
  },
  "17faad03d320da0eaef8170c3c10ffcd5fe171c24792e49cb5373560fb5aa0aa": {
    "text": "To use OpenAI models or OpenAI-compatible APIs, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `openai` optional group:",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Install"
  },
  "b295228fb4fd604c1ab1497c55268cfe02eef79ba1eedc133224608a8f14f36d": {
    "text": "To use `OpenAIChatModel` with the OpenAI API, go to [platform.openai.com](https://platform.openai.com/ \"https://platform.openai.com/\") and follow your nose until you find the place to generate an API key.",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Configuration"
  },
  "5a843d25585ede0e33dbefd0311c9c0a6088631e6a0bbf0eed04d957c54040f5": {
    "text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport OPENAI_API_KEY='your-api-key'\n```\n\nYou can then use `OpenAIChatModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nmodel = OpenAIChatModel('gpt-4o')\nagent = Agent(model)\n...\n```\n\nBy default, the `OpenAIChatModel` uses the `OpenAIProvider` with the `base_url` set to `https://api.openai.com/v1`.",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Environment variable"
  },
  "bc6f19c40d597380cc272dfee60a77af01144d75654ce8cf73c1caa343faabf3": {
    "text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the\n[OpenAIProvider](../../api/providers/#pydantic_ai.providers.openai.OpenAIProvider \"../../api/providers/#pydantic_ai.providers.openai.OpenAIProvider\") and pass it to the model:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Configure the provider"
  },
  "654659acb0fcb9953865d9629ddc92bebcbc6d76b810a0fa6fa677fc7d11c5ff": {
    "text": "`OpenAIProvider` also accepts a custom `AsyncOpenAI` client via the `openai_client` parameter, so you can customise the `organization`, `project`, `base_url` etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference \"https://platform.openai.com/docs/api-reference\").\n\ncustom\\_openai\\_client.py\n\n```\nfrom openai import AsyncOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncOpenAI(max_retries=3)\nmodel = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(openai_client=client))\nagent = Agent(model)\n...\n```\n\nYou could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints \"https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints\") client\nto use the Azure OpenAI API. Note that the `AsyncAzureOpenAI` is a subclass of `AsyncOpenAI`.\n\n```\nfrom openai import AsyncAzureOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncAzureOpenAI(\n    azure_endpoint='...',\n    api_version='2024-07-01-preview',\n    api_key='your-api-key',\n)\n\nmodel = OpenAIChatModel(\n    'gpt-4o',\n    provider=OpenAIProvider(openai_client=client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Custom OpenAI Client"
  },
  "62a7e7c5baba3ca8794af8cd81aebc7be48684deb0236f3bee0e2a09bd18667e": {
    "text": "Pydantic AI also supports OpenAI's [Responses API](https://platform.openai.com/docs/api-reference/responses \"https://platform.openai.com/docs/api-reference/responses\") through the\n[`OpenAIResponsesModel`](../../api/models/openai/#pydantic_ai.models.openai.OpenAIResponsesModel \"../../api/models/openai/#pydantic_ai.models.openai.OpenAIResponsesModel\") class.\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\nmodel = OpenAIResponsesModel('gpt-4o')\nagent = Agent(model)\n...\n```\n\nThe Responses API has built-in tools that you can use instead of building your own:\n\n* [Web search](https://platform.openai.com/docs/guides/tools-web-search \"https://platform.openai.com/docs/guides/tools-web-search\"): allow models to search the web for the latest information before generating a response.\n* [File search](https://platform.openai.com/docs/guides/tools-file-search \"https://platform.openai.com/docs/guides/tools-file-search\"): allow models to search your files for relevant information before generating a response.\n* [Computer use](https://platform.openai.com/docs/guides/tools-computer-use \"https://platform.openai.com/docs/guides/tools-computer-use\"): allow models to use a computer to perform tasks on your behalf.\n\nYou can use the `OpenAIResponsesModelSettings` class to make use of those built-in tools:\n\n```\nfrom openai.types.responses import WebSearchToolParam  # (1)!\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_builtin_tools=[WebSearchToolParam(type='web_search_preview')],\n)\nmodel = OpenAIResponsesModel('gpt-4o')\nagent = Agent(model=model, model_settings=model_settings)\n\nresult = agent.run_sync('What is the weather in Tokyo?')\nprint(result.output)\n\"\"\"\nAs of 7:48 AM on Wednesday, April 2, 2025, in Tokyo, Japan, the weather is cloudy with a temperature of 53°F (12°C).\n\"\"\"\n```\n\n1. The file search tool and computer use tool can also be imported from `openai.types.responses`.\n\nYou can learn more about the differences between the Responses API and Chat Completions API in the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions \"https://platform.openai.com/docs/guides/responses-vs-chat-completions\").",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "OpenAI Responses API"
  },
  "59b58998cb6cefbc2138daf9fff00a05cfe5bd01b128ab914e6e09aa2dcb6b4d": {
    "text": "Many providers and models are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI.\nBefore getting started, check the [installation and configuration](#install \"#install\") instructions above.\n\nTo use another OpenAI-compatible API, you can make use of the `base_url` and `api_key` arguments from `OpenAIProvider`:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n)\nagent = Agent(model)\n...\n```\n\nVarious providers also have their own provider classes so that you don't need to specify the base URL yourself and you can use the standard `<PROVIDER>_API_KEY` environment variable to set the API key.\nWhen a provider has its own provider class, you can use the `Agent(\"<provider>:<model>\")` shorthand, e.g. `Agent(\"deepseek:deepseek-chat\")` or `Agent(\"openrouter:google/gemini-2.5-pro-preview\")`, instead of building the `OpenAIChatModel` explicitly. Similarly, you can pass the provider name as a string to the `provider` argument on `OpenAIChatModel` instead of building instantiating the provider class explicitly.",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "OpenAI-compatible Models"
  },
  "84016db50edc7842c9da746cd185fe1631b54b9562e7af6123908494643c3723": {
    "text": "Sometimes, the provider or model you're using will have slightly different requirements than OpenAI's API or models, like having different restrictions on JSON schemas for tool definitions, or not supporting tool definitions to be marked as strict.\n\nWhen using an alternative provider class provided by Pydantic AI, an appropriate model profile is typically selected automatically based on the model name.\nIf the model you're using is not working correctly out of the box, you can tweak various aspects of how model requests are constructed by providing your own [`ModelProfile`](../../api/profiles/#pydantic_ai.profiles.ModelProfile \"../../api/profiles/#pydantic_ai.profiles.ModelProfile\") (for behaviors shared among all model classes) or [`OpenAIModelProfile`](../../api/profiles/#pydantic_ai.profiles.openai.OpenAIModelProfile \"../../api/profiles/#pydantic_ai.profiles.openai.OpenAIModelProfile\") (for behaviors specific to `OpenAIChatModel`):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.profiles import InlineDefsJsonSchemaTransformer\nfrom pydantic_ai.profiles.openai import OpenAIModelProfile\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n    profile=OpenAIModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,  # Supported by any model class on a plain ModelProfile\n        openai_supports_strict_tool_definition=False  # Supported by OpenAIModel only, requires OpenAIModelProfile\n    )\n)\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Model Profile"
  },
  "40143c2d69d7ee549d2d2002a68b720d3231e654040b572747b557b1f1bff04a": {
    "text": "To use the [DeepSeek](https://deepseek.com \"https://deepseek.com\") provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/ \"https://api-docs.deepseek.com/\").\nOnce you have the API key, you can use it with the [`DeepSeekProvider`](../../api/providers/#pydantic_ai.providers.deepseek.DeepSeekProvider \"../../api/providers/#pydantic_ai.providers.deepseek.DeepSeekProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(api_key='your-deepseek-api-key'),\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize any provider with a custom `http_client`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(\n        api_key='your-deepseek-api-key', http_client=custom_http_client\n    ),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "DeepSeek"
  },
  "6557f2e2221ec16df79121ea64dade9e3e36cdeafe0ff483bfa931e3b1b9cc69": {
    "text": "To use [Ollama](https://ollama.com/ \"https://ollama.com/\"), you must first download the Ollama client, and then download a model using the [Ollama model library](https://ollama.com/library \"https://ollama.com/library\").\n\nYou must also ensure the Ollama server is running when trying to make requests to it. For more information, please see the [Ollama documentation](https://github.com/ollama/ollama/tree/main/docs \"https://github.com/ollama/ollama/tree/main/docs\").\n\nYou can then use the model with the [`OllamaProvider`](../../api/providers/#pydantic_ai.providers.ollama.OllamaProvider \"../../api/providers/#pydantic_ai.providers.ollama.OllamaProvider\").",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Ollama"
  },
  "136646843a0ee6dde47b01df616ca8439480087694126989e92fd80ee525f1f3": {
    "text": "With `ollama` installed, you can run the server with the model you want to use:\n\n(this will pull the `llama3.2` model if you don't already have it downloaded)\n\nThen run your code, here's a minimal example:\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nollama_model = OpenAIChatModel(\n    model_name='llama3.2',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),\n)\nagent = Agent(ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Example local usage"
  },
  "d880ef57d529f5b19688f40b793a9f8610e360d8b251c4f387f01fabf4134742": {
    "text": "```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\nollama_model = OpenAIChatModel(\n    model_name='qwen2.5-coder:7b',  # (1)!\n    provider=OllamaProvider(base_url='http://192.168.1.74:11434/v1'),  # (2)!\n)\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nagent = Agent(model=ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n```\n\n1. The name of the model running on the remote server\n2. The url of the remote server",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Example using a remote server"
  },
  "30c25ad587b52b07e8bb7dbdc0da2429f8a8081aa0720131dac6342b2c1431d0": {
    "text": "If you want to use [Azure AI Foundry](https://ai.azure.com/ \"https://ai.azure.com/\") as your provider, you can do so by using the\n[`AzureProvider`](../../api/providers/#pydantic_ai.providers.azure.AzureProvider \"../../api/providers/#pydantic_ai.providers.azure.AzureProvider\") class.\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.azure import AzureProvider\n\nmodel = OpenAIChatModel(\n    'gpt-4o',\n    provider=AzureProvider(\n        azure_endpoint='your-azure-endpoint',\n        api_version='your-api-version',\n        api_key='your-api-key',\n    ),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Azure AI Foundry"
  },
  "2620e5fc250f4ada1e55d0d4454bdc33af23de7e8670715a8269513ac7113eb1": {
    "text": "To use [OpenRouter](https://openrouter.ai \"https://openrouter.ai\"), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys \"https://openrouter.ai/keys\").\n\nOnce you have the API key, you can use it with the [`OpenRouterProvider`](../../api/providers/#pydantic_ai.providers.openrouter.OpenRouterProvider \"../../api/providers/#pydantic_ai.providers.openrouter.OpenRouterProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openrouter import OpenRouterProvider\n\nmodel = OpenAIChatModel(\n    'anthropic/claude-3.5-sonnet',\n    provider=OpenRouterProvider(api_key='your-openrouter-api-key'),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "OpenRouter"
  },
  "f1ba530351344b42dadf73cb7e38488d6334ab9f26332329896b8572047b07c2": {
    "text": "To use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway \"https://vercel.com/docs/ai-gateway\"), first follow the [documentation](https://vercel.com/docs/ai-gateway \"https://vercel.com/docs/ai-gateway\") instructions on obtaining an API key or OIDC token.\n\nYou can set your credentials using one of these environment variables:\n\n```\nexport VERCEL_AI_GATEWAY_API_KEY='your-ai-gateway-api-key'",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Vercel AI Gateway"
  },
  "d59253f3361c0b88ad45b47053a24b361f5610d7b10d372cba7474bc378c92ab": {
    "text": "export VERCEL_OIDC_TOKEN='your-oidc-token'\n```\n\nOnce you have set the environment variable, you can use it with the [`VercelProvider`](../../api/providers/#pydantic_ai.providers.vercel.VercelProvider \"../../api/providers/#pydantic_ai.providers.vercel.VercelProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.vercel import VercelProvider",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "OR"
  },
  "534d802ca99fd3c4ede773df4ab57eb49143ca51b0d29dcb1a1076a146e5bbec": {
    "text": "model = OpenAIChatModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(),\n)\nagent = Agent(model)",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Uses environment variable automatically"
  },
  "76a0a8da76013eb0f91d139963391aed9fde351eb2054dea9e7b577104fef718": {
    "text": "model = OpenAIChatModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(api_key='your-vercel-ai-gateway-api-key'),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Or pass the API key directly"
  },
  "0fddfb993bf3284c0e58f471635cebf5a650ecd57f5d15377b3930ea22cb1d1d": {
    "text": "Go to [xAI API Console](https://console.x.ai/ \"https://console.x.ai/\") and create an API key.\nOnce you have the API key, you can use it with the [`GrokProvider`](../../api/providers/#pydantic_ai.providers.grok.GrokProvider \"../../api/providers/#pydantic_ai.providers.grok.GrokProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.grok import GrokProvider\n\nmodel = OpenAIChatModel(\n    'grok-2-1212',\n    provider=GrokProvider(api_key='your-xai-api-key'),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Grok (xAI)"
  },
  "60682041d27aae42b8aee85eff5fd77415fd0decf7f0303b7f48cf9c30f77826": {
    "text": "Create an API key in the [Moonshot Console](https://platform.moonshot.ai/console \"https://platform.moonshot.ai/console\").\nWith that key you can instantiate the [`MoonshotAIProvider`](../../api/providers/#pydantic_ai.providers.moonshotai.MoonshotAIProvider \"../../api/providers/#pydantic_ai.providers.moonshotai.MoonshotAIProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.moonshotai import MoonshotAIProvider\n\nmodel = OpenAIChatModel(\n    'kimi-k2-0711-preview',\n    provider=MoonshotAIProvider(api_key='your-moonshot-api-key'),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "MoonshotAI"
  },
  "b202bc6b37848fd49f2e74bdb406befdce42e9bc08911ac53511dbd571bd9b2c": {
    "text": "To use [GitHub Models](https://docs.github.com/en/github-models \"https://docs.github.com/en/github-models\"), you'll need a GitHub personal access token with the `models: read` permission.\n\nOnce you have the token, you can use it with the [`GitHubProvider`](../../api/providers/#pydantic_ai.providers.github.GitHubProvider \"../../api/providers/#pydantic_ai.providers.github.GitHubProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.github import GitHubProvider\n\nmodel = OpenAIChatModel(\n    'xai/grok-3-mini',  # GitHub Models uses prefixed model names\n    provider=GitHubProvider(api_key='your-github-token'),\n)\nagent = Agent(model)\n...\n```\n\nYou can also set the `GITHUB_API_KEY` environment variable:\n\n```\nexport GITHUB_API_KEY='your-github-token'\n```\n\nGitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models \"https://github.com/marketplace?type=models\") or the public [catalog endpoint](https://models.github.ai/catalog/models \"https://models.github.ai/catalog/models\").",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "GitHub Models"
  },
  "bc42e308f293f6ecd07d4930cf3c11e8f2f7fe68daa91900705ef1cbb402e301": {
    "text": "Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started \"https://docs.perplexity.ai/guides/getting-started\")\nguide to create an API key. Then, you can query the Perplexity API with the following:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key='your-perplexity-api-key',\n    ),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Perplexity"
  },
  "1a85676c7eb690095f851e488509bcaa8d538027e2df59553725cb71424c7e78": {
    "text": "Go to [Fireworks.AI](https://fireworks.ai/ \"https://fireworks.ai/\") and create an API key in your account settings.\nOnce you have the API key, you can use it with the [`FireworksProvider`](../../api/providers/#pydantic_ai.providers.fireworks.FireworksProvider \"../../api/providers/#pydantic_ai.providers.fireworks.FireworksProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.fireworks import FireworksProvider\n\nmodel = OpenAIChatModel(\n    'accounts/fireworks/models/qwq-32b',  # model library available at https://fireworks.ai/models\n    provider=FireworksProvider(api_key='your-fireworks-api-key'),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Fireworks AI"
  },
  "c033212cdd1d8622633d56eb37d705be382db703788c628a5976aea4d3457c36": {
    "text": "Go to [Together.ai](https://www.together.ai/ \"https://www.together.ai/\") and create an API key in your account settings.\nOnce you have the API key, you can use it with the [`TogetherProvider`](../../api/providers/#pydantic_ai.providers.together.TogetherProvider \"../../api/providers/#pydantic_ai.providers.together.TogetherProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.together import TogetherProvider\n\nmodel = OpenAIChatModel(\n    'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',  # model library available at https://www.together.ai/models\n    provider=TogetherProvider(api_key='your-together-api-key'),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Together AI"
  },
  "726625dc78a7cb5b88bf23ac27e80e6c2edee7d6fcd0965d97b1967e58cb3ec8": {
    "text": "To use [Heroku AI](https://www.heroku.com/ai \"https://www.heroku.com/ai\"), you can use the [`HerokuProvider`](../../api/providers/#pydantic_ai.providers.heroku.HerokuProvider \"../../api/providers/#pydantic_ai.providers.heroku.HerokuProvider\"):\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.heroku import HerokuProvider\n\nmodel = OpenAIChatModel(\n    'claude-3-7-sonnet',\n    provider=HerokuProvider(api_key='your-heroku-inference-key'),\n)\nagent = Agent(model)\n...\n```\n\nYou can set the `HEROKU_INFERENCE_KEY` and `HEROKU_INFERENCE_URL` environment variables to set the API key and base URL, respectively:\n\n```\nexport HEROKU_INFERENCE_KEY='your-heroku-inference-key'\nexport HEROKU_INFERENCE_URL='https://us.inference.heroku.com'\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Heroku AI"
  },
  "3c4d62eec98c9b7949acd335e70fd9104f2035dd01c93d3346e3892c34f2f2f6": {
    "text": "To use [Cerebras](https://cerebras.ai/ \"https://cerebras.ai/\"), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/ \"https://cloud.cerebras.ai/\").\n\nOnce you've set the `CEREBRAS_API_KEY` environment variable, you can run the following:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('cerebras:llama3.3-70b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nIf you need to configure the provider, you can use the [`CerebrasProvider`](../../api/providers/#pydantic_ai.providers.cerebras.CerebrasProvider \"../../api/providers/#pydantic_ai.providers.cerebras.CerebrasProvider\") class:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.cerebras import CerebrasProvider\n\nmodel = OpenAIChatModel(\n    'llama3.3-70b',\n    provider=CerebrasProvider(api_key='your-cerebras-api-key'),\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```",
    "source_url": "https://ai.pydantic.dev/models/openai/",
    "header": "Cerebras"
  },
  "2caa51d60a87530b716aa696359fee93e01303a7a7c1155db9be7beb4a06a631": {
    "text": "To use `AnthropicModel` models, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `anthropic` optional group:",
    "source_url": "https://ai.pydantic.dev/models/anthropic/",
    "header": "Install"
  },
  "44d84561f48b1c67f2b32197261a616e2345b4fc803b5ce9b77eaaa51a971f2d": {
    "text": "To use [Anthropic](https://anthropic.com \"https://anthropic.com\") through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys \"https://console.anthropic.com/settings/keys\") to generate an API key.\n\n`AnthropicModelName` contains a list of available Anthropic models.",
    "source_url": "https://ai.pydantic.dev/models/anthropic/",
    "header": "Configuration"
  },
  "da4f116b1a06ca00c9e48159c69a3b6af501631df17f95bc88816844521757f8": {
    "text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport ANTHROPIC_API_KEY='your-api-key'\n```\n\nYou can then use `AnthropicModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\n\nmodel = AnthropicModel('claude-3-5-sonnet-latest')\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/anthropic/",
    "header": "Environment variable"
  },
  "a33cd04e7bb47537b1e04bf9e4a79ab49269bcc883b68ddba4d6c1d37f391ce0": {
    "text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest', provider=AnthropicProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/anthropic/",
    "header": "`provider` argument"
  },
  "d011d2ca5295a670a30b1d4b3c56da54051ef8b061d45c938636313476565510": {
    "text": "You can customize the `AnthropicProvider` with a custom `httpx.AsyncClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest',\n    provider=AnthropicProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/anthropic/",
    "header": "Custom HTTP Client"
  },
  "a779abc4c160e11f4ed4b9d22427be9f8e6569f9e2d626bd640a8589fd38eaab": {
    "text": "The `GoogleModel` is a model that uses the [`google-genai`](https://pypi.org/project/google-genai/ \"https://pypi.org/project/google-genai/\") package under the hood to\naccess Google's Gemini models via both the Generative Language API and Vertex AI.",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Google"
  },
  "04fb2dc9b1a30e645460c2952b3e2ee1cc4dc5e19b98ff74e37cd33663a728cc": {
    "text": "To use `GoogleModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `google` optional group:\n\n---",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Install"
  },
  "e1b161cf7c9d8da3936ea80ba7170a859a8cf09fe3882507d2b5365f7404d79b": {
    "text": "`GoogleModel` lets you use Google's Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods \"https://ai.google.dev/api/all-methods\") (`generativelanguage.googleapis.com`) or [Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models \"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models\") (`*-aiplatform.googleapis.com`).",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Configuration"
  },
  "71a70e2a2030f630c35a43aa93a05da1bcdb2d821f68a381bc1ea112e47c4dac": {
    "text": "To use Gemini via the Generative Language API, go to [aistudio.google.com](https://aistudio.google.com/apikey \"https://aistudio.google.com/apikey\") and create an API key.\n\nOnce you have the API key, set it as an environment variable:\n\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\nYou can then use `GoogleModel` by explicitly creating a provider:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(api_key='your-api-key')\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "API Key (Generative Language API)"
  },
  "60b56d16b904310899caee22ce170e5bbd45ce3c76d953722179c7a82b916e62": {
    "text": "If you are an enterprise user, you can use the `google-vertex` provider with `GoogleModel` to access Gemini via Vertex AI.\n\nThis interface has a number of advantages over the Generative Language API:\n\n1. The VertexAI API comes with more enterprise readiness guarantees.\n2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput \"https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput\") with VertexAI to guarantee capacity.\n3. If you're running Pydantic AI inside GCP, you don't need to set up authentication, it should \"just work\".\n4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency.\n\nThe big disadvantage is that for local development you may need to create and configure a \"service account\", which can be challenging to get right.\n\nWhichever way you authenticate, you'll need to have VertexAI enabled in your GCP account.\n\nTo use Vertex AI, you may need to set up [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials \"https://cloud.google.com/docs/authentication/application-default-credentials\") or use a service account. You can also specify the region.",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Vertex AI (Enterprise/Cloud)"
  },
  "30fb362ce12be3935371cdf8f781b0b95786532eac779fde230ea78a1f9e6333": {
    "text": "If you have the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud \"https://cloud.google.com/sdk/gcloud\") installed and configured, you can use:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True)\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Application Default Credentials"
  },
  "f551439efacbbd046a68b3677e5d8c7cd91ee30ef18a9231c0a5ed725c9333a5": {
    "text": "To use a service account JSON file:\n\ngoogle\\_model\\_service\\_account.py\n\n```\nfrom google.oauth2 import service_account\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\ncredentials = service_account.Credentials.from_service_account_file(\n    'path/to/service-account.json',\n    scopes=['https://www.googleapis.com/auth/cloud-platform'],\n)\nprovider = GoogleProvider(credentials=credentials)\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Service Account"
  },
  "865882e651adb67d15e67c59c5fdf1e60a0879dce9447e9a6c9449fa90a745c8": {
    "text": "You can specify the location when using Vertex AI:\n\ngoogle\\_model\\_location.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, location='asia-east1')\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Customizing Location"
  },
  "d04b95634801cb8dc7e4d5373a0bfa0ea724b03f142c087ebafe2729f92d12b4": {
    "text": "You can supply a custom `GoogleProvider` instance using the `provider` argument to configure advanced client options, such as setting a custom `base_url`.\n\nThis is useful if you're using a custom-compatible endpoint with the Google Generative Language API.\n\n```\nfrom google.genai import Client\nfrom google.genai.types import HttpOptions\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nclient = Client(\n    api_key='gemini-custom-api-key',\n    http_options=HttpOptions(base_url='gemini-custom-base-url'),\n)\nprovider = GoogleProvider(client=client)\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Provider Argument"
  },
  "a7c723bf821cb999ef301a90d4290145e316c565453a876ab4531dca9b07b52d": {
    "text": "You can customize model behavior using [`GoogleModelSettings`](../../api/models/google/#pydantic_ai.models.google.GoogleModelSettings \"../../api/models/google/#pydantic_ai.models.google.GoogleModelSettings\"):\n\n```\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nsettings = GoogleModelSettings(\n    temperature=0.2,\n    max_tokens=1024,\n    google_thinking_config={'thinking_budget': 2048},\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-1.5-flash')\nagent = Agent(model, model_settings=settings)\n...\n```\n\nSee the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings \"https://ai.google.dev/gemini-api/docs/safety-settings\") for more on safety settings, and [thinking config](https://ai.google.dev/gemini-api/docs/thinking \"https://ai.google.dev/gemini-api/docs/thinking\").",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Model Settings"
  },
  "b633dd3ceeda2f4ef49afb4ca1f4060b9104854d418d4ff0a5a5b426c4d134f3": {
    "text": "`GoogleModel` supports multi-modal input, including documents, images, audio, and video. See the [input documentation](../../input/ \"../../input/\") for details and examples.",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Document, Image, Audio, and Video Input"
  },
  "e66cefc990c334856d0dfed7fda886c14ff3c6764d8642b39801235c4ff8ee9d": {
    "text": "You can use the [`GoogleModelSettings`](../../api/models/google/#pydantic_ai.models.google.GoogleModelSettings \"../../api/models/google/#pydantic_ai.models.google.GoogleModelSettings\") class to customize the model request.",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Model settings"
  },
  "a7eecaaf5f7a914e07660cb37d4ad7e76466d719ad9ec0716c5671f38ca03844": {
    "text": "You can disable thinking by setting the `thinking_budget` to `0` on the `google_thinking_config`:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(google_thinking_config={'thinking_budget': 0})\nmodel = GoogleModel('gemini-2.0-flash')\nagent = Agent(model, model_settings=model_settings)\n...\n```\n\nCheck out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking \"https://ai.google.dev/gemini-api/docs/thinking\") for more on thinking.",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Disable thinking"
  },
  "d8dea73e409f58b744607d4297df44ea91fb1fb032e02357b253ff0ca52accdb": {
    "text": "You can customize the safety settings by setting the `google_safety_settings` field.\n\n```\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-2.0-flash')\nagent = Agent(model, model_settings=model_settings)\n...\n```\n\nSee the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings \"https://ai.google.dev/gemini-api/docs/safety-settings\") for more on safety settings.",
    "source_url": "https://ai.pydantic.dev/models/google/",
    "header": "Safety settings"
  },
  "d70d945b0acc0aa281e5d1b2e0680a059c23fab9f888b180685076ad1619643b": {
    "text": "To use `BedrockConverseModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `bedrock` optional group:",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Install"
  },
  "1dd8c1db99bdbe934c406efd5bbcde676a40bc211350b521d30b197d47389c47": {
    "text": "To use [AWS Bedrock](https://aws.amazon.com/bedrock/ \"https://aws.amazon.com/bedrock/\"), you'll need an AWS account with Bedrock enabled and appropriate credentials. You can use either AWS credentials directly or a pre-configured boto3 client.\n\n`BedrockModelName` contains a list of available Bedrock models, including models from Anthropic, Amazon, Cohere, Meta, and Mistral.",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Configuration"
  },
  "6d6240119a12332a9a21ec632e43082f1442e8f4351deba50ba95848fde3a6c3": {
    "text": "You can set your AWS credentials as environment variables ([among other options](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables \"https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables\")):\n\n```\nexport AWS_BEARER_TOKEN_BEDROCK='your-api-key'",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Environment variables"
  },
  "85e2ef50072c9f76fd2f776e9585ef2f8f670a19887290a7520d9a15f4bef593": {
    "text": "export AWS_ACCESS_KEY_ID='your-access-key'\nexport AWS_SECRET_ACCESS_KEY='your-secret-key'\nexport AWS_DEFAULT_REGION='us-east-1'  # or your preferred region\n```\n\nYou can then use `BedrockConverseModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('bedrock:anthropic.claude-3-sonnet-20240229-v1:0')\n...\n```\n\nOr initialize the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel('anthropic.claude-3-sonnet-20240229-v1:0')\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "or:"
  },
  "e363a66a8f2f40ed3db72d230d453f123aa8252635f4c4b75ac1e248d243e5cd": {
    "text": "You can customize the Bedrock Runtime API calls by adding additional parameters, such as [guardrail\nconfigurations](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html \"https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html\") and [performance settings](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html \"https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html\"). For a complete list of configurable parameters, refer to the\ndocumentation for [`BedrockModelSettings`](../../api/models/bedrock/#pydantic_ai.models.bedrock.BedrockModelSettings \"../../api/models/bedrock/#pydantic_ai.models.bedrock.BedrockModelSettings\").\n\ncustomize\\_bedrock\\_model\\_settings.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Customizing Bedrock Runtime API"
  },
  "4c245b0d596c846c6bc519afa59e13938866eb045e13a982a53e8cc5c053f7f5": {
    "text": "bedrock_model_settings = BedrockModelSettings(\n    bedrock_guardrail_config={\n        'guardrailIdentifier': 'v1',\n        'guardrailVersion': 'v1',\n        'trace': 'enabled'\n    },\n    bedrock_performance_configuration={\n        'latency': 'optimized'\n    }\n)\n\n\nmodel = BedrockConverseModel(model_name='us.amazon.nova-pro-v1:0')\n\nagent = Agent(model=model, model_settings=bedrock_model_settings)\n```",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Define Bedrock model settings with guardrail and performance configurations"
  },
  "c088a36e87cbf7eed136423398525d61a03c38508c7cb5cf9a7294d582008ce2": {
    "text": "You can provide a custom `BedrockProvider` via the `provider` argument. This is useful when you want to specify credentials directly or use a custom boto3 client:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "`provider` argument"
  },
  "bb1fcb705c74eebf50b353c6e6adf213536be9d2398fc0559ea621e71041fec4": {
    "text": "model = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(\n        region_name='us-east-1',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key',\n    ),\n)\nagent = Agent(model)\n...\n```\n\nYou can also pass a pre-configured boto3 client:\n\n```\nimport boto3\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Using AWS credentials directly"
  },
  "4737ed8c89f53ddf541a46287703bb5ba28dc0688d1d27a580824b12d9e79ae4": {
    "text": "bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\nmodel = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(bedrock_client=bedrock_client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/bedrock/",
    "header": "Using a pre-configured boto3 client"
  },
  "5b542bc76e932d0fc323a714021e79356f5f86109ea3cf9b1107ad9a7b76c962": {
    "text": "To use `CohereModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `cohere` optional group:",
    "source_url": "https://ai.pydantic.dev/models/cohere/",
    "header": "Install"
  },
  "c6d6627251d06975886f6550e16c88b28c95fcc308138d4519d37e057bf46bcb": {
    "text": "To use [Cohere](https://cohere.com/ \"https://cohere.com/\") through their API, go to [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys \"https://dashboard.cohere.com/api-keys\") and follow your nose until you find the place to generate an API key.\n\n`CohereModelName` contains a list of the most popular Cohere models.",
    "source_url": "https://ai.pydantic.dev/models/cohere/",
    "header": "Configuration"
  },
  "6f57538eb8de20589f84f4a48b5c0b806e99f3c7e9a98649882680a8f939e086": {
    "text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport CO_API_KEY='your-api-key'\n```\n\nYou can then use `CohereModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('cohere:command')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\n\nmodel = CohereModel('command')\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/cohere/",
    "header": "Environment variable"
  },
  "105cc36a41a9bc564ed0efaba044c0e8d9b1626ff85484d578d4f2319b6c53aa": {
    "text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\nmodel = CohereModel('command', provider=CohereProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n```\n\nYou can also customize the `CohereProvider` with a custom `http_client`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = CohereModel(\n    'command',\n    provider=CohereProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/cohere/",
    "header": "`provider` argument"
  },
  "e23f0a7087df81011fa51ece670f5b4242be2c11967f349067157179895794c4": {
    "text": "To use `GroqModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `groq` optional group:",
    "source_url": "https://ai.pydantic.dev/models/groq/",
    "header": "Install"
  },
  "97477a065406b7eb2ca0c9c4c0c81db6a41f766860c7c697e095e9b496216235": {
    "text": "To use [Groq](https://groq.com/ \"https://groq.com/\") through their API, go to [console.groq.com/keys](https://console.groq.com/keys \"https://console.groq.com/keys\") and follow your nose until you find the place to generate an API key.\n\n`GroqModelName` contains a list of available Groq models.",
    "source_url": "https://ai.pydantic.dev/models/groq/",
    "header": "Configuration"
  },
  "cb5a59fa96c6f53423b3e974b542be8a7f681d50369fd6cabf0d2ad5d7571c09": {
    "text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport GROQ_API_KEY='your-api-key'\n```\n\nYou can then use `GroqModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('groq:llama-3.3-70b-versatile')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\n\nmodel = GroqModel('llama-3.3-70b-versatile')\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/groq/",
    "header": "Environment variable"
  },
  "0dcfe7e8b370f96006fb71066b68301bcf09d70e8103da919677fea123ca8e6b": {
    "text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\nmodel = GroqModel(\n    'llama-3.3-70b-versatile', provider=GroqProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize the `GroqProvider` with a custom `httpx.AsyncHTTPClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = GroqModel(\n    'llama-3.3-70b-versatile',\n    provider=GroqProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/groq/",
    "header": "`provider` argument"
  },
  "c2b91c7b1801e5cf6b784af1268f1669523eb15db9e4dea886dbddc9f6eff534": {
    "text": "To use `MistralModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `mistral` optional group:",
    "source_url": "https://ai.pydantic.dev/models/mistral/",
    "header": "Install"
  },
  "33d73295bf6cb4d29a5f188aa576c64bf269e184b4acb8e0e703ad2deb80f966": {
    "text": "To use [Mistral](https://mistral.ai \"https://mistral.ai\") through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/ \"https://console.mistral.ai/api-keys/\") and follow your nose until you find the place to generate an API key.\n\n`LatestMistralModelNames` contains a list of the most popular Mistral models.",
    "source_url": "https://ai.pydantic.dev/models/mistral/",
    "header": "Configuration"
  },
  "e126b99720b741b2e2de6b2c057390fcae28e7e11fbc578cbe0c0de792360ce2": {
    "text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport MISTRAL_API_KEY='your-api-key'\n```\n\nYou can then use `MistralModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('mistral:mistral-large-latest')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\n\nmodel = MistralModel('mistral-small-latest')\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/mistral/",
    "header": "Environment variable"
  },
  "f0221ab548ce788fa54e86968a24975d7ea9bc619c80bff49adbbde460dd3bf1": {
    "text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\nmodel = MistralModel(\n    'mistral-large-latest', provider=MistralProvider(api_key='your-api-key', base_url='https://<mistral-provider-endpoint>')\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize the provider with a custom `httpx.AsyncHTTPClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = MistralModel(\n    'mistral-large-latest',\n    provider=MistralProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/mistral/",
    "header": "`provider` argument"
  },
  "6069d6588f7a5dc99440fe50fb4c08554e9ff59dbabb5fd805cf336267e71cf5": {
    "text": "[Hugging Face](https://huggingface.co/ \"https://huggingface.co/\") is an AI platform with all major open source models, datasets, MCPs, and demos. You can use [Inference Providers](https://huggingface.co/docs/inference-providers \"https://huggingface.co/docs/inference-providers\") to run open source models like DeepSeek R1 on scalable serverless infrastructure.",
    "source_url": "https://ai.pydantic.dev/models/huggingface/",
    "header": "Hugging Face"
  },
  "f255b476cf74f0541455c42725c2c166872449498b774900f296edb315bdfefb": {
    "text": "To use `HuggingFaceModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `huggingface` optional group:",
    "source_url": "https://ai.pydantic.dev/models/huggingface/",
    "header": "Install"
  },
  "8e12fcd1a36aeb66035185cbd70c72b7085033d53d724ac97a72c8e0dce0a416": {
    "text": "To use [Hugging Face](https://huggingface.co/ \"https://huggingface.co/\") inference, you'll need to set up an account which will give you [free tier](https://huggingface.co/docs/inference-providers/pricing \"https://huggingface.co/docs/inference-providers/pricing\") allowance on [Inference Providers](https://huggingface.co/docs/inference-providers \"https://huggingface.co/docs/inference-providers\"). To setup inference, follow these steps:\n\n1. Go to [Hugging Face](https://huggingface.co/join \"https://huggingface.co/join\") and sign up for an account.\n2. Create a new access token in [Hugging Face](https://huggingface.co/settings/tokens \"https://huggingface.co/settings/tokens\").\n3. Set the `HF_TOKEN` environment variable to the token you just created.\n\nOnce you have a Hugging Face access token, you can set it as an environment variable:\n\n```\nexport HF_TOKEN='hf_token'\n```",
    "source_url": "https://ai.pydantic.dev/models/huggingface/",
    "header": "Configuration"
  },
  "899399f9ae51a51e72323ffed9c51e5d3e6e5f4ebadfb982616ae8afdfe6435c": {
    "text": "You can then use [`HuggingFaceModel`](../../api/models/huggingface/#pydantic_ai.models.huggingface.HuggingFaceModel \"../../api/models/huggingface/#pydantic_ai.models.huggingface.HuggingFaceModel\") by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('huggingface:Qwen/Qwen3-235B-A22B')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B')\nagent = Agent(model)\n...\n```\n\nBy default, the [`HuggingFaceModel`](../../api/models/huggingface/#pydantic_ai.models.huggingface.HuggingFaceModel \"../../api/models/huggingface/#pydantic_ai.models.huggingface.HuggingFaceModel\") uses the\n[`HuggingFaceProvider`](../../api/providers/#pydantic_ai.providers.huggingface.HuggingFaceProvider \"../../api/providers/#pydantic_ai.providers.huggingface.HuggingFaceProvider\") that will select automatically\nthe first of the inference providers (Cerebras, Together AI, Cohere..etc) available for the model, sorted by your\npreferred order in https://hf.co/settings/inference-providers.",
    "source_url": "https://ai.pydantic.dev/models/huggingface/",
    "header": "Usage"
  },
  "38b5c00183b74625bb2f594bdae9eabef76b93fc4e9e6e46a13a8bc4dacf36f3": {
    "text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the\n[`HuggingFaceProvider`](../../api/providers/#pydantic_ai.providers.huggingface.HuggingFaceProvider \"../../api/providers/#pydantic_ai.providers.huggingface.HuggingFaceProvider\") and pass it to the model:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B', provider=HuggingFaceProvider(api_key='hf_token', provider_name='nebius'))\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/huggingface/",
    "header": "Configure the provider"
  },
  "f97ff2fb80b1135573ff06cf110860be089ad6ce70bcdfdbf2574f0855280cbc": {
    "text": "[`HuggingFaceProvider`](../../api/providers/#pydantic_ai.providers.huggingface.HuggingFaceProvider \"../../api/providers/#pydantic_ai.providers.huggingface.HuggingFaceProvider\") also accepts a custom\n[`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient \"https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient\") client via the `hf_client` parameter, so you can customise\nthe `headers`, `bill_to` (billing to an HF organization you're a member of), `base_url` etc. as defined in the\n[Hugging Face Hub python library docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client \"https://huggingface.co/docs/huggingface_hub/package_reference/inference_client\").\n\n```\nfrom huggingface_hub import AsyncInferenceClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nclient = AsyncInferenceClient(\n    bill_to='openai',\n    api_key='hf_token',\n    provider='fireworks-ai',\n)\n\nmodel = HuggingFaceModel(\n    'Qwen/Qwen3-235B-A22B',\n    provider=HuggingFaceProvider(hf_client=client),\n)\nagent = Agent(model)\n...\n```",
    "source_url": "https://ai.pydantic.dev/models/huggingface/",
    "header": "Custom Hugging Face client"
  },
  "27d96efd08cb306ebf96d442d46ab75489405965a038338d092722d6667b1eb9": {
    "text": "Pydantic AI uses a dependency injection system to provide data and services to your agent's [system prompts](../agents/#system-prompts \"../agents/#system-prompts\"), [tools](../tools/ \"../tools/\") and [output validators](../output/#output-validator-functions \"../output/#output-validator-functions\").\n\nMatching Pydantic AI's design philosophy, our dependency system tries to use existing best practice in Python development rather than inventing esoteric \"magic\", this should make dependencies type-safe, understandable easier to test and ultimately easier to deploy in production.",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Dependencies"
  },
  "1b9b05f7a66ba2438870b3027a710057af966f08a9c35f136f31c76776a98f48": {
    "text": "Dependencies can be any python type. While in simple cases you might be able to pass a single object as a dependency (e.g. an HTTP connection), [dataclasses](https://docs.python.org/3/library/dataclasses.html#module-dataclasses \"https://docs.python.org/3/library/dataclasses.html#module-dataclasses\") are generally a convenient container when your dependencies included multiple objects.\n\nHere's an example of defining an agent that requires dependencies.\n\n(**Note:** dependencies aren't actually used in this example, see [Accessing Dependencies](#accessing-dependencies \"#accessing-dependencies\") below)\n\nunused\\_dependencies.py\n\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent\n\n\n@dataclass\nclass MyDeps:  # (1)!\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,  # (2)!\n)\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run(\n            'Tell me a joke.',\n            deps=deps,  # (3)!\n        )\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. Define a dataclass to hold dependencies.\n2. Pass the dataclass type to the `deps_type` argument of the [`Agent` constructor](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\"). **Note**: we're passing the type here, NOT an instance, this parameter is not actually used at runtime, it's here so we can get full type checking of the agent.\n3. When running the agent, pass an instance of the dataclass to the `deps` parameter.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Defining Dependencies"
  },
  "0cc8c6ce320c9d2d3e8102cfa90ce7bd70af17caa106e457b2c51a287581d0cb": {
    "text": "Dependencies are accessed through the [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") type, this should be the first parameter of system prompt functions etc.\n\nsystem\\_prompt\\_dependencies.py\n\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt  # (1)!\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)!\n    response = await ctx.deps.http_client.get(  # (3)!\n        'https://example.com',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},  # (4)!\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") may optionally be passed to a [`system_prompt`](../api/agent/#pydantic_ai.agent.Agent.system_prompt \"../api/agent/#pydantic_ai.agent.Agent.system_prompt\") function as the only argument.\n2. [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") is parameterized with the type of the dependencies, if this type is incorrect, static type checkers will raise an error.\n3. Access dependencies through the [`.deps`](../api/tools/#pydantic_ai.tools.RunContext.deps \"../api/tools/#pydantic_ai.tools.RunContext.deps\") attribute.\n4. Access dependencies through the [`.deps`](../api/tools/#pydantic_ai.tools.RunContext.deps \"../api/tools/#pydantic_ai.tools.RunContext.deps\") attribute.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Accessing Dependencies"
  },
  "b450386fe48040c1596776ebb9d19827fe71b58682d07a0adfaf29705497dd06": {
    "text": "[System prompt functions](../agents/#system-prompts \"../agents/#system-prompts\"), [function tools](../tools/ \"../tools/\") and [output validators](../output/#output-validator-functions \"../output/#output-validator-functions\") are all run in the async context of an agent run.\n\nIf these functions are not coroutines (e.g. `async def`) they are called with\n[`run_in_executor`](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor \"https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor\") in a thread pool, it's therefore marginally preferable\nto use `async` methods where dependencies perform IO, although synchronous dependencies should work fine too.\n\n`run` vs. `run_sync` and Asynchronous vs. Synchronous dependencies\n\nWhether you use synchronous or asynchronous dependencies, is completely independent of whether you use `run` or `run_sync` — `run_sync` is just a wrapper around `run` and agents are always run in an async context.\n\nHere's the same example as above, but with a synchronous dependency:\n\nsync\\_dependencies.py\n\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.Client  # (1)!\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt\ndef get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)!\n    response = ctx.deps.http_client.get(\n        'https://example.com', headers={'Authorization': f'Bearer {ctx.deps.api_key}'}\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\nasync def main():\n    deps = MyDeps('foobar', httpx.Client())\n    result = await agent.run(\n        'Tell me a joke.',\n        deps=deps,\n    )\n    print(result.output)\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. Here we use a synchronous `httpx.Client` instead of an asynchronous `httpx.AsyncClient`.\n2. To match the synchronous dependency, the system prompt function is now a plain function, not a coroutine.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Asynchronous vs. Synchronous dependencies"
  },
  "e31c5c0745cd5b3f7b75423309a384683e92a5f5584b3de5667067629a1152b7": {
    "text": "As well as system prompts, dependencies can be used in [tools](../tools/ \"../tools/\") and [output validators](../output/#output-validator-functions \"../output/#output-validator-functions\").\n\nfull\\_example.py\n\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n    response = await ctx.deps.http_client.get('https://example.com')\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\n@agent.tool  # (1)!\nasync def get_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\n    response = await ctx.deps.http_client.get(\n        'https://example.com#jokes',\n        params={'subject': subject},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n\n@agent.output_validator  # (2)!\nasync def validate_output(ctx: RunContext[MyDeps], output: str) -> str:\n    response = await ctx.deps.http_client.post(\n        'https://example.com#validate',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n        params={'query': output},\n    )\n    if response.status_code == 400:\n        raise ModelRetry(f'invalid response: {response.text}')\n    response.raise_for_status()\n    return output\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. To pass `RunContext` to a tool, use the [`tool`](../api/agent/#pydantic_ai.agent.Agent.tool \"../api/agent/#pydantic_ai.agent.Agent.tool\") decorator.\n2. `RunContext` may optionally be passed to a [`output_validator`](../api/agent/#pydantic_ai.agent.Agent.output_validator \"../api/agent/#pydantic_ai.agent.Agent.output_validator\") function as the first argument.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Full Example"
  },
  "e5be3ac4d3569bd00936b1e439d97216d6d4da7b9ed6fc28d241050826369389": {
    "text": "When testing agents, it's useful to be able to customise dependencies.\n\nWhile this can sometimes be done by calling the agent directly within unit tests, we can also override dependencies\nwhile calling application code which in turn calls the agent.\n\nThis is done via the [`override`](../api/agent/#pydantic_ai.agent.Agent.override \"../api/agent/#pydantic_ai.agent.Agent.override\") method on the agent.\n\njoke\\_app.py\n\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n    async def system_prompt_factory(self) -> str:  # (1)!\n        response = await self.http_client.get('https://example.com')\n        response.raise_for_status()\n        return f'Prompt: {response.text}'\n\n\njoke_agent = Agent('openai:gpt-4o', deps_type=MyDeps)\n\n\n@joke_agent.system_prompt\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n    return await ctx.deps.system_prompt_factory()  # (2)!\n\n\nasync def application_code(prompt: str) -> str:  # (3)!\n    ...\n    ...\n    # now deep within application code we call our agent\n    async with httpx.AsyncClient() as client:\n        app_deps = MyDeps('foobar', client)\n        result = await joke_agent.run(prompt, deps=app_deps)  # (4)!\n    return result.output\n```\n\n1. Define a method on the dependency to make the system prompt easier to customise.\n2. Call the system prompt factory from within the system prompt function.\n3. Application code that calls the agent, in a real application this might be an API endpoint.\n4. Call the agent from within the application code, in a real application this call might be deep within a call stack. Note `app_deps` here will NOT be used when deps are overridden.\n\n*(This example is complete, it can be run \"as is\")*\n\ntest\\_joke\\_app.py\n\n```\nfrom joke_app import MyDeps, application_code, joke_agent\n\n\nclass TestMyDeps(MyDeps):  # (1)!\n    async def system_prompt_factory(self) -> str:\n        return 'test prompt'\n\n\nasync def test_application_code():\n    test_deps = TestMyDeps('test_key', None)  # (2)!\n    with joke_agent.override(deps=test_deps):  # (3)!\n        joke = await application_code('Tell me a joke.')  # (4)!\n    assert joke.startswith('Did you hear about the toothpaste scandal?')\n```\n\n1. Define a subclass of `MyDeps` in tests to customise the system prompt factory.\n2. Create an instance of the test dependency, we don't need to pass an `http_client` here as it's not used.\n3. Override the dependencies of the agent for the duration of the `with` block, `test_deps` will be used when the agent is run.\n4. Now we can safely call our application code, the agent will use the overridden dependencies.",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Overriding Dependencies"
  },
  "4edc489a95d46533b5ca1b38a5077b8b600b2cbc4c6c97c0d211f695f5c5d25e": {
    "text": "The following examples demonstrate how to use dependencies in Pydantic AI:",
    "source_url": "https://ai.pydantic.dev/dependencies/",
    "header": "Examples"
  },
  "84c01948f275dea02b4514d662e74e1fa2b845043310401557372fac7266bc94": {
    "text": "Function tools provide a mechanism for models to perform actions and retrieve extra information to help them generate a response.\n\nThey're useful when you want to enable the model to take some action and use the result, when it is impractical or impossible to put all the context an agent might need into the instructions, or when you want to make agents' behavior more deterministic or reliable by deferring some of the logic required to generate a response to another (not necessarily AI-powered) tool.\n\nIf you want a model to be able to call a function as its final action, without the result being sent back to the model, you can use an [output function](../output/#output-functions \"../output/#output-functions\") instead.\n\nThere are a number of ways to register tools with an agent:\n\n* via the [`@agent.tool`](../api/agent/#pydantic_ai.agent.Agent.tool \"../api/agent/#pydantic_ai.agent.Agent.tool\") decorator — for tools that need access to the agent [context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\")\n* via the [`@agent.tool_plain`](../api/agent/#pydantic_ai.agent.Agent.tool_plain \"../api/agent/#pydantic_ai.agent.Agent.tool_plain\") decorator — for tools that do not need access to the agent [context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\")\n* via the [`tools`](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\") keyword argument to `Agent` which can take either plain functions, or instances of [`Tool`](../api/tools/#pydantic_ai.tools.Tool \"../api/tools/#pydantic_ai.tools.Tool\")\n\nFor more advanced use cases, the [toolsets](../toolsets/ \"../toolsets/\") feature lets you manage collections of tools (built by you or provided by an [MCP server](../mcp/client/ \"../mcp/client/\") or other [third party](#third-party-tools \"#third-party-tools\")) and register them with an agent in one go via the [`toolsets`](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\") keyword argument to `Agent`. Internally, all `tools` and `toolsets` are gathered into a single [combined toolset](../toolsets/#combining-toolsets \"../toolsets/#combining-toolsets\") that's made available to the model.\n\nFunction tools vs. RAG\n\nFunction tools are basically the \"R\" of RAG (Retrieval-Augmented Generation) — they augment what the model can do by letting it request extra information.\n\nThe main semantic difference between Pydantic AI Tools and RAG is RAG is synonymous with vector search, while Pydantic AI tools are more general-purpose. (Note: we may add support for vector search functionality in the future, particularly an API for generating embeddings. See [#58](https://github.com/pydantic/pydantic-ai/issues/58 \"https://github.com/pydantic/pydantic-ai/issues/58\"))\n\nFunction Tools vs. Structured Outputs\n\nAs the name suggests, function tools use the model's \"tools\" or \"functions\" API to let the model know what is available to call. Tools or functions are also used to define the schema(s) for [structured output](../output/ \"../output/\") when using the default [tool output mode](../output/#tool-output \"../output/#tool-output\"), thus a model might have access to many tools, some of which call function tools while others end the run and produce a final output.\n\n`@agent.tool` is considered the default decorator since in the majority of cases tools will need access to the agent [context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\").\n\nHere's an example using both:\n\ndice\\_game.py\n\n```\nimport random\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'google-gla:gemini-1.5-flash',  # (1)!\n    deps_type=str,  # (2)!\n    system_prompt=(\n        \"You're a dice game, you should roll the die and see if the number \"\n        \"you get back matches the user's guess. If so, tell them they're a winner. \"\n        \"Use the player's name in the response.\"\n    ),\n)\n\n\n@agent.tool_plain  # (3)!\ndef roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 6))\n\n\n@agent.tool  # (4)!\ndef get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"Get the player's name.\"\"\"\n    return ctx.deps\n\n\ndice_result = agent.run_sync('My guess is 4', deps='Anne')  # (5)!\nprint(dice_result.output)\n#> Congratulations Anne, you guessed correctly! You're a winner!\n```\n\n1. This is a pretty simple task, so we can use the fast and cheap Gemini flash model.\n2. We pass the user's name as the dependency, to keep things simple we use just the name as a string as the dependency.\n3. This tool doesn't need any context, it just returns a random number. You could probably use dynamic instructions in this case.\n4. This tool needs the player's name, so it uses `RunContext` to access dependencies which are just the player's name in this case.\n5. Run the agent, passing the player's name as the dependency.\n\n*(This example is complete, it can be run \"as is\")*\n\nLet's print the messages from that game to see what happened:\n\ndice\\_game\\_messages.py\n\n```\nfrom dice_game import dice_result\n\nprint(dice_result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content=\"You're a dice game, you should roll the die and see if the number you get back matches the user's guess. If so, tell them they're a winner. Use the player's name in the response.\",\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='My guess is 4',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='roll_dice', args={}, tool_call_id='pyd_ai_tool_call_id'\n            )\n        ],\n        usage=RequestUsage(input_tokens=90, output_tokens=2),\n        model_name='gemini-1.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='roll_dice',\n                content='4',\n                tool_call_id='pyd_ai_tool_call_id',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='get_player_name', args={}, tool_call_id='pyd_ai_tool_call_id'\n            )\n        ],\n        usage=RequestUsage(input_tokens=91, output_tokens=4),\n        model_name='gemini-1.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='get_player_name',\n                content='Anne',\n                tool_call_id='pyd_ai_tool_call_id',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content=\"Congratulations Anne, you guessed correctly! You're a winner!\"\n            )\n        ],\n        usage=RequestUsage(input_tokens=92, output_tokens=12),\n        model_name='gemini-1.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\nWe can represent this with a diagram:\n\n```\nsequenceDiagram\n    participant Agent\n    participant LLM\n\n    Note over Agent: Send prompts\n    Agent ->> LLM: System: \"You're a dice game...\"<br>User: \"My guess is 4\"\n    activate LLM\n    Note over LLM: LLM decides to use<br>a tool\n\n    LLM ->> Agent: Call tool<br>roll_dice()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Rolls a six-sided die\n\n    Agent -->> LLM: ToolReturn<br>\"4\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM decides to use<br>another tool\n\n    LLM ->> Agent: Call tool<br>get_player_name()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Retrieves player name\n    Agent -->> LLM: ToolReturn<br>\"Anne\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM constructs final response\n\n    LLM ->> Agent: ModelResponse<br>\"Congratulations Anne, ...\"\n    deactivate LLM\n    Note over Agent: Game session complete\n```\n\nAs well as using the decorators, we can register tools via the `tools` argument to the [`Agent` constructor](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\"). This is useful when you want to reuse tools, and can also give more fine-grained control over the tools.\n\ndice\\_game\\_tool\\_kwarg.py\n\n```\nimport random\n\nfrom pydantic_ai import Agent, RunContext, Tool\n\nsystem_prompt = \"\"\"\\\nYou're a dice game, you should roll the die and see if the number\nyou get back matches the user's guess. If so, tell them they're a winner.\nUse the player's name in the response.\n\"\"\"\n\n\ndef roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 6))\n\n\ndef get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"Get the player's name.\"\"\"\n    return ctx.deps\n\n\nagent_a = Agent(\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    tools=[roll_dice, get_player_name],  # (1)!\n    system_prompt=system_prompt,\n)\nagent_b = Agent(\n    'google-gla:gemini-1.5-flash',\n    deps_type=str,\n    tools=[  # (2)!\n        Tool(roll_dice, takes_ctx=False),\n        Tool(get_player_name, takes_ctx=True),\n    ],\n    system_prompt=system_prompt,\n)\n\ndice_result = {}\ndice_result['a'] = agent_a.run_sync('My guess is 6', deps='Yashar')\ndice_result['b'] = agent_b.run_sync('My guess is 4', deps='Anne')\nprint(dice_result['a'].output)\n#> Tough luck, Yashar, you rolled a 4. Better luck next time.\nprint(dice_result['b'].output)\n#> Congratulations Anne, you guessed correctly! You're a winner!\n```\n\n1. The simplest way to register tools via the `Agent` constructor is to pass a list of functions, the function signature is inspected to determine if the tool takes [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\").\n2. `agent_a` and `agent_b` are identical — but we can use [`Tool`](../api/tools/#pydantic_ai.tools.Tool \"../api/tools/#pydantic_ai.tools.Tool\") to reuse tool definitions and give more fine-grained control over how tools are defined, e.g. setting their name or description, or using a custom [`prepare`](#tool-prepare \"#tool-prepare\") method.\n\n*(This example is complete, it can be run \"as is\")*\n\nTools can return anything that Pydantic can serialize to JSON, as well as audio, video, image or document content depending on the types of [multi-modal input](../input/ \"../input/\") the model supports:\n\nfunction\\_tool\\_output.py\n\n```\nfrom datetime import datetime\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, DocumentUrl, ImageUrl\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nagent = Agent(model=OpenAIResponsesModel('gpt-4o'))\n\n\n@agent.tool_plain\ndef get_current_time() -> datetime:\n    return datetime.now()\n\n\n@agent.tool_plain\ndef get_user() -> User:\n    return User(name='John', age=30)\n\n\n@agent.tool_plain\ndef get_company_logo() -> ImageUrl:\n    return ImageUrl(url='https://iili.io/3Hs4FMg.png')\n\n\n@agent.tool_plain\ndef get_document() -> DocumentUrl:\n    return DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')\n\n\nresult = agent.run_sync('What time is it?')\nprint(result.output)\n#> The current time is 10:45 PM on April 17, 2025.\n\nresult = agent.run_sync('What is the user name?')\nprint(result.output)\n#> The user's name is John.\n\nresult = agent.run_sync('What is the company name in the logo?')\nprint(result.output)\n#> The company name in the logo is \"Pydantic.\"\n\nresult = agent.run_sync('What is the main content of the document?')\nprint(result.output)\n#> The document contains just the text \"Dummy PDF file.\"\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nSome models (e.g. Gemini) natively support semi-structured return values, while some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON.\n\nFor scenarios where you need more control over both the tool's return value and the content sent to the model, you can use [`ToolReturn`](../api/messages/#pydantic_ai.messages.ToolReturn \"../api/messages/#pydantic_ai.messages.ToolReturn\"). This is particularly useful when you want to:\n\n* Provide rich multi-modal content (images, documents, etc.) to the model as context\n* Separate the programmatic return value from the model's context\n* Include additional metadata that shouldn't be sent to the LLM\n\nHere's an example of a computer automation tool that captures screenshots and provides visual feedback:\n\nadvanced\\_tool\\_return.py\n\n```\nimport time\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ToolReturn, BinaryContent\n\nagent = Agent('openai:gpt-4o')\n\n@agent.tool_plain\ndef click_and_capture(x: int, y: int) -> ToolReturn:\n    \"\"\"Click at coordinates and show before/after screenshots.\"\"\"\n    # Take screenshot before action\n    before_screenshot = capture_screen()\n\n    # Perform click operation\n    perform_click(x, y)\n    time.sleep(0.5)  # Wait for UI to update\n\n    # Take screenshot after action\n    after_screenshot = capture_screen()\n\n    return ToolReturn(\n        return_value=f\"Successfully clicked at ({x}, {y})\",\n        content=[\n            f\"Clicked at coordinates ({x}, {y}). Here's the comparison:\",\n            \"Before:\",\n            BinaryContent(data=before_screenshot, media_type=\"image/png\"),\n            \"After:\",\n            BinaryContent(data=after_screenshot, media_type=\"image/png\"),\n            \"Please analyze the changes and suggest next steps.\"\n        ],\n        metadata={\n            \"coordinates\": {\"x\": x, \"y\": y},\n            \"action_type\": \"click_and_capture\",\n            \"timestamp\": time.time()\n        }\n    )",
    "source_url": "https://ai.pydantic.dev/tools/",
    "header": "Introduction"
  },
  "ea7323c978125e7c4860b9d9aa82221cf9c727e2cb5376b138d7a42aa7a38df8": {
    "text": "result = agent.run_sync(\"Click on the submit button and tell me what happened\")\nprint(result.output)",
    "source_url": "https://ai.pydantic.dev/tools/",
    "header": "while your application can access the structured return_value and metadata"
  },
  "89509ece1370b10cda5ed050bd237b71cbf2095a3773a026bb546125291aedcb": {
    "text": "```\n\n* **`return_value`**: The actual return value used in the tool response. This is what gets serialized and sent back to the model as the tool's result.\n* **`content`**: A sequence of content (text, images, documents, etc.) that provides additional context to the model. This appears as a separate user message.\n* **`metadata`**: Optional metadata that your application can access but is not sent to the LLM. Useful for logging, debugging, or additional processing. Some other AI frameworks call this feature \"artifacts\".\n\nThis separation allows you to provide rich context to the model while maintaining clean, structured return values for your application logic.\n\nFunction parameters are extracted from the function signature, and all parameters except `RunContext` are used to build the schema for that tool call.\n\nEven better, Pydantic AI extracts the docstring from functions and (thanks to [griffe](https://mkdocstrings.github.io/griffe/ \"https://mkdocstrings.github.io/griffe/\")) extracts parameter descriptions from the docstring and adds them to the schema.\n\n[Griffe supports](https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings \"https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings\") extracting parameter descriptions from `google`, `numpy`, and `sphinx` style docstrings. Pydantic AI will infer the format to use based on the docstring, but you can explicitly set it using [`docstring_format`](../api/tools/#pydantic_ai.tools.DocstringFormat \"../api/tools/#pydantic_ai.tools.DocstringFormat\"). You can also enforce parameter requirements by setting `require_parameter_descriptions=True`. This will raise a [`UserError`](../api/exceptions/#pydantic_ai.exceptions.UserError \"../api/exceptions/#pydantic_ai.exceptions.UserError\") if a parameter description is missing.\n\nTo demonstrate a tool's schema, here we use [`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\") to print the schema a model would receive:\n\ntool\\_schema.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nagent = Agent()\n\n\n@agent.tool_plain(docstring_format='google', require_parameter_descriptions=True)\ndef foobar(a: int, b: str, c: dict[str, list[float]]) -> str:\n    \"\"\"Get me foobar.\n\n    Args:\n        a: apple pie\n        b: banana cake\n        c: carrot smoothie\n    \"\"\"\n    return f'{a} {b} {c}'\n\n\ndef print_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n    tool = info.function_tools[0]\n    print(tool.description)\n    #> Get me foobar.\n    print(tool.parameters_json_schema)\n    \"\"\"\n    {\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'apple pie', 'type': 'integer'},\n            'b': {'description': 'banana cake', 'type': 'string'},\n            'c': {\n                'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'},\n                'description': 'carrot smoothie',\n                'type': 'object',\n            },\n        },\n        'required': ['a', 'b', 'c'],\n        'type': 'object',\n    }\n    \"\"\"\n    return ModelResponse(parts=[TextPart('foobar')])\n\n\nagent.run_sync('hello', model=FunctionModel(print_schema))\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nIf a tool has a single parameter that can be represented as an object in JSON schema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is simplified to be just that object.\n\nHere's an example where we use [`TestModel.last_model_request_parameters`](../api/models/test/#pydantic_ai.models.test.TestModel.last_model_request_parameters \"../api/models/test/#pydantic_ai.models.test.TestModel.last_model_request_parameters\") to inspect the tool schema that would be passed to the model.\n\nsingle\\_parameter\\_tool.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nagent = Agent()\n\n\nclass Foobar(BaseModel):\n    \"\"\"This is a Foobar\"\"\"\n\n    x: int\n    y: str\n    z: float = 3.14\n\n\n@agent.tool_plain\ndef foobar(f: Foobar) -> str:\n    return str(f)\n\n\ntest_model = TestModel()\nresult = agent.run_sync('hello', model=test_model)\nprint(result.output)\n#> {\"foobar\":\"x=0 y='a' z=3.14\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='foobar',\n        parameters_json_schema={\n            'properties': {\n                'x': {'type': 'integer'},\n                'y': {'type': 'string'},\n                'z': {'default': 3.14, 'type': 'number'},\n            },\n            'required': ['x', 'y'],\n            'title': 'Foobar',\n            'type': 'object',\n        },\n        description='This is a Foobar',\n    )\n]\n\"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nIf you have a function that lacks appropriate documentation (i.e. poorly named, no type information, poor docstring, use of \\*args or \\*\\*kwargs and suchlike) then you can still turn it into a tool that can be effectively used by the agent with the [`Tool.from_schema`](../api/tools/#pydantic_ai.tools.Tool.from_schema \"../api/tools/#pydantic_ai.tools.Tool.from_schema\") function. With this you provide the name, description, JSON schema, and whether the function takes a `RunContext` for the function directly:\n\n```\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.test import TestModel\n\n\ndef foobar(**kwargs) -> str:\n    return kwargs['a'] + kwargs['b']\n\ntool = Tool.from_schema(\n    function=foobar,\n    name='sum',\n    description='Sum two numbers.',\n    json_schema={\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'the first number', 'type': 'integer'},\n            'b': {'description': 'the second number', 'type': 'integer'},\n        },\n        'required': ['a', 'b'],\n        'type': 'object',\n    },\n    takes_ctx=False,\n)\n\ntest_model = TestModel()\nagent = Agent(test_model, tools=[tool])\n\nresult = agent.run_sync('testing...')\nprint(result.output)\n#> {\"sum\":0}\n```\n\nPlease note that validation of the tool arguments will not be performed, and this will pass all arguments as keyword arguments.\n\nTools can optionally be defined with another function: `prepare`, which is called at each step of a run to\ncustomize the definition of the tool passed to the model, or omit the tool completely from that step.\n\nA `prepare` method can be registered via the `prepare` kwarg to any of the tool registration mechanisms:\n\nThe `prepare` method, should be of type [`ToolPrepareFunc`](../api/tools/#pydantic_ai.tools.ToolPrepareFunc \"../api/tools/#pydantic_ai.tools.ToolPrepareFunc\"), a function which takes [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and a pre-built [`ToolDefinition`](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\"), and should either return that `ToolDefinition` with or without modifying it, return a new `ToolDefinition`, or return `None` to indicate this tools should not be registered for that step.\n\nHere's a simple `prepare` method that only includes the tool if the value of the dependency is `42`.\n\nAs with the previous example, we use [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") to demonstrate the behavior without calling a real model.\n\ntool\\_only\\_if\\_42.py\n\n```\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\n\nagent = Agent('test')\n\n\nasync def only_if_42(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    if ctx.deps == 42:\n        return tool_def\n\n\n@agent.tool(prepare=only_if_42)\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\n    return f'{ctx.deps} {answer}'\n\n\nresult = agent.run_sync('testing...', deps=41)\nprint(result.output)\n#> success (no tool calls)\nresult = agent.run_sync('testing...', deps=42)\nprint(result.output)\n#> {\"hitchhiker\":\"42 a\"}\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nHere's a more complex example where we change the description of the `name` parameter to based on the value of `deps`\n\nFor the sake of variation, we create this tool using the [`Tool`](../api/tools/#pydantic_ai.tools.Tool \"../api/tools/#pydantic_ai.tools.Tool\") dataclass.\n\ncustomize\\_name.py\n\n```\nfrom __future__ import annotations\n\nfrom typing import Literal\n\nfrom pydantic_ai import Agent, RunContext, Tool, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\ndef greet(name: str) -> str:\n    return f'hello {name}'\n\n\nasync def prepare_greet(\n    ctx: RunContext[Literal['human', 'machine']], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    d = f'Name of the {ctx.deps} to greet.'\n    tool_def.parameters_json_schema['properties']['name']['description'] = d\n    return tool_def\n\n\ngreet_tool = Tool(greet, prepare=prepare_greet)\ntest_model = TestModel()\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal['human', 'machine'])\n\nresult = agent.run_sync('testing...', deps='human')\nprint(result.output)\n#> {\"greet\":\"hello a\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='greet',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {\n                'name': {'type': 'string', 'description': 'Name of the human to greet.'}\n            },\n            'required': ['name'],\n            'type': 'object',\n        },\n    )\n]\n\"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nIn addition to per-tool `prepare` methods, you can also define an agent-wide `prepare_tools` function. This function is called at each step of a run and allows you to filter or modify the list of all tool definitions available to the agent for that step. This is especially useful if you want to enable or disable multiple tools at once, or apply global logic based on the current context.\n\nThe `prepare_tools` function should be of type [`ToolsPrepareFunc`](../api/tools/#pydantic_ai.tools.ToolsPrepareFunc \"../api/tools/#pydantic_ai.tools.ToolsPrepareFunc\"), which takes the [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and a list of [`ToolDefinition`](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\"), and returns a new list of tool definitions (or `None` to disable all tools for that step).\n\nNote\n\nThe list of tool definitions passed to `prepare_tools` includes both regular function tools and tools from any [toolsets](../toolsets/ \"../toolsets/\") registered on the agent, but not [output tools](../output/#tool-output \"../output/#tool-output\").\n\nTo modify output tools, you can set a `prepare_output_tools` function instead.\n\nHere's an example that makes all tools strict if the model is an OpenAI model:\n\nagent\\_prepare\\_tools\\_customize.py\n\n```\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def turn_on_strict_if_openai(\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\n\ntest_model = TestModel()\nagent = Agent(test_model, prepare_tools=turn_on_strict_if_openai)\n\n\n@agent.tool_plain\ndef echo(message: str) -> str:\n    return message\n\n\nagent.run_sync('testing...')\nassert test_model.last_model_request_parameters.function_tools[0].strict is None",
    "source_url": "https://ai.pydantic.dev/tools/",
    "header": "The model can analyze the screenshots and provide detailed feedback"
  },
  "63cde0180bbb352b7e16cff21419fcfe26dce548cc00175c4aa90a307a319204": {
    "text": "test_model._system = 'openai'\n\nagent.run_sync('testing with openai...')\nassert test_model.last_model_request_parameters.function_tools[0].strict\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nHere's another example that conditionally filters out the tools by name if the dependency (`ctx.deps`) is `True`:\n\nagent\\_prepare\\_tools\\_filter\\_out.py\n\n```\nfrom pydantic_ai import Agent, RunContext, Tool, ToolDefinition\n\n\ndef launch_potato(target: str) -> str:\n    return f'Potato launched at {target}!'\n\n\nasync def filter_out_tools_by_name(\n    ctx: RunContext[bool], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.deps:\n        return [tool_def for tool_def in tool_defs if tool_def.name != 'launch_potato']\n    return tool_defs\n\n\nagent = Agent(\n    'test',\n    tools=[Tool(launch_potato)],\n    prepare_tools=filter_out_tools_by_name,\n    deps_type=bool,\n)\n\nresult = agent.run_sync('testing...', deps=False)\nprint(result.output)\n#> {\"launch_potato\":\"Potato launched at a!\"}\nresult = agent.run_sync('testing...', deps=True)\nprint(result.output)\n#> success (no tool calls)\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can use `prepare_tools` to:\n\n* Dynamically enable or disable tools based on the current model, dependencies, or other context\n* Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.)\n\nIf both per-tool `prepare` and agent-wide `prepare_tools` are used, the per-tool `prepare` is applied first to each tool, and then `prepare_tools` is called with the resulting list of tool definitions.\n\nThere are a few scenarios where the model should be able to call a tool that should not or cannot be executed during the same agent run inside the same Python process:\n\n* it may need to be approved by the user first\n* it may depend on an upstream service, frontend, or user to provide the result\n* the result could take longer to generate than it's reasonable to keep the agent process running\n\nTo support these use cases, Pydantic AI provides the concept of deferred tools, which come in two flavors documented below:\n\nWhen the model calls a deferred tool, the agent run will end with a [`DeferredToolRequests`](../api/output/#pydantic_ai.output.DeferredToolRequests \"../api/output/#pydantic_ai.output.DeferredToolRequests\") output object containing information about the deferred tool calls. Once the approvals and/or results are ready, a new agent run can then be started with the original run's [message history](../message-history/ \"../message-history/\") plus a [`DeferredToolResults`](../api/tools/#pydantic_ai.tools.DeferredToolResults \"../api/tools/#pydantic_ai.tools.DeferredToolResults\") object holding results for each tool call in `DeferredToolRequests`, which will continue the original run where it left off.\n\nNote that handling deferred tool calls requires `DeferredToolRequests` to be in the `Agent`'s [`output_type`](../output/#structured-output \"../output/#structured-output\") so that the possible types of the agent run output are correctly inferred. If your agent can also be used in a context where no deferred tools are available and you don't want to deal with that type everywhere you use the agent, you can instead pass the `output_type` argument when you run the agent using [`agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\"), [`agent.run_sync()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync\"), [`agent.run_stream()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\"), or [`agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\"). Note that the run-time `output_type` overrides the one specified at construction time (for type inference reasons), so you'll need to include the original output type explicitly.\n\nIf a tool function always requires approval, you can pass the `requires_approval=True` argument to the [`@agent.tool`](../api/agent/#pydantic_ai.agent.Agent.tool \"../api/agent/#pydantic_ai.agent.Agent.tool\") decorator, [`@agent.tool_plain`](../api/agent/#pydantic_ai.agent.Agent.tool_plain \"../api/agent/#pydantic_ai.agent.Agent.tool_plain\") decorator, [`Tool`](../api/tools/#pydantic_ai.tools.Tool \"../api/tools/#pydantic_ai.tools.Tool\") class, [`FunctionToolset.tool`](../api/toolsets/#pydantic_ai.toolsets.FunctionToolset.tool \"../api/toolsets/#pydantic_ai.toolsets.FunctionToolset.tool\") decorator, or [`FunctionToolset.add_function()`](../api/toolsets/#pydantic_ai.toolsets.FunctionToolset.add_function \"../api/toolsets/#pydantic_ai.toolsets.FunctionToolset.add_function\") method. Inside the function, you can then assume that the tool call has been approved.\n\nIf whether a tool function requires approval depends on the tool call arguments or the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") (e.g. [dependencies](../dependencies/ \"../dependencies/\") or message history), you can raise the [`ApprovalRequired`](../api/exceptions/#pydantic_ai.exceptions.ApprovalRequired \"../api/exceptions/#pydantic_ai.exceptions.ApprovalRequired\") exception from the tool function. The [`RunContext.tool_call_approved`](../api/tools/#pydantic_ai.tools.RunContext.tool_call_approved \"../api/tools/#pydantic_ai.tools.RunContext.tool_call_approved\") property will be `True` if the tool call has already been approved.\n\nTo require approval for calls to tools provided by a [toolset](../toolsets/ \"../toolsets/\") (like an [MCP server](../mcp/client/ \"../mcp/client/\")), see the [`ApprovalRequiredToolset` documentation](../toolsets/#requiring-tool-approval \"../toolsets/#requiring-tool-approval\").\n\nWhen the model calls a tool that requires approval, the agent run will end with a [`DeferredToolRequests`](../api/output/#pydantic_ai.output.DeferredToolRequests \"../api/output/#pydantic_ai.output.DeferredToolRequests\") output object with an `approvals` list holding [`ToolCallPart`s](../api/messages/#pydantic_ai.messages.ToolCallPart \"../api/messages/#pydantic_ai.messages.ToolCallPart\") containing the tool name, validated arguments, and a unique tool call ID.\n\nOnce you've gathered the user's approvals or denials, you can build a [`DeferredToolResults`](../api/tools/#pydantic_ai.tools.DeferredToolResults \"../api/tools/#pydantic_ai.tools.DeferredToolResults\") object with an `approvals` dictionary that maps each tool call ID to a boolean, a [`ToolApproved`](../api/tools/#pydantic_ai.tools.ToolApproved \"../api/tools/#pydantic_ai.tools.ToolApproved\") object (with optional `override_args`), or a [`ToolDenied`](../api/tools/#pydantic_ai.tools.ToolDenied \"../api/tools/#pydantic_ai.tools.ToolDenied\") object (with an optional custom `message` to provide to the model). This `DeferredToolResults` object can then be provided to one of the agent run methods as `deferred_tool_results`, alongside the original run's [message history](../message-history/ \"../message-history/\").\n\nHere's an example that shows how to require approval for all file deletions, and for updates of specific protected files:\n\ntool\\_requires\\_approval.py\n\n```\nfrom pydantic_ai import (\n    Agent,\n    ApprovalRequired,\n    DeferredToolRequests,\n    DeferredToolResults,\n    RunContext,\n    ToolDenied,\n)\n\nagent = Agent('openai:gpt-5', output_type=[str, DeferredToolRequests])\n\nPROTECTED_FILES = {'.env'}\n\n\n@agent.tool\ndef update_file(ctx: RunContext, path: str, content: str) -> str:\n    if path in PROTECTED_FILES and not ctx.tool_call_approved:\n        raise ApprovalRequired\n    return f'File {path!r} updated: {content!r}'\n\n\n@agent.tool_plain(requires_approval=True)\ndef delete_file(path: str) -> str:\n    return f'File {path!r} deleted'\n\n\nresult = agent.run_sync('Delete `__init__.py`, write `Hello, world!` to `README.md`, and clear `.env`')\nmessages = result.all_messages()\n\nassert isinstance(result.output, DeferredToolRequests)\nrequests = result.output\nprint(requests)\n\"\"\"\nDeferredToolRequests(\n    calls=[],\n    approvals=[\n        ToolCallPart(\n            tool_name='update_file',\n            args={'path': '.env', 'content': ''},\n            tool_call_id='update_file_dotenv',\n        ),\n        ToolCallPart(\n            tool_name='delete_file',\n            args={'path': '__init__.py'},\n            tool_call_id='delete_file',\n        ),\n    ],\n)\n\"\"\"\n\nresults = DeferredToolResults()\nfor call in requests.approvals:\n    result = False\n    if call.tool_name == 'update_file':\n        # Approve all updates\n        result = True\n    elif call.tool_name == 'delete_file':\n        # deny all deletes\n        result = ToolDenied('Deleting files is not allowed')\n\n    results.approvals[call.tool_call_id] = result\n\nresult = agent.run_sync(message_history=messages, deferred_tool_results=results)\nprint(result.output)\n\"\"\"\nI successfully deleted `__init__.py` and updated `README.md`, but was not able to delete `.env`.\n\"\"\"\nprint(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Delete `__init__.py`, write `Hello, world!` to `README.md`, and clear `.env`',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='delete_file',\n                args={'path': '__init__.py'},\n                tool_call_id='delete_file',\n            ),\n            ToolCallPart(\n                tool_name='update_file',\n                args={'path': 'README.md', 'content': 'Hello, world!'},\n                tool_call_id='update_file_readme',\n            ),\n            ToolCallPart(\n                tool_name='update_file',\n                args={'path': '.env', 'content': ''},\n                tool_call_id='update_file_dotenv',\n            ),\n        ],\n        usage=RequestUsage(input_tokens=63, output_tokens=21),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='delete_file',\n                content='Deleting files is not allowed',\n                tool_call_id='delete_file',\n                timestamp=datetime.datetime(...),\n            ),\n            ToolReturnPart(\n                tool_name='update_file',\n                content=\"File 'README.md' updated: 'Hello, world!'\",\n                tool_call_id='update_file_readme',\n                timestamp=datetime.datetime(...),\n            ),\n            ToolReturnPart(\n                tool_name='update_file',\n                content=\"File '.env' updated: ''\",\n                tool_call_id='update_file_dotenv',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='I successfully deleted `__init__.py` and updated `README.md`, but was not able to delete `.env`.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=79, output_tokens=39),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nWhen the result of a tool call cannot be generated inside the same agent run in which it was called, the tool is considered to be external.\nExamples of external tools are client-side tools implemented by a web or app frontend, and slow tasks that are passed off to a background worker or external service instead of keeping the agent process running.\n\nIf whether a tool call should be executed externally depends on the tool call arguments, the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") (e.g. [dependencies](../dependencies/ \"../dependencies/\") or message history), or how long the task is expected to take, you can define a tool function and conditionally raise the [`CallDeferred`](../api/exceptions/#pydantic_ai.exceptions.CallDeferred \"../api/exceptions/#pydantic_ai.exceptions.CallDeferred\") exception. Before raising the exception, the tool function would typically schedule some background task and pass along the [`RunContext.tool_call_id`](../api/tools/#pydantic_ai.tools.RunContext.tool_call_id \"../api/tools/#pydantic_ai.tools.RunContext.tool_call_id\") so that the result can be matched to the deferred tool call later.\n\nIf a tool is always executed externally and its definition is provided to your code along with a JSON schema for its arguments, you can use an [`ExternalToolset`](../toolsets/#external-toolset \"../toolsets/#external-toolset\"). If the external tools are known up front and you don't have the arguments JSON schema handy, you can also define a tool function with the appropriate signature that does nothing but raise the [`CallDeferred`](../api/exceptions/#pydantic_ai.exceptions.CallDeferred \"../api/exceptions/#pydantic_ai.exceptions.CallDeferred\") exception.\n\nWhen the model calls an external tool, the agent run will end with a [`DeferredToolRequests`](../api/output/#pydantic_ai.output.DeferredToolRequests \"../api/output/#pydantic_ai.output.DeferredToolRequests\") output object with a `calls` list holding [`ToolCallPart`s](../api/messages/#pydantic_ai.messages.ToolCallPart \"../api/messages/#pydantic_ai.messages.ToolCallPart\") containing the tool name, validated arguments, and a unique tool call ID.\n\nOnce the tool call results are ready, you can build a [`DeferredToolResults`](../api/tools/#pydantic_ai.tools.DeferredToolResults \"../api/tools/#pydantic_ai.tools.DeferredToolResults\") object with a `calls` dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [`ToolReturn`](#advanced-tool-returns \"#advanced-tool-returns\") object, or a [`ModelRetry`](../api/exceptions/#pydantic_ai.exceptions.ModelRetry \"../api/exceptions/#pydantic_ai.exceptions.ModelRetry\") exception in case the tool call failed and the model should [try again](#tool-retries \"#tool-retries\"). This `DeferredToolResults` object can then be provided to one of the agent run methods as `deferred_tool_results`, alongside the original run's [message history](../message-history/ \"../message-history/\").\n\nHere's an example that shows how to move a task that takes a while to complete to the background and return the result to the model once the task is complete:\n\nexternal\\_tool.py\n\n```\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_ai import (\n    Agent,\n    CallDeferred,\n    DeferredToolRequests,\n    DeferredToolResults,\n    ModelRetry,\n    RunContext,\n)\n\n\n@dataclass\nclass TaskResult:\n    tool_call_id: str\n    result: Any\n\n\nasync def calculate_answer_task(tool_call_id: str, question: str) -> TaskResult:\n    await asyncio.sleep(1)\n    return TaskResult(tool_call_id=tool_call_id, result=42)\n\n\nagent = Agent('openai:gpt-5', output_type=[str, DeferredToolRequests])\n\ntasks: list[asyncio.Task[TaskResult]] = []\n\n\n@agent.tool\nasync def calculate_answer(ctx: RunContext, question: str) -> str:\n    assert ctx.tool_call_id is not None\n\n    task = asyncio.create_task(calculate_answer_task(ctx.tool_call_id, question))  # (1)!\n    tasks.append(task)\n\n    raise CallDeferred\n\n\nasync def main():\n    result = await agent.run('Calculate the answer to the ultimate question of life, the universe, and everything')\n    messages = result.all_messages()\n\n    assert isinstance(result.output, DeferredToolRequests)\n    requests = result.output\n    print(requests)\n    \"\"\"\n    DeferredToolRequests(\n        calls=[\n            ToolCallPart(\n                tool_name='calculate_answer',\n                args={\n                    'question': 'the ultimate question of life, the universe, and everything'\n                },\n                tool_call_id='pyd_ai_tool_call_id',\n            )\n        ],\n        approvals=[],\n    )\n    \"\"\"\n\n    done, _ = await asyncio.wait(tasks)  # (2)!\n    task_results = [task.result() for task in done]\n    task_results_by_tool_call_id = {result.tool_call_id: result.result for result in task_results}\n\n    results = DeferredToolResults()\n    for call in requests.calls:\n        try:\n            result = task_results_by_tool_call_id[call.tool_call_id]\n        except KeyError:\n            result = ModelRetry('No result for this tool call was found.')\n\n        results.calls[call.tool_call_id] = result\n\n    result = await agent.run(message_history=messages, deferred_tool_results=results)\n    print(result.output)\n    #> The answer to the ultimate question of life, the universe, and everything is 42.\n    print(result.all_messages())\n    \"\"\"\n    [\n        ModelRequest(\n            parts=[\n                UserPromptPart(\n                    content='Calculate the answer to the ultimate question of life, the universe, and everything',\n                    timestamp=datetime.datetime(...),\n                )\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='calculate_answer',\n                    args={\n                        'question': 'the ultimate question of life, the universe, and everything'\n                    },\n                    tool_call_id='pyd_ai_tool_call_id',\n                )\n            ],\n            usage=RequestUsage(input_tokens=63, output_tokens=13),\n            model_name='gpt-5',\n            timestamp=datetime.datetime(...),\n        ),\n        ModelRequest(\n            parts=[\n                ToolReturnPart(\n                    tool_name='calculate_answer',\n                    content=42,\n                    tool_call_id='pyd_ai_tool_call_id',\n                    timestamp=datetime.datetime(...),\n                )\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                TextPart(\n                    content='The answer to the ultimate question of life, the universe, and everything is 42.'\n                )\n            ],\n            usage=RequestUsage(input_tokens=64, output_tokens=28),\n            model_name='gpt-5',\n            timestamp=datetime.datetime(...),\n        ),\n    ]\n    \"\"\"\n```\n\n1. In reality, you'd likely use Celery or a similar task queue to run the task in the background.\n2. In reality, this would typically happen in a separate process that polls for the task status or is notified when all pending tasks are complete.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nWhen a tool is executed, its arguments (provided by the LLM) are first validated against the function's signature using Pydantic. If validation fails (e.g., due to incorrect types or missing required arguments), a `ValidationError` is raised, and the framework automatically generates a [`RetryPromptPart`](../api/messages/#pydantic_ai.messages.RetryPromptPart \"../api/messages/#pydantic_ai.messages.RetryPromptPart\") containing the validation details. This prompt is sent back to the LLM, informing it of the error and allowing it to correct the parameters and retry the tool call.\n\nBeyond automatic validation errors, the tool's own internal logic can also explicitly request a retry by raising the [`ModelRetry`](../api/exceptions/#pydantic_ai.exceptions.ModelRetry \"../api/exceptions/#pydantic_ai.exceptions.ModelRetry\") exception. This is useful for situations where the parameters were technically valid, but an issue occurred during execution (like a transient network error, or the tool determining the initial attempt needs modification).\n\n```\nfrom pydantic_ai import ModelRetry\n\n\ndef my_flaky_tool(query: str) -> str:\n    if query == 'bad':\n        # Tell the LLM the query was bad and it should try again\n        raise ModelRetry(\"The query 'bad' is not allowed. Please provide a different query.\")\n    # ... process query ...\n    return 'Success!'\n```\n\nRaising `ModelRetry` also generates a `RetryPromptPart` containing the exception message, which is sent back to the LLM to guide its next attempt. Both `ValidationError` and `ModelRetry` respect the `retries` setting configured on the `Tool` or `Agent`.\n\nWhen a model returns multiple tool calls in one response, Pydantic AI schedules them concurrently using `asyncio.create_task`.\n\nAsync functions are run on the event loop, while sync functions are offloaded to threads. To get the best performance, *always* use an async function *unless* you're doing blocking I/O (and there's no way to use a non-blocking library instead) or CPU-bound work (like `numpy` or `scikit-learn` operations), so that simple functions are not offloaded to threads unnecessarily.\n\nSee the [MCP Client](../mcp/client/ \"../mcp/client/\") documentation for how to use MCP servers with Pydantic AI as [toolsets](../toolsets/ \"../toolsets/\").\n\nIf you'd like to use a tool from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/ \"https://python.langchain.com/docs/integrations/tools/\") with Pydantic AI, you can use the [`tool_from_langchain`](../api/ext/#pydantic_ai.ext.langchain.tool_from_langchain \"../api/ext/#pydantic_ai.ext.langchain.tool_from_langchain\") convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid.\n\nYou will need to install the `langchain-community` package and any others required by the tool in question.\n\nHere is how you can use the LangChain `DuckDuckGoSearchRun` tool, which requires the `ddgs` package:\n\n```\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import tool_from_langchain\n\nsearch = DuckDuckGoSearchRun()\nsearch_tool = tool_from_langchain(search)\n\nagent = Agent(\n    'google-gla:gemini-2.0-flash',\n    tools=[search_tool],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')  # (1)!\nprint(result.output)\n#> Elden Ring Nightreign is planned to be released on May 30, 2025.\n```\n\n1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024).\n\nIf you'd like to use multiple LangChain tools or a LangChain [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits \"https://python.langchain.com/docs/concepts/tools/#toolkits\"), you can use the [`LangChainToolset`](../api/ext/#pydantic_ai.ext.langchain.LangChainToolset \"../api/ext/#pydantic_ai.ext.langchain.LangChainToolset\") [toolset](../toolsets/ \"../toolsets/\") which takes a list of LangChain tools:\n\n```\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])",
    "source_url": "https://ai.pydantic.dev/tools/",
    "header": "Set the system attribute of the test_model to 'openai'"
  },
  "b9690e36ee04072b3b0fe652d23fc15fc2cff9a30f5b5d5af32bcb0046473400": {
    "text": "```\n\nIf you'd like to use a tool from the [ACI.dev tool library](https://www.aci.dev/tools \"https://www.aci.dev/tools\") with Pydantic AI, you can use the [`tool_from_aci`](../api/ext/#pydantic_ai.ext.aci.tool_from_aci \"../api/ext/#pydantic_ai.ext.aci.tool_from_aci\") convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid.\n\nYou will need to install the `aci-sdk` package, set your ACI API key in the `ACI_API_KEY` environment variable, and pass your ACI \"linked account owner ID\" to the function.\n\nHere is how you can use the ACI.dev `TAVILY__SEARCH` tool:\n\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import tool_from_aci\n\ntavily_search = tool_from_aci(\n    'TAVILY__SEARCH',\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent(\n    'google-gla:gemini-2.0-flash',\n    tools=[tavily_search],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')  # (1)!\nprint(result.output)\n#> Elden Ring Nightreign is planned to be released on May 30, 2025.\n```\n\n1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024).\n\nIf you'd like to use multiple ACI.dev tools, you can use the [`ACIToolset`](../api/ext/#pydantic_ai.ext.aci.ACIToolset \"../api/ext/#pydantic_ai.ext.aci.ACIToolset\") [toolset](../toolsets/ \"../toolsets/\") which takes a list of ACI tool names as well as the `linked_account_owner_id`:\n\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n```",
    "source_url": "https://ai.pydantic.dev/tools/",
    "header": "..."
  },
  "11e66bca6243490c8eac10fc2d80427d151e039cb9fa3ad62c5b07c6ad854265": {
    "text": "A toolset represents a collection of [tools](../tools/ \"../tools/\") that can be registered with an agent in one go. They can be reused by different agents, swapped out at runtime or during testing, and composed in order to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. A toolset can contain locally defined functions, depend on an external service to provide them, or implement custom logic to list available tools and handle them being called.\n\nToolsets are used (among many other things) to define [MCP servers](../mcp/client/ \"../mcp/client/\") available to an agent. Pydantic AI includes many kinds of toolsets which are described below, and you can define a [custom toolset](#building-a-custom-toolset \"#building-a-custom-toolset\") by inheriting from the [`AbstractToolset`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset\") class.\n\nThe toolsets that will be available during an agent run can be specified in four different ways:\n\n* at agent construction time, via the [`toolsets`](../api/agent/#pydantic_ai.agent.Agent.__init__ \"../api/agent/#pydantic_ai.agent.Agent.__init__\") keyword argument to `Agent`, which takes toolset instances as well as functions that generate toolsets [dynamically](#dynamically-building-a-toolset \"#dynamically-building-a-toolset\") based on the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\")\n* at agent run time, via the `toolsets` keyword argument to [`agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\"), [`agent.run_sync()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync\"), [`agent.run_stream()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\"), or [`agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\"). These toolsets will be additional to those registered on the `Agent`\n* [dynamically](#dynamically-building-a-toolset \"#dynamically-building-a-toolset\"), via the [`@agent.toolset`](../api/agent/#pydantic_ai.agent.Agent.toolset \"../api/agent/#pydantic_ai.agent.Agent.toolset\") decorator which lets you build a toolset based on the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\")\n* as a contextual override, via the `toolsets` keyword argument to the [`agent.override()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\") context manager. These toolsets will replace those provided at agent construction or run time during the life of the context manager\n\ntoolsets.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import FunctionToolset\n\n\ndef agent_tool():\n    return \"I'm registered directly on the agent\"\n\n\ndef extra_tool():\n    return \"I'm passed as an extra tool for a specific run\"\n\n\ndef override_tool():\n    return 'I override all other tools'\n\n\nagent_toolset = FunctionToolset(tools=[agent_tool]) # (1)!\nextra_toolset = FunctionToolset(tools=[extra_tool])\noverride_toolset = FunctionToolset(tools=[override_tool])\n\ntest_model = TestModel() # (2)!\nagent = Agent(test_model, toolsets=[agent_toolset])\n\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['agent_tool']\n\nresult = agent.run_sync('What tools are available?', toolsets=[extra_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['agent_tool', 'extra_tool']\n\nwith agent.override(toolsets=[override_toolset]):\n    result = agent.run_sync('What tools are available?', toolsets=[extra_toolset]) # (3)!\n    print([t.name for t in test_model.last_model_request_parameters.function_tools])\n    #> ['override_tool']\n```\n\n1. The [`FunctionToolset`](../api/toolsets/#pydantic_ai.toolsets.FunctionToolset \"../api/toolsets/#pydantic_ai.toolsets.FunctionToolset\") will be explained in detail in the next section.\n2. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n3. This `extra_toolset` will be ignored because we're inside an override context.\n\n*(This example is complete, it can be run \"as is\")*\n\nAs the name suggests, a [`FunctionToolset`](../api/toolsets/#pydantic_ai.toolsets.FunctionToolset \"../api/toolsets/#pydantic_ai.toolsets.FunctionToolset\") makes locally defined functions available as tools.\n\nFunctions can be added as tools in three different ways:\n\nFunctions registered in any of these ways can define an initial `ctx: RunContext` argument in order to receive the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\"). The `add_function()` and `add_tool()` methods can also be used from a tool function to dynamically register new tools during a run to be available in future run steps.\n\nfunction\\_toolset.py\n\n```\nfrom datetime import datetime\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import FunctionToolset\n\n\ndef temperature_celsius(city: str) -> float:\n    return 21.0\n\n\ndef temperature_fahrenheit(city: str) -> float:\n    return 69.8\n\n\nweather_toolset = FunctionToolset(tools=[temperature_celsius, temperature_fahrenheit])\n\n\n@weather_toolset.tool\ndef conditions(ctx: RunContext, city: str) -> str:\n    if ctx.run_step % 2 == 0:\n        return \"It's sunny\"\n    else:\n        return \"It's raining\"\n\n\ndatetime_toolset = FunctionToolset()\ndatetime_toolset.add_function(lambda: datetime.now(), name='now')\n\ntest_model = TestModel()  # (1)!\nagent = Agent(test_model)\n\nresult = agent.run_sync('What tools are available?', toolsets=[weather_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['temperature_celsius', 'temperature_fahrenheit', 'conditions']\n\nresult = agent.run_sync('What tools are available?', toolsets=[datetime_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['now']\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n\n*(This example is complete, it can be run \"as is\")*\n\nToolsets can be composed to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. Multiple toolsets can also be combined into one.\n\n[`CombinedToolset`](../api/toolsets/#pydantic_ai.toolsets.CombinedToolset \"../api/toolsets/#pydantic_ai.toolsets.CombinedToolset\") takes a list of toolsets and lets them be used as one.\n\ncombined\\_toolset.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import CombinedToolset\n\nfrom function_toolset import datetime_toolset, weather_toolset\n\ncombined_toolset = CombinedToolset([weather_toolset, datetime_toolset])\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[combined_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['temperature_celsius', 'temperature_fahrenheit', 'conditions', 'now']\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n\n*(This example is complete, it can be run \"as is\")*\n\n[`FilteredToolset`](../api/toolsets/#pydantic_ai.toolsets.FilteredToolset \"../api/toolsets/#pydantic_ai.toolsets.FilteredToolset\") wraps a toolset and filters available tools ahead of each step of the run based on a user-defined function that is passed the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and each tool's [`ToolDefinition`](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\") and returns a boolean to indicate whether or not a given tool should be available.\n\nTo easily chain different modifications, you can also call [`filtered()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.filtered \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.filtered\") on any toolset instead of directly constructing a `FilteredToolset`.\n\nfiltered\\_toolset.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nfrom combined_toolset import combined_toolset\n\nfiltered_toolset = combined_toolset.filtered(lambda ctx, tool_def: 'fahrenheit' not in tool_def.name)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[filtered_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['weather_temperature_celsius', 'weather_conditions', 'datetime_now']\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n\n*(This example is complete, it can be run \"as is\")*\n\n[`PrefixedToolset`](../api/toolsets/#pydantic_ai.toolsets.PrefixedToolset \"../api/toolsets/#pydantic_ai.toolsets.PrefixedToolset\") wraps a toolset and adds a prefix to each tool name to prevent tool name conflicts between different toolsets.\n\nTo easily chain different modifications, you can also call [`prefixed()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.prefixed \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.prefixed\") on any toolset instead of directly constructing a `PrefixedToolset`.\n\ncombined\\_toolset.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import CombinedToolset\n\nfrom function_toolset import datetime_toolset, weather_toolset\n\ncombined_toolset = CombinedToolset(\n    [\n        weather_toolset.prefixed('weather'),\n        datetime_toolset.prefixed('datetime')\n    ]\n)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[combined_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n\"\"\"\n[\n    'weather_temperature_celsius',\n    'weather_temperature_fahrenheit',\n    'weather_conditions',\n    'datetime_now',\n]\n\"\"\"\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n\n*(This example is complete, it can be run \"as is\")*\n\n[`RenamedToolset`](../api/toolsets/#pydantic_ai.toolsets.RenamedToolset \"../api/toolsets/#pydantic_ai.toolsets.RenamedToolset\") wraps a toolset and lets you rename tools using a dictionary mapping new names to original names. This is useful when the names provided by a toolset are ambiguous or would conflict with tools defined by other toolsets, but [prefixing them](#prefixing-tool-names \"#prefixing-tool-names\") creates a name that is unnecessarily long or could be confusing to the model.\n\nTo easily chain different modifications, you can also call [`renamed()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.renamed \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.renamed\") on any toolset instead of directly constructing a `RenamedToolset`.\n\nrenamed\\_toolset.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nfrom combined_toolset import combined_toolset\n\nrenamed_toolset = combined_toolset.renamed(\n    {\n        'current_time': 'datetime_now',\n        'temperature_celsius': 'weather_temperature_celsius',\n        'temperature_fahrenheit': 'weather_temperature_fahrenheit'\n    }\n)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[renamed_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n\"\"\"\n['temperature_celsius', 'temperature_fahrenheit', 'weather_conditions', 'current_time']\n\"\"\"\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n\n*(This example is complete, it can be run \"as is\")*\n\n[`PreparedToolset`](../api/toolsets/#pydantic_ai.toolsets.PreparedToolset \"../api/toolsets/#pydantic_ai.toolsets.PreparedToolset\") lets you modify the entire list of available tools ahead of each step of the agent run using a user-defined function that takes the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and a list of [`ToolDefinition`s](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\") and returns a list of modified `ToolDefinition`s.\n\nThis is the toolset-specific equivalent of the [`prepare_tools`](../tools/#prepare-tools \"../tools/#prepare-tools\") argument to `Agent` that prepares all tool definitions registered on an agent across toolsets.\n\nNote that it is not possible to add or rename tools using `PreparedToolset`. Instead, you can use [`FunctionToolset.add_function()`](#function-toolset \"#function-toolset\") or [`RenamedToolset`](#renaming-tools \"#renaming-tools\").\n\nTo easily chain different modifications, you can also call [`prepared()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.prepared \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.prepared\") on any toolset instead of directly constructing a `PreparedToolset`.\n\nprepared\\_toolset.py\n\n```\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\nfrom renamed_toolset import renamed_toolset\n\ndescriptions = {\n    'temperature_celsius': 'Get the temperature in degrees Celsius',\n    'temperature_fahrenheit': 'Get the temperature in degrees Fahrenheit',\n    'weather_conditions': 'Get the current weather conditions',\n    'current_time': 'Get the current time',\n}\n\nasync def add_descriptions(ctx: RunContext, tool_defs: list[ToolDefinition]) -> list[ToolDefinition] | None:\n    return [\n        replace(tool_def, description=description)\n        if (description := descriptions.get(tool_def.name, None))\n        else tool_def\n        for tool_def\n        in tool_defs\n    ]\n\nprepared_toolset = renamed_toolset.prepared(add_descriptions)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[prepared_toolset])\nresult = agent.run_sync('What tools are available?')\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='temperature_celsius',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the temperature in degrees Celsius',\n    ),\n    ToolDefinition(\n        name='temperature_fahrenheit',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the temperature in degrees Fahrenheit',\n    ),\n    ToolDefinition(\n        name='weather_conditions',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the current weather conditions',\n    ),\n    ToolDefinition(\n        name='current_time',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {},\n            'type': 'object',\n        },\n        description='Get the current time',\n    ),\n]\n\"\"\"\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n\n[`ApprovalRequiredToolset`](../api/toolsets/#pydantic_ai.toolsets.ApprovalRequiredToolset \"../api/toolsets/#pydantic_ai.toolsets.ApprovalRequiredToolset\") wraps a toolset and lets you dynamically [require approval](../tools/#human-in-the-loop-tool-approval \"../tools/#human-in-the-loop-tool-approval\") for a given tool call based on a user-defined function that is passed the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\"), the tool's [`ToolDefinition`](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\"), and the validated tool call arguments. If no function is provided, all tool calls will require approval.\n\nTo easily chain different modifications, you can also call [`approval_required()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.approval_required \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.approval_required\") on any toolset instead of directly constructing a `ApprovalRequiredToolset`.\n\nSee the [Human-in-the-Loop Tool Approval](../tools/#human-in-the-loop-tool-approval \"../tools/#human-in-the-loop-tool-approval\") documentation for more information on how to handle agent runs that call tools that require approval and how to pass in the results.\n\napproval\\_required\\_toolset.py\n\n```\nfrom pydantic_ai import Agent, DeferredToolRequests, DeferredToolResults\nfrom pydantic_ai.models.test import TestModel\n\nfrom prepared_toolset import prepared_toolset\n\napproval_required_toolset = prepared_toolset.approval_required(lambda ctx, tool_def, tool_args: tool_def.name.startswith('temperature'))\n\ntest_model = TestModel(call_tools=['temperature_celsius', 'temperature_fahrenheit']) # (1)!\nagent = Agent(\n    test_model,\n    toolsets=[approval_required_toolset],\n    output_type=[str, DeferredToolRequests],\n)\nresult = agent.run_sync('Call the temperature tools')\nmessages = result.all_messages()\nprint(result.output)\n\"\"\"\nDeferredToolRequests(\n    calls=[],\n    approvals=[\n        ToolCallPart(\n            tool_name='temperature_celsius',\n            args={'city': 'a'},\n            tool_call_id='pyd_ai_tool_call_id__temperature_celsius',\n        ),\n        ToolCallPart(\n            tool_name='temperature_fahrenheit',\n            args={'city': 'a'},\n            tool_call_id='pyd_ai_tool_call_id__temperature_fahrenheit',\n        ),\n    ],\n)\n\"\"\"\n\nresult = agent.run_sync(\n    message_history=messages,\n    deferred_tool_results=DeferredToolResults(\n        approvals={\n            'pyd_ai_tool_call_id__temperature_celsius': True,\n            'pyd_ai_tool_call_id__temperature_fahrenheit': False,\n        }\n    )\n)\nprint(result.output)\n#> {\"temperature_celsius\":21.0,\"temperature_fahrenheit\":\"The tool call was denied.\"}\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to specify which tools to call.\n\n*(This example is complete, it can be run \"as is\")*\n\n[`WrapperToolset`](../api/toolsets/#pydantic_ai.toolsets.WrapperToolset \"../api/toolsets/#pydantic_ai.toolsets.WrapperToolset\") wraps another toolset and delegates all responsibility to it.\n\nIt is is a no-op by default, but you can subclass `WrapperToolset` to change the wrapped toolset's tool execution behavior by overriding the [`call_tool()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.call_tool \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.call_tool\") method.\n\nlogging\\_toolset.py\n\n```\nimport asyncio\n\nfrom typing_extensions import Any\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.toolsets import ToolsetTool, WrapperToolset\n\nfrom prepared_toolset import prepared_toolset\n\nLOG = []\n\nclass LoggingToolset(WrapperToolset):\n    async def call_tool(self, name: str, tool_args: dict[str, Any], ctx: RunContext, tool: ToolsetTool) -> Any:\n        LOG.append(f'Calling tool {name!r} with args: {tool_args!r}')\n        try:\n            await asyncio.sleep(0.1 * len(LOG)) # (1)!\n\n            result = await super().call_tool(name, tool_args, ctx, tool)\n            LOG.append(f'Finished calling tool {name!r} with result: {result!r}')\n        except Exception as e:\n            LOG.append(f'Error calling tool {name!r}: {e}')\n            raise e\n        else:\n            return result\n\n\nlogging_toolset = LoggingToolset(prepared_toolset)\n\nagent = Agent(TestModel(), toolsets=[logging_toolset]) # (2)!\nresult = agent.run_sync('Call all the tools')\nprint(LOG)\n\"\"\"\n[\n    \"Calling tool 'temperature_celsius' with args: {'city': 'a'}\",\n    \"Calling tool 'temperature_fahrenheit' with args: {'city': 'a'}\",\n    \"Calling tool 'weather_conditions' with args: {'city': 'a'}\",\n    \"Calling tool 'current_time' with args: {}\",\n    \"Finished calling tool 'temperature_celsius' with result: 21.0\",\n    \"Finished calling tool 'temperature_fahrenheit' with result: 69.8\",\n    'Finished calling tool \\'weather_conditions\\' with result: \"It\\'s raining\"',\n    \"Finished calling tool 'current_time' with result: datetime.datetime(...)\",\n]\n\"\"\"\n```\n\n1. All docs examples are tested in CI and their their output is verified, so we need `LOG` to always have the same order whenever this code is run. Since the tools could finish in any order, we sleep an increasing amount of time based on which number tool call we are to ensure that they finish (and log) in the same order they were called in.\n2. We use [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here as it will automatically call each tool.\n\n*(This example is complete, it can be run \"as is\")*\n\nIf your agent needs to be able to call [external tools](../tools/#external-tool-execution \"../tools/#external-tool-execution\") that are provided and executed by an upstream service or frontend, you can build an [`ExternalToolset`](../api/toolsets/#pydantic_ai.toolsets.ExternalToolset \"../api/toolsets/#pydantic_ai.toolsets.ExternalToolset\") from a list of [`ToolDefinition`s](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\") containing the tool names, arguments JSON schemas, and descriptions.\n\nWhen the model calls an external tool, the call is considered to be [\"deferred\"](../tools/#deferred-tools \"../tools/#deferred-tools\"), and the agent run will end with a [`DeferredToolRequests`](../api/output/#pydantic_ai.output.DeferredToolRequests \"../api/output/#pydantic_ai.output.DeferredToolRequests\") output object with a `calls` list holding [`ToolCallPart`s](../api/messages/#pydantic_ai.messages.ToolCallPart \"../api/messages/#pydantic_ai.messages.ToolCallPart\") containing the tool name, validated arguments, and a unique tool call ID, which are expected to be passed to the upstream service or frontend that will produce the results.\n\nWhen the tool call results are received from the upstream service or frontend, you can build a [`DeferredToolResults`](../api/tools/#pydantic_ai.tools.DeferredToolResults \"../api/tools/#pydantic_ai.tools.DeferredToolResults\") object with a `calls` dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [`ToolReturn`](../tools/#advanced-tool-returns \"../tools/#advanced-tool-returns\") object, or a [`ModelRetry`](../api/exceptions/#pydantic_ai.exceptions.ModelRetry \"../api/exceptions/#pydantic_ai.exceptions.ModelRetry\") exception in case the tool call failed and the model should [try again](../tools/#tool-retries \"../tools/#tool-retries\"). This `DeferredToolResults` object can then be provided to one of the agent run methods as `deferred_tool_results`, alongside the original run's [message history](../message-history/ \"../message-history/\").\n\nNote that you need to add `DeferredToolRequests` to the `Agent`'s or `agent.run()`'s [`output_type`](../output/#structured-output \"../output/#structured-output\") so that the possible types of the agent run output are correctly inferred. For more information, see the [Deferred Tools](../tools/#deferred-tools \"../tools/#deferred-tools\") documentation.\n\nTo demonstrate, let us first define a simple agent *without* deferred tools:\n\ndeferred\\_toolset\\_agent.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.function import FunctionToolset\n\ntoolset = FunctionToolset()\n\n\n@toolset.tool\ndef get_default_language():\n    return 'en-US'\n\n\n@toolset.tool\ndef get_user_name():\n    return 'David'\n\n\nclass PersonalizedGreeting(BaseModel):\n    greeting: str\n    language_code: str\n\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset], output_type=PersonalizedGreeting)\n\nresult = agent.run_sync('Greet the user in a personalized way')\nprint(repr(result.output))\n#> PersonalizedGreeting(greeting='Hello, David!', language_code='en-US')\n```\n\nNext, let's define a function that represents a hypothetical \"run agent\" API endpoint that can be called by the frontend and takes a list of messages to send to the model, a list of frontend tool definitions, and optional deferred tool results. This is where `ExternalToolset`, `DeferredToolRequests`, and `DeferredToolResults` come in:\n\ndeferred\\_toolset\\_api.py\n\n```\nfrom pydantic_ai import DeferredToolRequests, DeferredToolResults, ToolDefinition\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_ai.toolsets import ExternalToolset\n\nfrom deferred_toolset_agent import PersonalizedGreeting, agent\n\n\ndef run_agent(\n    messages: list[ModelMessage] = [],\n    frontend_tools: list[ToolDefinition] = {},\n    deferred_tool_results: DeferredToolResults | None = None,\n) -> tuple[PersonalizedGreeting | DeferredToolRequests, list[ModelMessage]]:\n    deferred_toolset = ExternalToolset(frontend_tools)\n    result = agent.run_sync(\n        toolsets=[deferred_toolset], # (1)!\n        output_type=[agent.output_type, DeferredToolRequests], # (2)!\n        message_history=messages, # (3)!\n        deferred_tool_results=deferred_tool_results,\n    )\n    return result.output, result.new_messages()\n```\n\n1. As mentioned in the [Deferred Tools](../tools/#deferred-tools \"../tools/#deferred-tools\") documentation, these `toolsets` are additional to those provided to the `Agent` constructor\n2. As mentioned in the [Deferred Tools](../tools/#deferred-tools \"../tools/#deferred-tools\") documentation, this `output_type` overrides the one provided to the `Agent` constructor, so we have to make sure to not lose it\n3. We don't include an `user_prompt` keyword argument as we expect the frontend to provide it via `messages`\n\nNow, imagine that the code below is implemented on the frontend, and `run_agent` stands in for an API call to the backend that runs the agent. This is where we actually execute the deferred tool calls and start a new run with the new result included:\n\ndeferred\\_tools.py\n\n```\nfrom pydantic_ai import (\n    DeferredToolRequests,\n    DeferredToolResults,\n    ModelRetry,\n    ToolDefinition,\n)\nfrom pydantic_ai.messages import ModelMessage, ModelRequest, UserPromptPart\n\nfrom deferred_toolset_api import run_agent\n\nfrontend_tool_definitions = [\n    ToolDefinition(\n        name='get_preferred_language',\n        parameters_json_schema={'type': 'object', 'properties': {'default_language': {'type': 'string'}}},\n        description=\"Get the user's preferred language from their browser\",\n    )\n]\n\ndef get_preferred_language(default_language: str) -> str:\n    return 'es-MX' # (1)!\n\nfrontend_tool_functions = {'get_preferred_language': get_preferred_language}\n\nmessages: list[ModelMessage] = [\n    ModelRequest(\n        parts=[\n            UserPromptPart(content='Greet the user in a personalized way')\n        ]\n    )\n]\n\ndeferred_tool_results: DeferredToolResults | None = None\n\nfinal_output = None\nwhile True:\n    output, new_messages = run_agent(messages, frontend_tool_definitions, deferred_tool_results)\n    messages += new_messages\n\n    if not isinstance(output, DeferredToolRequests):\n        final_output = output\n        break\n\n    print(output.calls)\n    \"\"\"\n    [\n        ToolCallPart(\n            tool_name='get_preferred_language',\n            args={'default_language': 'en-US'},\n            tool_call_id='pyd_ai_tool_call_id',\n        )\n    ]\n    \"\"\"\n    deferred_tool_results = DeferredToolResults()\n    for tool_call in output.calls:\n        if function := frontend_tool_functions.get(tool_call.tool_name):\n            result = function(**tool_call.args_as_dict())\n        else:\n            result = ModelRetry(f'Unknown tool {tool_call.tool_name!r}')\n        deferred_tool_results.calls[tool_call.tool_call_id] = result\n\nprint(repr(final_output))\n\"\"\"\nPersonalizedGreeting(greeting='Hola, David! Espero que tengas un gran día!', language_code='es-MX')\n\"\"\"\n```\n\n1. Imagine that this returns the frontend [`navigator.language`](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/language \"https://developer.mozilla.org/en-US/docs/Web/API/Navigator/language\").\n\n*(This example is complete, it can be run \"as is\")*\n\nToolsets can be built dynamically ahead of each agent run or run step using a function that takes the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and returns a toolset or `None`. This is useful when a toolset (like an MCP server) depends on information specific to an agent run, like its [dependencies](../dependencies/ \"../dependencies/\").\n\nTo register a dynamic toolset, you can pass a function that takes [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") to the `toolsets` argument of the `Agent` constructor, or you can wrap a compliant function in the [`@agent.toolset`](../api/agent/#pydantic_ai.agent.Agent.toolset \"../api/agent/#pydantic_ai.agent.Agent.toolset\") decorator.\n\nBy default, the function will be called again ahead of each agent run step. If you are using the decorator, you can optionally provide a `per_run_step=False` argument to indicate that the toolset only needs to be built once for the entire run.\n\ndynamic\\_toolset.py\n\n```\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\n\nfrom function_toolset import datetime_toolset, weather_toolset\n\n\n@dataclass\nclass ToggleableDeps:\n    active: Literal['weather', 'datetime']\n\n    def toggle(self):\n        if self.active == 'weather':\n            self.active = 'datetime'\n        else:\n            self.active = 'weather'\n\ntest_model = TestModel()  # (1)!\nagent = Agent(\n    test_model,\n    deps_type=ToggleableDeps  # (2)!\n)\n\n@agent.toolset\ndef toggleable_toolset(ctx: RunContext[ToggleableDeps]):\n    if ctx.deps.active == 'weather':\n        return weather_toolset\n    else:\n        return datetime_toolset\n\n@agent.tool\ndef toggle(ctx: RunContext[ToggleableDeps]):\n    ctx.deps.toggle()\n\ndeps = ToggleableDeps('weather')\n\nresult = agent.run_sync('Toggle the toolset', deps=deps)\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])  # (3)!\n#> ['toggle', 'now']\n\nresult = agent.run_sync('Toggle the toolset', deps=deps)\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['toggle', 'temperature_celsius', 'temperature_fahrenheit', 'conditions']\n```\n\n1. We're using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") here because it makes it easy to see which tools were available on each run.\n2. We're using the agent's dependencies to give the `toggle` tool access to the `active` via the `RunContext` argument.\n3. This shows the available tools *after* the `toggle` tool was executed, as the \"last model request\" was the one that returned the `toggle` tool result to the model.\n\n*(This example is complete, it can be run \"as is\")*\n\nTo define a fully custom toolset with its own logic to list available tools and handle them being called, you can subclass [`AbstractToolset`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset\") and implement the [`get_tools()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.get_tools \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.get_tools\") and [`call_tool()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.call_tool \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.call_tool\") methods.\n\nIf you want to reuse a network connection or session across tool listings and calls during an agent run, you can implement [`__aenter__()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.__aenter__ \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.__aenter__\") and [`__aexit__()`](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.__aexit__ \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.__aexit__\").",
    "source_url": "https://ai.pydantic.dev/toolsets/",
    "header": "Introduction"
  },
  "8c35581cbbafb0796317f611393e8e95c91574477bcd5d3608f90a3c1f3e379e": {
    "text": "See the [MCP Client](../mcp/client/ \"../mcp/client/\") documentation for how to use MCP servers with Pydantic AI.\n\nIf you'd like to use tools or a [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits \"https://python.langchain.com/docs/concepts/tools/#toolkits\") from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/ \"https://python.langchain.com/docs/integrations/tools/\") with Pydantic AI, you can use the [`LangChainToolset`](../api/ext/#pydantic_ai.ext.langchain.LangChainToolset \"../api/ext/#pydantic_ai.ext.langchain.LangChainToolset\") which takes a list of LangChain tools. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid.\n\nYou will need to install the `langchain-community` package and any others required by the tools in question.\n\n```\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])",
    "source_url": "https://ai.pydantic.dev/toolsets/",
    "header": "MCP Servers"
  },
  "a98aa4dc28bac2208852f0d75d79bbece2bde3892370955e4896064c2520da98": {
    "text": "```\n\nIf you'd like to use tools from the [ACI.dev tool library](https://www.aci.dev/tools \"https://www.aci.dev/tools\") with Pydantic AI, you can use the [`ACIToolset`](../api/ext/#pydantic_ai.ext.aci.ACIToolset \"../api/ext/#pydantic_ai.ext.aci.ACIToolset\") [toolset](./ \"./\") which takes a list of ACI tool names as well as the `linked_account_owner_id`. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid.\n\nYou will need to install the `aci-sdk` package, set your ACI API key in the `ACI_API_KEY` environment variable, and pass your ACI \"linked account owner ID\" to the function.\n\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-4o', toolsets=[toolset])\n```",
    "source_url": "https://ai.pydantic.dev/toolsets/",
    "header": "..."
  },
  "b301bca721d9d50ff10878be23c3455d154263669152556ebfa41311e9059a49": {
    "text": "\"Output\" refers to the final value returned from [running an agent](../agents/#running-agents \"../agents/#running-agents\"). This can be either plain text, [structured data](#structured-output \"#structured-output\"), or the result of a [function](#output-functions \"#output-functions\") called with arguments provided by the model.\n\nThe output is wrapped in [`AgentRunResult`](../api/agent/#pydantic_ai.agent.AgentRunResult \"../api/agent/#pydantic_ai.agent.AgentRunResult\") or [`StreamedRunResult`](../api/result/#pydantic_ai.result.StreamedRunResult \"../api/result/#pydantic_ai.result.StreamedRunResult\") so that you can access other data, like [usage](../api/usage/#pydantic_ai.usage.RunUsage \"../api/usage/#pydantic_ai.usage.RunUsage\") of the run and [message history](../message-history/#accessing-messages-from-results \"../message-history/#accessing-messages-from-results\").\n\nBoth `AgentRunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\n\nA run ends when the model responds with one of the structured output types, or, if no output type is specified or `str` is one of the allowed options, when a plain text response is received. A run can also be cancelled if usage limits are exceeded, see [Usage Limits](../agents/#usage-limits \"../agents/#usage-limits\").\n\nHere's an example using a Pydantic model as the `output_type`, forcing the model to respond with data matching our specification:\n\nolympics.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nagent = Agent('google-gla:gemini-1.5-flash', output_type=CityLocation)\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Output"
  },
  "e936bea92028a70eb39b587f66c5388939726a8df073c82b3e2ba7c847340c64": {
    "text": "The [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\") class constructor takes an `output_type` argument that takes one or more types or [output functions](#output-functions \"#output-functions\"). It supports simple scalar types, list and dict types (including `TypedDict`s and [`StructuredDict`s](#structured-dict \"#structured-dict\")), dataclasses and Pydantic models, as well as type unions -- generally everything supported as type hints in a Pydantic model. You can also pass a list of multiple choices.\n\nBy default, Pydantic AI leverages the model's tool calling capability to make it return structured data. When multiple output types are specified (in a union or list), each member is registered with the model as a separate output tool in order to reduce the complexity of the schema and maximise the chances a model will respond correctly. This has been shown to work well across a wide range of models. If you'd like to change the names of the output tools, use a model's native structured output feature, or pass the output schema to the model in its [instructions](../agents/#instructions \"../agents/#instructions\"), you can use an [output mode](#output-modes \"#output-modes\") marker class.\n\nWhen no output type is specified, or when `str` is among the output types, any plain text response from the model will be used as the output data.\nIf `str` is not among the output types, the model is forced to return structured data or call an output function.\n\nIf the output type schema is not of type `\"object\"` (e.g. it's `int` or `list[int]`), the output type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\n\nStructured outputs (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model.\n\nType checking considerations\n\nThe Agent class is generic in its output type, and this type is carried through to `AgentRunResult.output` and `StreamedRunResult.output` so that your IDE or static type checker can warn you when your code doesn't properly take into account all the possible values those outputs could have.\n\nStatic type checkers like pyright and mypy will do their best to infer the agent's output type from the `output_type` you've specified, but they're not always able to do so correctly when you provide functions or multiple types in a union or list, even though Pydantic AI will behave correctly. When this happens, your type checker will complain even when you're confident you've passed a valid `output_type`, and you'll need to help the type checker by explicitly specifying the generic parameters on the `Agent` constructor. This is shown in the second example below and the output functions example further down.\n\nSpecifically, there are three valid uses of `output_type` where you'll need to do this:\n\n1. When using a union of types, e.g. `output_type=Foo | Bar`. Until [PEP-747](https://peps.python.org/pep-0747/ \"https://peps.python.org/pep-0747/\") \"Annotating Type Forms\" lands in Python 3.15, type checkers do not consider these a valid value for `output_type`. In addition to the generic parameters on the `Agent` constructor, you'll need to add `# type: ignore` to the line that passes the union to `output_type`. Alternatively, you can use a list: `output_type=[Foo, Bar]`.\n2. With mypy: When using a list, as a functionally equivalent alternative to a union, or because you're passing in [output functions](#output-functions \"#output-functions\"). Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19142 \"https://github.com/python/mypy/issues/19142\") with mypy to try and get this fixed.\n3. With mypy: when using an async output function. Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19143 \"https://github.com/python/mypy/issues/19143\") with mypy to try and get this fixed.\n\nHere's an example of returning either text or structured data:\n\nbox\\_or\\_error.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n\nclass Box(BaseModel):\n    width: int\n    height: int\n    depth: int\n    units: str\n\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    output_type=[Box, str], # (1)!\n    system_prompt=(\n        \"Extract me the dimensions of a box, \"\n        \"if you can't extract all data, ask the user to try again.\"\n    ),\n)\n\nresult = agent.run_sync('The box is 10x20x30')\nprint(result.output)\n#> Please provide the units for the dimensions (e.g., cm, in, m).\n\nresult = agent.run_sync('The box is 10x20x30 cm')\nprint(result.output)\n#> width=10 height=20 depth=30 units='cm'\n```\n\n1. This could also have been a union: `output_type=Box | str`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n*(This example is complete, it can be run \"as is\")*\n\nHere's an example of using a union return type, which will register multiple output tools and wrap non-object schemas in an object:\n\ncolors\\_or\\_sizes.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent[None, list[str] | list[int]](\n    'openai:gpt-4o-mini',\n    output_type=list[str] | list[int],  # type: ignore # (1)!\n    system_prompt='Extract either colors or sizes from the shapes provided.',\n)\n\nresult = agent.run_sync('red square, blue circle, green triangle')\nprint(result.output)\n#> ['red', 'blue', 'green']\n\nresult = agent.run_sync('square size 10, circle size 20, triangle size 30')\nprint(result.output)\n#> [10, 20, 30]\n```\n\n1. As explained in the \"Type checking considerations\" section above, using a union rather than a list requires explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Output data"
  },
  "58c27f02be04373bdcfb16f5a2a0349098419b20cff7d8f3c1a1285f3e514b8d": {
    "text": "Instead of plain text or structured data, you may want the output of your agent run to be the result of a function called with arguments provided by the model, for example to further process or validate the data provided through the arguments (with the option to tell the model to try again), or to hand off to another agent.\n\nOutput functions are similar to [function tools](../tools/ \"../tools/\"), but the model is forced to call one of them, the call ends the agent run, and the result is not passed back to the model.\n\nAs with tool functions, output function arguments provided by the model are validated using Pydantic, they can optionally take [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") as the first argument, and they can raise [`ModelRetry`](../api/exceptions/#pydantic_ai.exceptions.ModelRetry \"../api/exceptions/#pydantic_ai.exceptions.ModelRetry\") to ask the model to try again with modified arguments (or with a different output type).\n\nTo specify output functions, you set the agent's `output_type` to either a single function (or bound instance method), or a list of functions. The list can also contain other output types like simple scalars or entire Pydantic models.\nYou typically do not want to also register your output function as a tool (using the `@agent.tool` decorator or `tools` argument), as this could confuse the model about which it should be calling.\n\nHere's an example of all of these features in action:\n\noutput\\_functions.py\n\n```\nimport re\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext, UnexpectedModelBehavior\n\n\nclass Row(BaseModel):\n    name: str\n    country: str\n\n\ntables = {\n    'capital_cities': [\n        Row(name='Amsterdam', country='Netherlands'),\n        Row(name='Mexico City', country='Mexico'),\n    ]\n}\n\n\nclass SQLFailure(BaseModel):\n    \"\"\"An unrecoverable failure. Only use this when you can't change the query to make it work.\"\"\"\n\n    explanation: str\n\n\ndef run_sql_query(query: str) -> list[Row]:\n    \"\"\"Run a SQL query on the database.\"\"\"\n\n    select_table = re.match(r'SELECT (.+) FROM (\\w+)', query)\n    if select_table:\n        column_names = select_table.group(1)\n        if column_names != '*':\n            raise ModelRetry(\"Only 'SELECT *' is supported, you'll have to do column filtering manually.\")\n\n        table_name = select_table.group(2)\n        if table_name not in tables:\n            raise ModelRetry(\n                f\"Unknown table '{table_name}' in query '{query}'. Available tables: {', '.join(tables.keys())}.\"\n            )\n\n        return tables[table_name]\n\n    raise ModelRetry(f\"Unsupported query: '{query}'.\")\n\n\nsql_agent = Agent[None, list[Row] | SQLFailure](\n    'openai:gpt-4o',\n    output_type=[run_sql_query, SQLFailure],\n    instructions='You are a SQL agent that can run SQL queries on a database.',\n)\n\n\nasync def hand_off_to_sql_agent(ctx: RunContext, query: str) -> list[Row]:\n    \"\"\"I take natural language queries, turn them into SQL, and run them on a database.\"\"\"\n\n    # Drop the final message with the output tool call, as it shouldn't be passed on to the SQL agent\n    messages = ctx.messages[:-1]\n    try:\n        result = await sql_agent.run(query, message_history=messages)\n        output = result.output\n        if isinstance(output, SQLFailure):\n            raise ModelRetry(f'SQL agent failed: {output.explanation}')\n        return output\n    except UnexpectedModelBehavior as e:\n        # Bubble up potentially retryable errors to the router agent\n        if (cause := e.__cause__) and isinstance(cause, ModelRetry):\n            raise ModelRetry(f'SQL agent failed: {cause.message}') from e\n        else:\n            raise\n\n\nclass RouterFailure(BaseModel):\n    \"\"\"Use me when no appropriate agent is found or the used agent failed.\"\"\"\n\n    explanation: str\n\n\nrouter_agent = Agent[None, list[Row] | RouterFailure](\n    'openai:gpt-4o',\n    output_type=[hand_off_to_sql_agent, RouterFailure],\n    instructions='You are a router to other agents. Never try to solve a problem yourself, just pass it on.',\n)\n\nresult = router_agent.run_sync('Select the names and countries of all capitals')\nprint(result.output)\n\"\"\"\n[\n    Row(name='Amsterdam', country='Netherlands'),\n    Row(name='Mexico City', country='Mexico'),\n]\n\"\"\"\n\nresult = router_agent.run_sync('Select all pets')\nprint(repr(result.output))\n\"\"\"\nRouterFailure(explanation=\"The requested table 'pets' does not exist in the database. The only available table is 'capital_cities', which does not contain data about pets.\")\n\"\"\"\n\nresult = router_agent.run_sync('How do I fly from Amsterdam to Mexico City?')\nprint(repr(result.output))\n\"\"\"\nRouterFailure(explanation='I am not equipped to provide travel information, such as flights from Amsterdam to Mexico City.')\n\"\"\"\n```",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Output functions"
  },
  "33b40681bc2b2c3d6866ba80e668cb84795f9fbc146a8a63d950a26c89165bc9": {
    "text": "If you provide an output function that takes a string, Pydantic AI will by default create an output tool like for any other output function. If instead you'd like the model to provide the string using plain text output, you can wrap the function in the [`TextOutput`](../api/output/#pydantic_ai.output.TextOutput \"../api/output/#pydantic_ai.output.TextOutput\") marker class. If desired, this marker class can be used alongside one or more [`ToolOutput`](#tool-output \"#tool-output\") marker classes (or unmarked types or functions) in a list provided to `output_type`.\n\ntext\\_output\\_function.py\n\n```\nfrom pydantic_ai import Agent, TextOutput\n\n\ndef split_into_words(text: str) -> list[str]:\n    return text.split()\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=TextOutput(split_into_words),\n)\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Text output"
  },
  "ab48877b882af43cfcf79ac1a33bd09a6f85044408ec2d2c28c5eea33a1f038a": {
    "text": "Pydantic AI implements three different methods to get a model to output structured data:\n\n1. [Tool Output](#tool-output \"#tool-output\"), where tool calls are used to produce the output.\n2. [Native Output](#native-output \"#native-output\"), where the model is required to produce text content compliant with a provided JSON schema.\n3. [Prompted Output](#prompted-output \"#prompted-output\"), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model's plain-text response as appropriate.\n\nIn the default Tool Output mode, the output JSON schema of each output type (or function) is provided to the model as the parameters schema of a special output tool. This is the default as it's supported by virtually all models and has been shown to work very well.\n\nIf you'd like to change the name of the output tool, pass a custom description to aid the model, or turn on or off strict mode, you can wrap the type(s) in the [`ToolOutput`](../api/output/#pydantic_ai.output.ToolOutput \"../api/output/#pydantic_ai.output.ToolOutput\") marker class and provide the appropriate arguments. Note that by default, the description is taken from the docstring specified on a Pydantic model or output function, so specifying it using the marker class is typically not necessary.\n\nTo dynamically modify or filter the available output tools during an agent run, you can define an agent-wide `prepare_output_tools` function that will be called ahead of each step of a run. This function should be of type [`ToolsPrepareFunc`](../api/tools/#pydantic_ai.tools.ToolsPrepareFunc \"../api/tools/#pydantic_ai.tools.ToolsPrepareFunc\"), which takes the [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and a list of [`ToolDefinition`](../api/tools/#pydantic_ai.tools.ToolDefinition \"../api/tools/#pydantic_ai.tools.ToolDefinition\"), and returns a new list of tool definitions (or `None` to disable all tools for that step). This is analogous to the [`prepare_tools` function](../tools/#prepare-tools \"../tools/#prepare-tools\") for non-output tools.\n\ntool\\_output.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ToolOutput\n\n\nclass Fruit(BaseModel):\n    name: str\n    color: str\n\n\nclass Vehicle(BaseModel):\n    name: str\n    wheels: int\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=[ # (1)!\n        ToolOutput(Fruit, name='return_fruit'),\n        ToolOutput(Vehicle, name='return_vehicle'),\n    ],\n)\nresult = agent.run_sync('What is a banana?')\nprint(repr(result.output))\n#> Fruit(name='banana', color='yellow')\n```\n\n1. If we were passing just `Fruit` and `Vehicle` without custom tool names, we could have used a union: `output_type=Fruit | Vehicle`. However, as `ToolOutput` is an object rather than a type, we have to use a list.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Output modes"
  },
  "5e20220c31e827f644c1fabb90e46fbc93d27c0257ca079a9edc7bd6c82fcb14": {
    "text": "Native Output mode uses a model's native \"Structured Outputs\" feature (aka \"JSON Schema response format\"), where the model is forced to only output text matching the provided JSON schema. Note that this is not supported by all models, and sometimes comes with restrictions. For example, Anthropic does not support this at all, and Gemini cannot use tools at the same time as structured output, and attempting to do so will result in an error.\n\nTo use this mode, you can wrap the output type(s) in the [`NativeOutput`](../api/output/#pydantic_ai.output.NativeOutput \"../api/output/#pydantic_ai.output.NativeOutput\") marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient.\n\nnative\\_output.py\n\n```\nfrom pydantic_ai import Agent, NativeOutput\n\nfrom tool_output import Fruit, Vehicle\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=NativeOutput(\n        [Fruit, Vehicle], # (1)!\n        name='Fruit_or_vehicle',\n        description='Return a fruit or vehicle.'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\n1. This could also have been a union: `output_type=Fruit | Vehicle`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Native Output"
  },
  "ce2d3f424c882169fa2bb11b8d6f64cec9b3a739a868ee2b2db5e4b579bbfc9c": {
    "text": "In this mode, the model is prompted to output text matching the provided JSON schema through its [instructions](../agents/#instructions \"../agents/#instructions\") and it's up to the model to interpret those instructions correctly. This is usable with all models, but is often the least reliable approach as the model is not forced to match the schema.\n\nWhile we would generally suggest starting with tool or native output, in some cases this mode may result in higher quality outputs, and for models without native tool calling or structured output support it is the only option for producing structured outputs.\n\nIf the model API supports the \"JSON Mode\" feature (aka \"JSON Object response format\") to force the model to output valid JSON, this is enabled, but it's still up to the model to abide by the schema. Pydantic AI will validate the returned structured data and tell the model to try again if validation fails, but if the model is not intelligent enough this may not be sufficient.\n\nTo use this mode, you can wrap the output type(s) in the [`PromptedOutput`](../api/output/#pydantic_ai.output.PromptedOutput \"../api/output/#pydantic_ai.output.PromptedOutput\") marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient. Additionally, it supports an `template` argument lets you specify a custom instructions template to be used instead of the [default](../api/profiles/#pydantic_ai.profiles.ModelProfile.prompted_output_template \"../api/profiles/#pydantic_ai.profiles.ModelProfile.prompted_output_template\").\n\nprompted\\_output.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, PromptedOutput\n\nfrom tool_output import Vehicle\n\n\nclass Device(BaseModel):\n    name: str\n    kind: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device], # (1)!\n        name='Vehicle or device',\n        description='Return a vehicle or device.'\n    ),\n)\nresult = agent.run_sync('What is a MacBook?')\nprint(repr(result.output))\n#> Device(name='MacBook', kind='laptop')\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        template='Gimme some JSON: {schema}'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\n1. This could also have been a union: `output_type=Vehicle | Device`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Prompted Output"
  },
  "6d60ddafdf48d62fba7670451792eae432636db48258aabbbd0fd078dd28e159": {
    "text": "If it's not feasible to define your desired structured output object using a Pydantic `BaseModel`, dataclass, or `TypedDict`, for example when you get a JSON schema from an external source or generate it dynamically, you can use the [`StructuredDict()`](../api/output/#pydantic_ai.output.StructuredDict \"../api/output/#pydantic_ai.output.StructuredDict\") helper function to generate a `dict[str, Any]` subclass with a JSON schema attached that Pydantic AI will pass to the model.\n\nNote that Pydantic AI will not perform any validation of the received JSON object and it's up to the model to correctly interpret the schema and any constraints expressed in it, like required fields or integer value ranges.\n\nThe output type will be a `dict[str, Any]` and it's up to your code to defensively read from it in case the model made a mistake. You can use an [output validator](#output-validator-functions \"#output-validator-functions\") to reflect validation errors back to the model and get it to try again.\n\nAlong with the JSON schema, you can optionally pass `name` and `description` arguments to provide additional context to the model:\n\n```\nfrom pydantic_ai import Agent, StructuredDict\n\nHumanDict = StructuredDict(\n    {\n        'type': 'object',\n        'properties': {\n            'name': {'type': 'string'},\n            'age': {'type': 'integer'}\n        },\n        'required': ['name', 'age']\n    },\n    name='Human',\n    description='A human with a name and age',\n)\n\nagent = Agent('openai:gpt-4o', output_type=HumanDict)\nresult = agent.run_sync('Create a person')\n#> {'name': 'John Doe', 'age': 30}\n```",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Custom JSON schema"
  },
  "361fc941d2f0b75249a99a76f4955e0f4a69846b6c819d00f3b7d8b09cf72e76": {
    "text": "Some validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. Pydantic AI provides a way to add validation functions via the [`agent.output_validator`](../api/agent/#pydantic_ai.agent.Agent.output_validator \"../api/agent/#pydantic_ai.agent.Agent.output_validator\") decorator.\n\nIf you want to implement separate validation logic for different output types, it's recommended to use [output functions](#output-functions \"#output-functions\") instead, to save you from having to do `isinstance` checks inside the output validator.\nIf you want the model to output plain text, do your own processing or validation, and then have the agent's final output be the result of your function, it's recommended to use an [output function](#output-functions \"#output-functions\") with the [`TextOutput` marker class](#text-output \"#text-output\").\n\nHere's a simplified variant of the [SQL Generation example](../examples/sql-gen/ \"../examples/sql-gen/\"):\n\nsql\\_gen.py\n\n```\nfrom fake_database import DatabaseConn, QueryError\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\n\nclass Success(BaseModel):\n    sql_query: str\n\n\nclass InvalidRequest(BaseModel):\n    error_message: str\n\n\nOutput = Success | InvalidRequest\nagent = Agent[DatabaseConn, Output](\n    'google-gla:gemini-1.5-flash',\n    output_type=Output,  # type: ignore\n    deps_type=DatabaseConn,\n    system_prompt='Generate PostgreSQL flavored SQL queries based on user input.',\n)\n\n\n@agent.output_validator\nasync def validate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\n    if isinstance(output, InvalidRequest):\n        return output\n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\nresult = agent.run_sync(\n    'get me users who were last active yesterday.', deps=DatabaseConn()\n)\nprint(result.output)\n#> sql_query='SELECT * FROM users WHERE last_active::date = today() - interval 1 day'\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Output validators"
  },
  "1c09bd9e66ae0d12d6b5ebbcf7c2bd4af617e33b0cdac37a71abf48021bcdfcd": {
    "text": "There two main challenges with streamed results:\n\n1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748 \"https://github.com/pydantic/pydantic/pull/10748\").\n2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. Pydantic AI streams just enough of the response to sniff out if it's a tool call or an output, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`](../api/result/#pydantic_ai.result.StreamedRunResult \"../api/result/#pydantic_ai.result.StreamedRunResult\").\n\nNote\n\nAs the `run_stream()` method will consider the first output matching the `output_type` to be the final output,\nit will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\n\nIf you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools,\nuse [`agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\") with an `event_stream_handler` ([docs](../agents/#streaming-all-events \"../agents/#streaming-all-events\")) or [`agent.iter()`](../api/agent/#pydantic_ai.agent.AbstractAgent.iter \"../api/agent/#pydantic_ai.agent.AbstractAgent.iter\") ([docs](../agents/#streaming-all-events-and-output \"../agents/#streaming-all-events-and-output\")) instead.",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Streamed Results"
  },
  "5da9a31c2a298f06e22c4f9023594945d34a01d7d4631f6f8f9db48a6acad5d4": {
    "text": "Example of streamed text output:\n\nstreamed\\_hello\\_world.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-1.5-flash')  # (1)!\n\n\nasync def main():\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:  # (2)!\n        async for message in result.stream_text():  # (3)!\n            print(message)\n            #> The first known\n            #> The first known use of \"hello,\n            #> The first known use of \"hello, world\" was in\n            #> The first known use of \"hello, world\" was in a 1974 textbook\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n```\n\n1. Streaming works with the standard [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\") class, and doesn't require any special setup, just a model that supports streaming (currently all models support streaming).\n2. The [`Agent.run_stream()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\") method is used to start a streamed run, this method returns a context manager so the connection can be closed when the stream completes.\n3. Each item yield by [`StreamedRunResult.stream_text()`](../api/result/#pydantic_ai.result.StreamedRunResult.stream_text \"../api/result/#pydantic_ai.result.StreamedRunResult.stream_text\") is the complete text response, extended as new data is received.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nWe can also stream text as deltas rather than the entire text in each item:\n\nstreamed\\_delta\\_hello\\_world.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-1.5-flash')\n\n\nasync def main():\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:\n        async for message in result.stream_text(delta=True):  # (1)!\n            print(message)\n            #> The first known\n            #> use of \"hello,\n            #> world\" was in\n            #> a 1974 textbook\n            #> about the C\n            #> programming language.\n```\n\n1. [`stream_text`](../api/result/#pydantic_ai.result.StreamedRunResult.stream_text \"../api/result/#pydantic_ai.result.StreamedRunResult.stream_text\") will error if the response is not text.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nOutput message not included in `messages`\n\nThe final output message will **NOT** be added to result messages if you use `.stream_text(delta=True)`,\nsee [Messages and chat history](../message-history/ \"../message-history/\") for more information.",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Streaming Text"
  },
  "dc15b67ef1b3fa28d960b49fda1a8d38bceaf0cf8f1535326f6ed69601bd91fb": {
    "text": "Here's an example of streaming a user profile as it's built:\n\nstreamed\\_user\\_profile.py\n\n```\nfrom datetime import date\n\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict):\n    name: str\n    dob: NotRequired[date]\n    bio: NotRequired[str]\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=UserProfile,\n    system_prompt='Extract a user profile from the input',\n)\n\n\nasync def main():\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for profile in result.stream_output():\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nIf you want fine-grained control of validation, you can use the following pattern to get the entire partial [`ModelResponse`](../api/messages/#pydantic_ai.messages.ModelResponse \"../api/messages/#pydantic_ai.messages.ModelResponse\"):\n\nstreamed\\_user\\_profile.py\n\n```\nfrom datetime import date\n\nfrom pydantic import ValidationError\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict, total=False):\n    name: str\n    dob: date\n    bio: str\n\n\nagent = Agent('openai:gpt-4o', output_type=UserProfile)\n\n\nasync def main():\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for message, last in result.stream_responses(debounce_by=0.01):  # (1)!\n            try:\n                profile = await result.validate_response_output(  # (2)!\n                    message,\n                    allow_partial=not last,\n                )\n            except ValidationError:\n                continue\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n```\n\n1. [`stream_responses`](../api/result/#pydantic_ai.result.StreamedRunResult.stream_responses \"../api/result/#pydantic_ai.result.StreamedRunResult.stream_responses\") streams the data as [`ModelResponse`](../api/messages/#pydantic_ai.messages.ModelResponse \"../api/messages/#pydantic_ai.messages.ModelResponse\") objects, thus iteration can't fail with a `ValidationError`.\n2. [`validate_response_output`](../api/result/#pydantic_ai.result.StreamedRunResult.validate_response_output \"../api/result/#pydantic_ai.result.StreamedRunResult.validate_response_output\") validates the data, `allow_partial=True` enables pydantic's [`experimental_allow_partial` flag on `TypeAdapter`](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json \"https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json\").\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Streaming Structured Output"
  },
  "c9645fe609d15862a8d8121efe1b5f8d43001f3196bbf1133d42e6a00caf324d": {
    "text": "The following examples demonstrate how to use streamed responses in Pydantic AI:",
    "source_url": "https://ai.pydantic.dev/output/",
    "header": "Examples"
  },
  "2ae2533d0a02c3dadd0779f1553b5013f4411c270c2e0faf1f739ae1e94da19e": {
    "text": "Pydantic AI provides access to messages exchanged during an agent run. These messages can be used both to continue a coherent conversation, and to understand how an agent performed.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Messages and chat history"
  },
  "5c7401eecf58c338f3627bb8991e8fa39bfe21841c9b9881f2dedbe0feefddf7": {
    "text": "After running an agent, you can access the messages exchanged during that run from the `result` object.\n\nBoth [`RunResult`](../api/agent/#pydantic_ai.agent.AgentRunResult \"../api/agent/#pydantic_ai.agent.AgentRunResult\")\n(returned by [`Agent.run`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\"), [`Agent.run_sync`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync\"))\nand [`StreamedRunResult`](../api/result/#pydantic_ai.result.StreamedRunResult \"../api/result/#pydantic_ai.result.StreamedRunResult\") (returned by [`Agent.run_stream`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\")) have the following methods:\n\nStreamedRunResult and complete messages\n\nOn [`StreamedRunResult`](../api/result/#pydantic_ai.result.StreamedRunResult \"../api/result/#pydantic_ai.result.StreamedRunResult\"), the messages returned from these methods will only include the final result message once the stream has finished.\n\nE.g. you've awaited one of the following coroutines:\n\n**Note:** The final result message will NOT be added to result messages if you use [`.stream_text(delta=True)`](../api/result/#pydantic_ai.result.StreamedRunResult.stream_text \"../api/result/#pydantic_ai.result.StreamedRunResult.stream_text\") since in this case the result content is never built as one string.\n\nExample of accessing methods on a [`RunResult`](../api/agent/#pydantic_ai.agent.AgentRunResult \"../api/agent/#pydantic_ai.agent.AgentRunResult\") :\n\nrun\\_result\\_messages.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult = agent.run_sync('Tell me a joke.')\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Accessing Messages from Results"
  },
  "74bbad691414fc911c4fe1ce4f23d20c3d4e7640a51bbba23f4a73878237a739": {
    "text": "print(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nExample of accessing methods on a [`StreamedRunResult`](../api/result/#pydantic_ai.result.StreamedRunResult \"../api/result/#pydantic_ai.result.StreamedRunResult\") :\n\nstreamed\\_run\\_result\\_messages.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\n\nasync def main():\n    async with agent.run_stream('Tell me a joke.') as result:\n        # incomplete messages before the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            )\n        ]\n        \"\"\"\n\n        async for text in result.stream_text():\n            print(text)\n            #> Did you hear\n            #> Did you hear about the toothpaste\n            #> Did you hear about the toothpaste scandal? They called\n            #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n        # complete messages once the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    TextPart(\n                        content='Did you hear about the toothpaste scandal? They called it Colgate.'\n                    )\n                ],\n                usage=RequestUsage(input_tokens=50, output_tokens=12),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "all messages from the run"
  },
  "e561d1ab6c690d8cab3c7f0a4b4c686c269a0dd58885e3b9aeca4519f3cdff29": {
    "text": "The primary use of message histories in Pydantic AI is to maintain context across multiple agent runs.\n\nTo use existing messages in a run, pass them to the `message_history` parameter of\n[`Agent.run`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\"), [`Agent.run_sync`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_sync\") or\n[`Agent.run_stream`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\").\n\nIf `message_history` is set and not empty, a new system prompt is not generated — we assume the existing message history includes a system prompt.\n\nReusing messages in a conversation\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync('Explain?', message_history=result1.new_messages())\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Using Messages as Input for Further Agent Runs"
  },
  "9565122399d0dbb69ae613e25f448119bb44e43447b211f2ae8e9e9d7c4d88c1": {
    "text": "While maintaining conversation state in memory is enough for many applications, often times you may want to store the messages history of an agent run on disk or in a database. This might be for evals, for sharing data between Python and JavaScript/TypeScript, or any number of other use cases.\n\nThe intended way to do this is using a `TypeAdapter`.\n\nWe export [`ModelMessagesTypeAdapter`](../api/messages/#pydantic_ai.messages.ModelMessagesTypeAdapter \"../api/messages/#pydantic_ai.messages.ModelMessagesTypeAdapter\") that can be used for this, or you can create your own.\n\nHere's an example showing how:\n\nserialize messages to json\n\n```\nfrom pydantic_core import to_jsonable_python\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessagesTypeAdapter  # (1)!\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nhistory_step_1 = result1.all_messages()\nas_python_objects = to_jsonable_python(history_step_1)  # (2)!\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_python(as_python_objects)\n\nresult2 = agent.run_sync(  # (3)!\n    'Tell me a different joke.', message_history=same_history_as_step_1\n)\n```\n\n1. Alternatively, you can create a `TypeAdapter` from scratch:\n\n   ```\n   from pydantic import TypeAdapter\n   from pydantic_ai.messages import ModelMessage\n   ModelMessagesTypeAdapter = TypeAdapter(list[ModelMessage])\n   ```\n2. Alternatively you can serialize to/from JSON directly:\n\n   ```\n   from pydantic_core import to_json\n   ...\n   as_json_objects = to_json(history_step_1)\n   same_history_as_step_1 = ModelMessagesTypeAdapter.validate_json(as_json_objects)\n   ```\n3. You can now continue the conversation with history `same_history_as_step_1` despite creating a new agent run.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Storing and loading messages (to JSON)"
  },
  "a111addadcd5106d3f2297e57683bec970289572021320c3478adf6499e62cbf": {
    "text": "Since messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing.\n\nThe message format is independent of the model used, so you can use messages in different agents, or the same agent with different models.\n\nIn the example below, we reuse the message from the first agent run, which uses the `openai:gpt-4o` model, in a second agent run using the `google-gla:gemini-1.5-pro` model.\n\nReusing messages with a different model\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync(\n    'Explain?',\n    model='google-gla:gemini-1.5-pro',\n    message_history=result1.new_messages(),\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-4o',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gemini-1.5-pro',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Other ways of using messages"
  },
  "befaa75f9048aa0cf432055d93259968b595fca25b7002baf6034aa811dfec99": {
    "text": "Sometimes you may want to modify the message history before it's sent to the model. This could be for privacy\nreasons (filtering out sensitive information), to save costs on tokens, to give less context to the LLM, or\ncustom processing logic.\n\nPydantic AI provides a `history_processors` parameter on `Agent` that allows you to intercept and modify\nthe message history before each model request.\n\nHistory processors replace the message history\n\nHistory processors replace the message history in the state with the processed messages, including the new user prompt part.\nThis means that if you want to keep the original message history, you need to make a copy of it.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Processing Message History"
  },
  "0996de5fa2bcb6790e239dcdfea5586f04470049bcad32230f9705917c24f09e": {
    "text": "The `history_processors` is a list of callables that take a list of\n[`ModelMessage`](../api/messages/#pydantic_ai.messages.ModelMessage \"../api/messages/#pydantic_ai.messages.ModelMessage\") and return a modified list of the same type.\n\nEach processor is applied in sequence, and processors can be either synchronous or asynchronous.\n\nsimple\\_history\\_processor.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Remove all ModelResponse messages, keeping only ModelRequest messages.\"\"\"\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Usage"
  },
  "043f36d58cc66eadf55c53022c0b874a5a536e51dade15feeec57e81f3aaca1c": {
    "text": "agent = Agent('openai:gpt-4o', history_processors=[filter_responses])",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Create agent with history processor"
  },
  "a2668af7cd9e935beeec9087abad175fabd0612e29ad8f194fe700dc9960bbfc": {
    "text": "message_history = [\n    ModelRequest(parts=[UserPromptPart(content='What is 2+2?')]),\n    ModelResponse(parts=[TextPart(content='2+2 equals 4')]),  # This will be filtered out\n]",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Example: Create some conversation history"
  },
  "bb2725545e78984c3b92b008201d7dc18e12fa72c238100c6c0496de8c45e0b0": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "result = agent.run_sync('What about 3+3?', message_history=message_history)"
  },
  "b44cd8dda3639956b61a96095921a6da8f126f9ef2f391ef450bb7547663c64d": {
    "text": "You can use the `history_processor` to only keep the recent messages:\n\nkeep\\_recent\\_messages.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage\n\n\nasync def keep_recent_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Keep only the last 5 messages to manage token usage.\"\"\"\n    return messages[-5:] if len(messages) > 5 else messages\n\nagent = Agent('openai:gpt-4o', history_processors=[keep_recent_messages])",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Keep Only Recent Messages"
  },
  "42f51f10bf727a1812f3b7370286d893499b44226d26f0e1aa66a582b32186d3": {
    "text": "long_conversation_history: list[ModelMessage] = []  # Your long conversation history here",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Example: Even with a long conversation history, only the last 5 messages are sent to the model"
  },
  "fcb9f3ca11e615c996927b284a881176fbc11dfbe73972a6003b81e246e80116": {
    "text": "```\n\nBe careful when slicing the message history\n\nWhen slicing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269 \"https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269\").",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "result = agent.run_sync('What did we discuss?', message_history=long_conversation_history)"
  },
  "9d43b317e767d3e1371340dedd2ab2ab7711a583c7023e9eaab403b10de4c8cc": {
    "text": "History processors can optionally accept a [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") parameter to access\nadditional information about the current run, such as dependencies, model information, and usage statistics:\n\ncontext\\_aware\\_processor.py\n\n```\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.messages import ModelMessage\n\n\ndef context_aware_processor(\n    ctx: RunContext[None],\n    messages: list[ModelMessage],\n) -> list[ModelMessage]:\n    # Access current usage\n    current_tokens = ctx.usage.total_tokens\n\n    # Filter messages based on context\n    if current_tokens > 1000:\n        return messages[-3:]  # Keep only recent messages when token usage is high\n    return messages\n\nagent = Agent('openai:gpt-4o', history_processors=[context_aware_processor])\n```\n\nThis allows for more sophisticated message processing based on the current state of the agent run.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "`RunContext` parameter"
  },
  "5990adc9a1c0a095382f53d2bcc9d96c54b2a61f121a74a9cd54ff19190104d6": {
    "text": "Use an LLM to summarize older messages to preserve context while reducing tokens.\n\nsummarize\\_old\\_messages.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Summarize Old Messages"
  },
  "cb299f7f77426041a72e1856ed46bc775bc6c58ed71a794d1cc30cd6190b4854": {
    "text": "summarize_agent = Agent(\n    'openai:gpt-4o-mini',\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics.\nFocus on the technical discussion and next steps.\n\"\"\",\n)\n\n\nasync def summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\n\nagent = Agent('openai:gpt-4o', history_processors=[summarize_old_messages])\n```\n\nBe careful when summarizing the message history\n\nWhen summarizing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269 \"https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269\"), where you can find examples of summarizing the message history.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Use a cheaper model to summarize old messages."
  },
  "ebf2c97646f49d4ebe0fa979ca7ed24b382ccb2783a70cf6f896adf9f2cf9ede": {
    "text": "You can test what messages are actually sent to the model provider using\n[`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\"):\n\ntest\\_history\\_processor.py\n\n```\nimport pytest\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\n\n@pytest.fixture\ndef received_messages() -> list[ModelMessage]:\n    return []\n\n\n@pytest.fixture\ndef function_model(received_messages: list[ModelMessage]) -> FunctionModel:\n    def capture_model_function(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n        # Capture the messages that the provider actually receives\n        received_messages.clear()\n        received_messages.extend(messages)\n        return ModelResponse(parts=[TextPart(content='Provider response')])\n\n    return FunctionModel(capture_model_function)\n\n\ndef test_history_processor(function_model: FunctionModel, received_messages: list[ModelMessage]):\n    def filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n        return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n    agent = Agent(function_model, history_processors=[filter_responses])\n\n    message_history = [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelResponse(parts=[TextPart(content='Answer 1')]),\n    ]\n\n    agent.run_sync('Question 2', message_history=message_history)\n    assert received_messages == [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelRequest(parts=[UserPromptPart(content='Question 2')]),\n    ]\n```",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Testing History Processors"
  },
  "050cc8308dec0f9b02386eb683b8eba6e6e0707f20321dad91ad00db4191bb6d": {
    "text": "You can also use multiple processors:\n\nmultiple\\_history\\_processors.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import ModelMessage, ModelRequest\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n\ndef summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return messages[-5:]\n\n\nagent = Agent('openai:gpt-4o', history_processors=[filter_responses, summarize_old_messages])\n```\n\nIn this case, the `filter_responses` processor will be applied first, and the\n`summarize_old_messages` processor will be applied second.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Multiple Processors"
  },
  "4c7e5185b975424ca9d95628ede7fc60657b1ce3459e968fea21088a4401cbdb": {
    "text": "For a more complete example of using messages in conversations, see the [chat app](../examples/chat-app/ \"../examples/chat-app/\") example.",
    "source_url": "https://ai.pydantic.dev/message-history/",
    "header": "Examples"
  },
  "bcef598370cbff78d4f987b9fc403afc52217b4ba70ce1a90bb703050a1cec29": {
    "text": "Writing unit tests for Pydantic AI code is just like unit tests for any other Python code.\n\nBecause for the most part they're nothing new, we have pretty well established tools and patterns for writing and running these kinds of tests.\n\nUnless you're really sure you know better, you'll probably want to follow roughly this strategy:\n\n* Use [`pytest`](https://docs.pytest.org/en/stable/ \"https://docs.pytest.org/en/stable/\") as your test harness\n* If you find yourself typing out long assertions, use [inline-snapshot](https://15r10nk.github.io/inline-snapshot/latest/ \"https://15r10nk.github.io/inline-snapshot/latest/\")\n* Similarly, [dirty-equals](https://dirty-equals.helpmanual.io/latest/ \"https://dirty-equals.helpmanual.io/latest/\") can be useful for comparing large data structures\n* Use [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") or [`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\") in place of your actual model to avoid the usage, latency and variability of real LLM calls\n* Use [`Agent.override`](../api/agent/#pydantic_ai.agent.Agent.override \"../api/agent/#pydantic_ai.agent.Agent.override\") to replace an agent's model, dependencies, or toolsets inside your application logic\n* Set [`ALLOW_MODEL_REQUESTS=False`](../api/models/base/#pydantic_ai.models.ALLOW_MODEL_REQUESTS \"../api/models/base/#pydantic_ai.models.ALLOW_MODEL_REQUESTS\") globally to block any requests from being made to non-test models accidentally",
    "source_url": "https://ai.pydantic.dev/testing/",
    "header": "Unit testing"
  },
  "bf408ed87c3e25f51bb4b327b8ff58bd733d302ffa5a3bb92a9b0bb5035fae13": {
    "text": "The simplest and fastest way to exercise most of your application code is using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\"), this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent.\n\n`TestModel` is not magic\n\nThe \"clever\" (but not too clever) part of `TestModel` is that it will attempt to generate valid structured data for [function tools](../tools/ \"../tools/\") and [output types](../output/#structured-output \"../output/#structured-output\") based on the schema of the registered tools.\n\nThere's no ML or AI in `TestModel`, it's just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool.\n\nThe resulting data won't look pretty or relevant, but it should pass Pydantic's validation in most cases.\nIf you want something more sophisticated, use [`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\") and write your own data generation logic.\n\nLet's write unit tests for the following application code:\n\nweather\\_app.py\n\n```\nimport asyncio\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nfrom fake_database import DatabaseConn  # (1)!\nfrom weather_service import WeatherService  # (2)!\n\nweather_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=WeatherService,\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\ndef weather_forecast(\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\n) -> str:\n    if forecast_date < date.today():  # (3)!\n        return ctx.deps.get_historic_weather(location, forecast_date)\n    else:\n        return ctx.deps.get_forecast(location, forecast_date)\n\n\nasync def run_weather_forecast(  # (4)!\n    user_prompts: list[tuple[str, int]], conn: DatabaseConn\n):\n    \"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\n    async with WeatherService() as weather_service:\n\n        async def run_forecast(prompt: str, user_id: int):\n            result = await weather_agent.run(prompt, deps=weather_service)\n            await conn.store_forecast(user_id, result.output)\n\n        # run all prompts in parallel\n        await asyncio.gather(\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\n        )\n```\n\n1. `DatabaseConn` is a class that holds a database connection\n2. `WeatherService` has methods to get weather forecasts and historic data about the weather\n3. We need to call a different endpoint depending on whether the date is in the past or the future, you'll see why this nuance is important below\n4. This function is the code we want to test, together with the agent it uses\n\nHere we have a function that takes a list of `(user_prompt, user_id)` tuples, gets a weather forecast for each prompt, and stores the result in the database.\n\n**We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.**\n\nHere's how we would write tests using [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\"):\n\ntest\\_weather\\_app.py\n\n```\nfrom datetime import timezone\nimport pytest\n\nfrom dirty_equals import IsNow, IsStr\n\nfrom pydantic_ai import models, capture_run_messages, RequestUsage\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai.messages import (\n    ModelResponse,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    ModelRequest,\n)\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio  # (1)!\nmodels.ALLOW_MODEL_REQUESTS = False  # (2)!\n\n\nasync def test_forecast():\n    conn = DatabaseConn()\n    user_id = 1\n    with capture_run_messages() as messages:\n        with weather_agent.override(model=TestModel()):  # (3)!\n            prompt = 'What will the weather be like in London on 2024-11-28?'\n            await run_weather_forecast([(prompt, user_id)], conn)  # (4)!\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == '{\"weather_forecast\":\"Sunny with a chance of rain\"}'  # (5)!\n\n    assert messages == [  # (6)!\n        ModelRequest(\n            parts=[\n                SystemPromptPart(\n                    content='Providing a weather forecast at the locations the user provides.',\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n                UserPromptPart(\n                    content='What will the weather be like in London on 2024-11-28?',\n                    timestamp=IsNow(tz=timezone.utc),  # (7)!\n                ),\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='weather_forecast',\n                    args={\n                        'location': 'a',\n                        'forecast_date': '2024-01-01',  # (8)!\n                    },\n                    tool_call_id=IsStr(),\n                )\n            ],\n            usage=RequestUsage(\n                input_tokens=71,\n                output_tokens=7,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n        ModelRequest(\n            parts=[\n                ToolReturnPart(\n                    tool_name='weather_forecast',\n                    content='Sunny with a chance of rain',\n                    tool_call_id=IsStr(),\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n            ],\n        ),\n        ModelResponse(\n            parts=[\n                TextPart(\n                    content='{\"weather_forecast\":\"Sunny with a chance of rain\"}',\n                )\n            ],\n            usage=RequestUsage(\n                input_tokens=77,\n                output_tokens=16,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n    ]\n```\n\n1. We're using [anyio](https://anyio.readthedocs.io/en/stable/ \"https://anyio.readthedocs.io/en/stable/\") to run async tests.\n2. This is a safety measure to make sure we don't accidentally make real requests to the LLM while testing, see [`ALLOW_MODEL_REQUESTS`](../api/models/base/#pydantic_ai.models.ALLOW_MODEL_REQUESTS \"../api/models/base/#pydantic_ai.models.ALLOW_MODEL_REQUESTS\") for more details.\n3. We're using [`Agent.override`](../api/agent/#pydantic_ai.agent.Agent.override \"../api/agent/#pydantic_ai.agent.Agent.override\") to replace the agent's model with [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\"), the nice thing about `override` is that we can replace the model inside agent without needing access to the agent `run*` methods call site.\n4. Now we call the function we want to test inside the `override` context manager.\n5. But default, `TestModel` will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add [`custom_output_text='Sunny'`](../api/models/test/#pydantic_ai.models.test.TestModel.custom_output_text \"../api/models/test/#pydantic_ai.models.test.TestModel.custom_output_text\") when defining `TestModel`.\n6. So far we don't actually know which tools were called and with which values, we can use [`capture_run_messages`](../api/agent/#pydantic_ai.agent.capture_run_messages \"../api/agent/#pydantic_ai.agent.capture_run_messages\") to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\n7. The [`IsNow`](https://dirty-equals.helpmanual.io/latest/types/datetime/#dirty_equals.IsNow \"https://dirty-equals.helpmanual.io/latest/types/datetime/#dirty_equals.IsNow\") helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\n8. `TestModel` isn't doing anything clever to extract values from the prompt, so these values are hardcoded.",
    "source_url": "https://ai.pydantic.dev/testing/",
    "header": "Unit testing with `TestModel`"
  },
  "046841f9d37ad7b8e5f925790e1ef779516d068036963ceb76b163fe2c1a55de": {
    "text": "The above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\n\nTo fully exercise `weather_forecast`, we need to use [`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\") to customise how the tools is called.\n\nHere's an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\n\ntest\\_weather\\_app2.py\n\n```\nimport re\n\nimport pytest\n\nfrom pydantic_ai import models\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\n\ndef call_weather_forecast(  # (1)!\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  # (2)!\n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)!\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n```\n\n1. We define a function `call_weather_forecast` that will be called by `FunctionModel` in place of the LLM, this function has access to the list of [`ModelMessage`](../api/messages/#pydantic_ai.messages.ModelMessage \"../api/messages/#pydantic_ai.messages.ModelMessage\")s that make up the run, and [`AgentInfo`](../api/models/function/#pydantic_ai.models.function.AgentInfo \"../api/models/function/#pydantic_ai.models.function.AgentInfo\") which contains information about the agent and the function tools and return tools.\n2. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\n3. We use [`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\") to replace the agent's model with our custom function.",
    "source_url": "https://ai.pydantic.dev/testing/",
    "header": "Unit testing with `FunctionModel`"
  },
  "c013166632ed758bc0e6a737039b91f4182a072518bcd889665dd3bbda92e6f0": {
    "text": "If you're writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html \"https://docs.pytest.org/en/6.2.x/fixture.html\") to override the model with [`TestModel`](../api/models/test/#pydantic_ai.models.test.TestModel \"../api/models/test/#pydantic_ai.models.test.TestModel\") or [`FunctionModel`](../api/models/function/#pydantic_ai.models.function.FunctionModel \"../api/models/function/#pydantic_ai.models.function.FunctionModel\") in a reusable way.\n\nHere's an example of a fixture that overrides the model with `TestModel`:\n\ntest\\_agent.py\n\n```\nimport pytest\n\nfrom pydantic_ai.models.test import TestModel\n\nfrom weather_app import weather_agent\n\n\n@pytest.fixture\ndef override_weather_agent():\n    with weather_agent.override(model=TestModel()):\n        yield\n\n\nasync def test_forecast(override_weather_agent: None):\n    ...\n    # test code here\n```",
    "source_url": "https://ai.pydantic.dev/testing/",
    "header": "Overriding model via pytest fixtures"
  },
  "0b4f42901eae0ff4dbeeeb4db448f4e2407d96a2b5560ec7894254052d42f113": {
    "text": "Applications that use LLMs have some challenges that are well known and understood: LLMs are **slow**, **unreliable** and **expensive**.\n\nThese applications also have some challenges that most developers have encountered much less often: LLMs are **fickle** and **non-deterministic**. Subtle changes in a prompt can completely change a model's performance, and there's no `EXPLAIN` query you can run to understand why.\n\nWarning\n\nFrom a software engineers point of view, you can think of LLMs as the worst database you've ever heard of, but worse.\n\nIf LLMs weren't so bloody useful, we'd never touch them.\n\nTo build successful applications with LLMs, we need new tools to understand both model performance, and the behavior of applications that rely on them.\n\nLLM Observability tools that just let you understand how your model is performing are useless: making API calls to an LLM is easy, it's building that into an application that's hard.",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Pydantic Logfire Debugging and Monitoring"
  },
  "a63624a5a56b40d6c8515c6d0cb7119d97c30e70a5348f864b5131e3ac17c402": {
    "text": "[Pydantic Logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\") is an observability platform developed by the team who created and maintain Pydantic Validation and Pydantic AI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs, all using OpenTelemetry.\n\nPydantic Logfire is a commercial product\n\nLogfire is a commercially supported, hosted platform with an extremely generous and perpetual [free tier](https://pydantic.dev/pricing/ \"https://pydantic.dev/pricing/\").\nYou can sign up and start using Logfire in a couple of minutes. Logfire can also be self-hosted on the enterprise tier.\n\nPydantic AI has built-in (but optional) support for Logfire. That means if the `logfire` package is installed and configured and agent instrumentation is enabled then detailed information about agent runs is sent to Logfire. Otherwise there's virtually no overhead and nothing is sent.\n\nHere's an example showing details of running the [Weather Agent](../examples/weather-agent/ \"../examples/weather-agent/\") in Logfire:\n\nA trace is generated for the agent run, and spans are emitted for each model request and tool call.",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Pydantic Logfire"
  },
  "cfbf218b04ac7abe525d767d9b962ce371b94ebd1ca711d7d74151a84f11c5cc": {
    "text": "To use Logfire, you'll need a Logfire [account](https://logfire.pydantic.dev \"https://logfire.pydantic.dev\"). The Logfire Python SDK is included with `pydantic-ai`:\n\nOr if you're using the slim package, you can install it with the `logfire` optional group:\n\nThen authenticate your local environment with Logfire:\n\nAnd configure a project to send data to:\n\n(Or use an existing project with `logfire projects use`)\n\nThis will write to a `.logfire` directory in the current working directory, which the Logfire SDK will use for configuration at run time.\n\nWith that, you can start using Logfire to instrument Pydantic AI code:\n\ninstrument\\_pydantic\\_ai.py\n\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()  # (1)!\nlogfire.instrument_pydantic_ai()  # (2)!\n\nagent = Agent('openai:gpt-4o', instructions='Be concise, reply with one sentence.')\nresult = agent.run_sync('Where does \"hello world\" come from?')  # (3)!\nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n```\n\n1. [`logfire.configure()`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure \"https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure\") configures the SDK, by default it will find the write token from the `.logfire` directory, but you can also pass a token directly.\n2. [`logfire.instrument_pydantic_ai()`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai \"https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai\") enables instrumentation of Pydantic AI.\n3. Since we've enabled instrumentation, a trace will be generated for each run, with spans emitted for models calls and tool function execution\n\n*(This example is complete, it can be run \"as is\")*\n\nWhich will display in Logfire thus:\n\nThe [Logfire documentation](https://logfire.pydantic.dev/docs/ \"https://logfire.pydantic.dev/docs/\") has more details on how to use Logfire,\nincluding how to instrument other libraries like [HTTPX](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/ \"https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/\") and [FastAPI](https://logfire.pydantic.dev/docs/integrations/web-frameworks/fastapi/ \"https://logfire.pydantic.dev/docs/integrations/web-frameworks/fastapi/\").\n\nSince Logfire is built on [OpenTelemetry](https://opentelemetry.io/ \"https://opentelemetry.io/\"), you can use the Logfire Python SDK to send data to any OpenTelemetry collector, see [below](#using-opentelemetry \"#using-opentelemetry\").",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Using Logfire"
  },
  "df30149893dafb7aa2c9d14174773fb0f3fe42e981959281c2fad373ba75226c": {
    "text": "To demonstrate how Logfire can let you visualise the flow of a Pydantic AI run, here's the view you get from Logfire while running the [chat app examples](../examples/chat-app/ \"../examples/chat-app/\"):",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Debugging"
  },
  "e043072f66ef91a67def98d712d09b06fe9c8abcd6f5fc55f5fad12b3abe4c50": {
    "text": "We can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor Pydantic AI runs inside Logfire itself:",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Monitoring Performance"
  },
  "94967b1e8a64fd3a4f6a0a791fde66de37ea16ed1508dc6b39e5df959c792877": {
    "text": "\\\"F\\*\\*k you, show me the prompt.\\\"\n\nAs per Hamel Husain's influential 2024 blog post [\"Fuck You, Show Me The Prompt.\"](https://hamel.dev/blog/posts/prompt/ \"https://hamel.dev/blog/posts/prompt/\")\n(bear with the capitalization, the point is valid), it's often useful to be able to view the raw HTTP requests and responses made to model providers.\n\nTo observe raw HTTP requests made to model providers, you can use Logfire's [HTTPX instrumentation](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/ \"https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/\") since all provider SDKs use the [HTTPX](https://www.python-httpx.org/ \"https://www.python-httpx.org/\") library internally.",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Monitoring HTTP Requests"
  },
  "0d942df758e1908d9fde2d4611de89e68f31627bf424f7f16f86191d8cf583ee": {
    "text": "Pydantic AI's instrumentation uses [OpenTelemetry](https://opentelemetry.io/ \"https://opentelemetry.io/\") (OTel), which Logfire is based on.\n\nThis means you can debug and monitor Pydantic AI with any OpenTelemetry backend.\n\nPydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/ \"https://opentelemetry.io/docs/specs/semconv/gen-ai/\"), so while we think you'll have the best experience using the Logfire platform , you should be able to use any OTel service with GenAI support.",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Using OpenTelemetry"
  },
  "7446c198fb3ec72de36a4bdd353f058d51319a1e3e9dc95d5574027ad9db544d": {
    "text": "You can use the Logfire SDK completely freely and send the data to any OpenTelemetry backend.\n\nHere's an example of configuring the Logfire library to send data to the excellent [otel-tui](https://github.com/ymtdzzz/otel-tui \"https://github.com/ymtdzzz/otel-tui\") — an open source terminal based OTel backend and viewer (no association with Pydantic Validation).\n\nRun `otel-tui` with docker (see [the otel-tui readme](https://github.com/ymtdzzz/otel-tui \"https://github.com/ymtdzzz/otel-tui\") for more instructions):\n\nTerminal\n\n```\ndocker run --rm -it -p 4318:4318 --name otel-tui ymtdzzz/otel-tui:latest\n```\n\nthen run,\n\notel\\_tui.py\n\n```\nimport os\n\nimport logfire\n\nfrom pydantic_ai import Agent\n\nos.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'  # (1)!\nlogfire.configure(send_to_logfire=False)  # (2)!\nlogfire.instrument_pydantic_ai()\nlogfire.instrument_httpx(capture_all=True)\n\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> Paris\n```\n\n1. Set the `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable to the URL of your OpenTelemetry backend. If you're using a backend that requires authentication, you may need to set [other environment variables](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/ \"https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/\"). Of course, these can also be set outside the process, e.g. with `export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318`.\n2. We [configure](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure \"https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure\") Logfire to disable sending data to the Logfire OTel backend itself. If you removed `send_to_logfire=False`, data would be sent to both Logfire and your OpenTelemetry backend.\n\nRunning the above code will send tracing data to `otel-tui`, which will display like this:\n\nRunning the [weather agent](../examples/weather-agent/ \"../examples/weather-agent/\") example connected to `otel-tui` shows how it can be used to visualise a more complex trace:\n\nFor more information on using the Logfire SDK to send data to alternative backends, see\n[the Logfire documentation](https://logfire.pydantic.dev/docs/how-to-guides/alternative-backends/ \"https://logfire.pydantic.dev/docs/how-to-guides/alternative-backends/\").",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Logfire with an alternative OTel backend"
  },
  "c7c85319ffd7a6c9f80cfad27f9879d3ad747a422fcc0d982c2aa7bf52f2d69d": {
    "text": "You can also emit OpenTelemetry data from Pydantic AI without using Logfire at all.\n\nTo do this, you'll need to install and configure the OpenTelemetry packages you need. To run the following examples, use\n\nTerminal\n\n```\nuv run \\\n  --with 'pydantic-ai-slim[openai]' \\\n  --with opentelemetry-sdk \\\n  --with opentelemetry-exporter-otlp \\\n  raw_otel.py\n```\n\nraw\\_otel.py\n\n```\nimport os\n\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.trace import set_tracer_provider\n\nfrom pydantic_ai import Agent\n\nos.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'\nexporter = OTLPSpanExporter()\nspan_processor = BatchSpanProcessor(exporter)\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(span_processor)\n\nset_tracer_provider(tracer_provider)\n\nAgent.instrument_all()\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> Paris\n```",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "OTel without Logfire"
  },
  "b243c1e6ae204c624838c48d87f312195062a681a5e4740b79593b2565de8568": {
    "text": "Because Pydantic AI uses OpenTelemetry for observability, you can easily configure it to send data to any OpenTelemetry-compatible backend, not just our observability platform [Pydantic Logfire](#pydantic-logfire \"#pydantic-logfire\").\n\nThe following providers have dedicated documentation on Pydantic AI:",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Alternative Observability backends"
  },
  "8380cb276f50f10865a0b704009fac2fcb36bd4da038ace87a51b8532e4317bf": {
    "text": "Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/ \"https://opentelemetry.io/docs/specs/semconv/gen-ai/\"). Specifically, it follows version 1.37.0 of the conventions by default. To use [version 1.36.0](https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/README.md \"https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/README.md\") or older, pass [`InstrumentationSettings(version=1)`](../api/models/instrumented/#pydantic_ai.models.instrumented.InstrumentationSettings \"../api/models/instrumented/#pydantic_ai.models.instrumented.InstrumentationSettings\") (the default is `version=2`). Moreover, those semantic conventions specify that messages should be captured as individual events (logs) that are children of the request span, whereas by default, Pydantic AI instead collects these events into a JSON array which is set as a single large attribute called `events` on the request span. To change this, use `event_mode='logs'`:\n\ninstrumentation\\_settings\\_event\\_mode.py\n\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai(version=1, event_mode='logs')\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nThis won't look as good in the Logfire UI, and will also be removed from Pydantic AI in a future release, but may be useful for backwards compatibility.\n\nNote that the OpenTelemetry Semantic Conventions are still experimental and are likely to change.",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Configuring data format"
  },
  "214462c9021b4ca9dd7097515eb93e788c3599c2f4ed8af7dc55c577c9e81170": {
    "text": "By default, the global `TracerProvider` and `EventLoggerProvider` are used. These are set automatically by `logfire.configure()`. They can also be set by the `set_tracer_provider` and `set_event_logger_provider` functions in the OpenTelemetry Python SDK. You can set custom providers with [`InstrumentationSettings`](../api/models/instrumented/#pydantic_ai.models.instrumented.InstrumentationSettings \"../api/models/instrumented/#pydantic_ai.models.instrumented.InstrumentationSettings\").\n\ninstrumentation\\_settings\\_providers.py\n\n```\nfrom opentelemetry.sdk._events import EventLoggerProvider\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom pydantic_ai import Agent, InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(\n    tracer_provider=TracerProvider(),\n    event_logger_provider=EventLoggerProvider(),\n)\n\nagent = Agent('openai:gpt-4o', instrument=instrumentation_settings)",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Setting OpenTelemetry SDK providers"
  },
  "7a32199fba8f470fb386711c194a14267440f5ef84ca711a3c735da2cc30a049": {
    "text": "Agent.instrument_all(instrumentation_settings)\n```",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "or to instrument all agents:"
  },
  "4d1afa2519d8c283868ee12f02dff2ab81b248ef1a0f439ea6289e9015ea47a8": {
    "text": "instrumented\\_model\\_example.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.instrumented import InstrumentationSettings, InstrumentedModel\n\nsettings = InstrumentationSettings()\nmodel = InstrumentedModel('openai:gpt-4o', settings)\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Instrumenting a specific `Model`"
  },
  "6b5d9a119d17852a53d5be0852426fea52580ab88cfa48a5ff15d506cc45f7a1": {
    "text": "excluding\\_binary\\_content.py\n\n```\nfrom pydantic_ai import Agent, InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(include_binary_content=False)\n\nagent = Agent('openai:gpt-4o', instrument=instrumentation_settings)",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Excluding binary content"
  },
  "96b1854df9c53c0033429e967b6a1270b8e2d1b988dc6c735eb85e41a5e6b407": {
    "text": "Agent.instrument_all(instrumentation_settings)\n```",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "or to instrument all agents:"
  },
  "93a05e127d53889d102c95a2fae6cbd7976b37e9428f33e1a56ce582eba3f4bc": {
    "text": "For privacy and security reasons, you may want to monitor your agent's behavior and performance without exposing sensitive user data or proprietary prompts in your observability platform. Pydantic AI allows you to exclude the actual content from instrumentation events while preserving the structural information needed for debugging and monitoring.\n\nWhen `include_content=False` is set, Pydantic AI will exclude sensitive content from OpenTelemetry events, including user prompts and model completions, tool call arguments and responses, and any other message content.\n\nexcluding\\_sensitive\\_content.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.instrumented import InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(include_content=False)\n\nagent = Agent('openai:gpt-4o', instrument=instrumentation_settings)",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "Excluding prompts and completions"
  },
  "9f200783af9fee32f57f6793bc223b282f0591a917b607475435a78903a415b3": {
    "text": "Agent.instrument_all(instrumentation_settings)\n```\n\nThis setting is particularly useful in production environments where compliance requirements or data sensitivity concerns make it necessary to limit what content is sent to your observability platform.",
    "source_url": "https://ai.pydantic.dev/logfire/",
    "header": "or to instrument all agents:"
  },
  "85bb6e1a084138d971f0939c81f732e3dbbbf7145465e9639cf407a47610c5c1": {
    "text": "There are roughly four levels of complexity when building applications with Pydantic AI:\n\n1. Single agent workflows — what most of the `pydantic_ai` documentation covers\n2. [Agent delegation](#agent-delegation \"#agent-delegation\") — agents using another agent via tools\n3. [Programmatic agent hand-off](#programmatic-agent-hand-off \"#programmatic-agent-hand-off\") — one agent runs, then application code calls another agent\n4. [Graph based control flow](../graph/ \"../graph/\") — for the most complex cases, a graph-based state machine can be used to control the execution of multiple agents\n\nOf course, you can combine multiple strategies in a single application.",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "Multi-agent Applications"
  },
  "602d30dc3e8f562e14014199066005b04674e18172053888dd05713290956719": {
    "text": "\"Agent delegation\" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes.\nIf you want to hand off control to another agent completely, without coming back to the first agent, you can use an [output function](../output/#output-functions \"../output/#output-functions\").\n\nSince agents are stateless and designed to be global, you do not need to include the agent itself in agent [dependencies](../dependencies/ \"../dependencies/\").\n\nYou'll generally want to pass [`ctx.usage`](../api/tools/#pydantic_ai.tools.RunContext.usage \"../api/tools/#pydantic_ai.tools.RunContext.usage\") to the [`usage`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\") keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run.\n\nMultiple models\n\nAgent delegation doesn't need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final [`result.usage()`](../api/agent/#pydantic_ai.agent.AgentRunResult.usage \"../api/agent/#pydantic_ai.agent.AgentRunResult.usage\") of the run will not be possible, but you can still use [`UsageLimits`](../api/usage/#pydantic_ai.usage.UsageLimits \"../api/usage/#pydantic_ai.usage.UsageLimits\") to avoid unexpected costs.\n\nagent\\_delegation\\_simple.py\n\n```\nfrom pydantic_ai import Agent, RunContext, UsageLimits\n\njoke_selection_agent = Agent(  # (1)!\n    'openai:gpt-4o',\n    system_prompt=(\n        'Use the `joke_factory` to generate some jokes, then choose the best. '\n        'You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(  # (2)!\n    'google-gla:gemini-1.5-flash', output_type=list[str]\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\n    r = await joke_generation_agent.run(  # (3)!\n        f'Please generate {count} jokes.',\n        usage=ctx.usage,  # (4)!\n    )\n    return r.output  # (5)!\n\n\nresult = joke_selection_agent.run_sync(\n    'Tell me a joke.',\n    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=500),\n)\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\nprint(result.usage())\n#> RunUsage(input_tokens=204, output_tokens=24, requests=3)\n```\n\n1. The \"parent\" or controlling agent.\n2. The \"delegate\" agent, which is called from within a tool of the parent agent.\n3. Call the delegate agent from within a tool of the parent agent.\n4. Pass the usage from the parent agent to the delegate agent so the final [`result.usage()`](../api/agent/#pydantic_ai.agent.AgentRunResult.usage \"../api/agent/#pydantic_ai.agent.AgentRunResult.usage\") includes the usage from both agents.\n5. Since the function returns `list[str]`, and the `output_type` of `joke_generation_agent` is also `list[str]`, we can simply return `r.output` from the tool.\n\n*(This example is complete, it can be run \"as is\")*\n\nThe control flow for this example is pretty simple and can be summarised as follows:\n\n```\ngraph TD\n  START --> joke_selection_agent\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\n  joke_factory --> joke_generation_agent\n  joke_generation_agent --> joke_factory\n  joke_factory --> joke_selection_agent\n  joke_selection_agent --> END\n```",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "Agent delegation"
  },
  "7931601f47465a3e60f1143aae7244e026c0bc1950fda91725d0853b2cadb13b": {
    "text": "Generally the delegate agent needs to either have the same [dependencies](../dependencies/ \"../dependencies/\") as the calling agent, or dependencies which are a subset of the calling agent's dependencies.\n\nInitializing dependencies\n\nWe say \"generally\" above since there's nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent.\n\nagent\\_delegation\\_deps.py\n\n```\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass ClientAndKey:  # (1)!\n    http_client: httpx.AsyncClient\n    api_key: str\n\n\njoke_selection_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=ClientAndKey,  # (2)!\n    system_prompt=(\n        'Use the `joke_factory` tool to generate some jokes on the given subject, '\n        'then choose the best. You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(\n    'google-gla:gemini-1.5-flash',\n    deps_type=ClientAndKey,  # (4)!\n    output_type=list[str],\n    system_prompt=(\n        'Use the \"get_jokes\" tool to get some jokes on the given subject, '\n        'then extract each joke into a list.'\n    ),\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:\n    r = await joke_generation_agent.run(\n        f'Please generate {count} jokes.',\n        deps=ctx.deps,  # (3)!\n        usage=ctx.usage,\n    )\n    return r.output\n\n\n@joke_generation_agent.tool  # (5)!\nasync def get_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:\n    response = await ctx.deps.http_client.get(\n        'https://example.com',\n        params={'count': count},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = ClientAndKey(client, 'foobar')\n        result = await joke_selection_agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n        print(result.usage())  # (6)!\n        #> RunUsage(input_tokens=309, output_tokens=32, requests=4)\n```\n\n1. Define a dataclass to hold the client and API key dependencies.\n2. Set the `deps_type` of the calling agent — `joke_selection_agent` here.\n3. Pass the dependencies to the delegate agent's run method within the tool call.\n4. Also set the `deps_type` of the delegate agent — `joke_generation_agent` here.\n5. Define a tool on the delegate agent that uses the dependencies to make an HTTP request.\n6. Usage now includes 4 requests — 2 from the calling agent and 2 from the delegate agent.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nThis example shows how even a fairly simple agent delegation can lead to a complex control flow:\n\n```\ngraph TD\n  START --> joke_selection_agent\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\n  joke_factory --> joke_generation_agent\n  joke_generation_agent --> get_jokes[\"get_jokes (tool)\"]\n  get_jokes --> http_request[\"HTTP request\"]\n  http_request --> get_jokes\n  get_jokes --> joke_generation_agent\n  joke_generation_agent --> joke_factory\n  joke_factory --> joke_selection_agent\n  joke_selection_agent --> END\n```",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "Agent delegation and dependencies"
  },
  "00ae43655c1e15641cb4b9d960edbc8333895ee8d61dab7669d30514685bb30a": {
    "text": "\"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next.\n\nHere agents don't need to use the same deps.\n\nHere we show two agents used in succession, the first to find a flight and the second to extract the user's seat preference.\n\nprogrammatic\\_handoff.py\n\n```\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\nfrom rich.prompt import Prompt\n\nfrom pydantic_ai import Agent, RunContext, RunUsage, UsageLimits\nfrom pydantic_ai.messages import ModelMessage\n\n\nclass FlightDetails(BaseModel):\n    flight_number: str\n\n\nclass Failed(BaseModel):\n    \"\"\"Unable to find a satisfactory choice.\"\"\"\n\n\nflight_search_agent = Agent[None, FlightDetails | Failed](  # (1)!\n    'openai:gpt-4o',\n    output_type=FlightDetails | Failed,  # type: ignore\n    system_prompt=(\n        'Use the \"flight_search\" tool to find a flight '\n        'from the given origin to the given destination.'\n    ),\n)\n\n\n@flight_search_agent.tool  # (2)!\nasync def flight_search(\n    ctx: RunContext[None], origin: str, destination: str\n) -> FlightDetails | None:\n    # in reality, this would call a flight search API or\n    # use a browser to scrape a flight search website\n    return FlightDetails(flight_number='AK456')\n\n\nusage_limits = UsageLimits(request_limit=15)  # (3)!\n\n\nasync def find_flight(usage: RunUsage) -> FlightDetails | None:  # (4)!\n    message_history: list[ModelMessage] | None = None\n    for _ in range(3):\n        prompt = Prompt.ask(\n            'Where would you like to fly from and to?',\n        )\n        result = await flight_search_agent.run(\n            prompt,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, FlightDetails):\n            return result.output\n        else:\n            message_history = result.all_messages(\n                output_tool_return_content='Please try again.'\n            )\n\n\nclass SeatPreference(BaseModel):\n    row: int = Field(ge=1, le=30)\n    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "Programmatic agent hand-off"
  },
  "65c4d4ee3dff9f8fe2d7e99f811520ea19cfb93a774e9b87aca39a922d14b159": {
    "text": "seat_preference_agent = Agent[None, SeatPreference | Failed](  # (5)!\n    'openai:gpt-4o',\n    output_type=SeatPreference | Failed,  # type: ignore\n    system_prompt=(\n        \"Extract the user's seat preference. \"\n        'Seats A and F are window seats. '\n        'Row 1 is the front row and has extra leg room. '\n        'Rows 14, and 20 also have extra leg room. '\n    ),\n)\n\n\nasync def find_seat(usage: RunUsage) -> SeatPreference:  # (6)!\n    message_history: list[ModelMessage] | None = None\n    while True:\n        answer = Prompt.ask('What seat would you like?')\n\n        result = await seat_preference_agent.run(\n            answer,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, SeatPreference):\n            return result.output\n        else:\n            print('Could not understand seat preference. Please try again.')\n            message_history = result.all_messages()\n\n\nasync def main():  # (7)!\n    usage: RunUsage = RunUsage()\n\n    opt_flight_details = await find_flight(usage)\n    if opt_flight_details is not None:\n        print(f'Flight found: {opt_flight_details.flight_number}')\n        #> Flight found: AK456\n        seat_preference = await find_seat(usage)\n        print(f'Seat preference: {seat_preference}')\n        #> Seat preference: row=1 seat='A'\n```\n\n1. Define the first agent, which finds a flight. We use an explicit type annotation until [PEP-747](https://peps.python.org/pep-0747/ \"https://peps.python.org/pep-0747/\") lands, see [structured output](../output/#structured-output \"../output/#structured-output\"). We use a union as the output type so the model can communicate if it's unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool.\n2. Define a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary.\n3. Define usage limits for the entire app.\n4. Define a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight.\n5. As with `flight_search_agent` above, we use an explicit type annotation to define the agent.\n6. Define a function to find the user's seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference.\n7. Now that we've put our logic for running each agent into separate functions, our main app becomes very simple.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nThe control flow for this example can be summarised as follows:\n\n```\ngraph TB\n  START --> ask_user_flight[\"ask user for flight\"]\n\n  subgraph find_flight\n    flight_search_agent --> ask_user_flight\n    ask_user_flight --> flight_search_agent\n  end\n\n  flight_search_agent --> ask_user_seat[\"ask user for seat\"]\n  flight_search_agent --> END\n\n  subgraph find_seat\n    seat_preference_agent --> ask_user_seat\n    ask_user_seat --> seat_preference_agent\n  end\n\n  seat_preference_agent --> END\n```",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "This agent is responsible for extracting the user's seat selection"
  },
  "3f30fcadc5d68e685af8fd0317bed34e047148a0c95740553a62b5442f6e9ed1": {
    "text": "See the [graph](../graph/ \"../graph/\") documentation on when and how to use graphs.",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "Pydantic Graphs"
  },
  "35d83f264daff7e563968ab9c57e7006f6a390cf3f8f9a8234ae60ec99c86f26": {
    "text": "The following examples demonstrate how to use dependencies in Pydantic AI:",
    "source_url": "https://ai.pydantic.dev/multi-agent-applications/",
    "header": "Examples"
  },
  "b7b611143670655953d12a9512c7ff5c0f305bb9fa99643947cfec55c0a79397": {
    "text": "Don't use a nail gun unless you need a nail gun\n\nIf Pydantic AI [agents](../agents/ \"../agents/\") are a hammer, and [multi-agent workflows](../multi-agent-applications/ \"../multi-agent-applications/\") are a sledgehammer, then graphs are a nail gun:\n\n* sure, nail guns look cooler than hammers\n* but nail guns take a lot more setup than hammers\n* and nail guns don't make you a better builder, they make you a builder with a nail gun\n* Lastly, (and at the risk of torturing this metaphor), if you're a fan of medieval tools like mallets and untyped Python, you probably won't like nail guns or our approach to graphs. (But then again, if you're not a fan of type hints in Python, you've probably already bounced off Pydantic AI to use one of the toy agent frameworks — good luck, and feel free to borrow my sledgehammer when you realize you need it)\n\nIn short, graphs are a powerful tool, but they're not the right tool for every job. Please consider other [multi-agent approaches](../multi-agent-applications/ \"../multi-agent-applications/\") before proceeding.\n\nIf you're not confident a graph-based approach is a good idea, it might be unnecessary.\n\nGraphs and finite state machines (FSMs) are a powerful abstraction to model, execute, control and visualize complex workflows.\n\nAlongside Pydantic AI, we've developed `pydantic-graph` — an async graph and state machine library for Python where nodes and edges are defined using type hints.\n\nWhile this library is developed as part of Pydantic AI; it has no dependency on `pydantic-ai` and can be considered as a pure graph-based state machine library. You may find it useful whether or not you're using Pydantic AI or even building with GenAI.\n\n`pydantic-graph` is designed for advanced users and makes heavy use of Python generics and type hints. It is not designed to be as beginner-friendly as Pydantic AI.",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Graphs"
  },
  "84c5477ed465c5fcf47747cdea0ea2da0e3ab64af7e4fe0596b2a1f6e2a3b267": {
    "text": "`pydantic-graph` is a required dependency of `pydantic-ai`, and an optional dependency of `pydantic-ai-slim`, see [installation instructions](../install/#slim-install \"../install/#slim-install\") for more information. You can also install it directly:",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Installation"
  },
  "075530e3957e7f2754adfdb5e254fc016e77c71d84ebc46409ae06ea32061c2e": {
    "text": "`pydantic-graph` is made up of a few key components:",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Graph Types"
  },
  "742e89759d0dad716e1dd1887ffef28d8a357fd8540cdbe8d5c5d399b324d227": {
    "text": "[`GraphRunContext`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.GraphRunContext \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.GraphRunContext\") — The context for the graph run, similar to Pydantic AI's [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\"). This holds the state of the graph and dependencies and is passed to nodes when they're run.\n\n`GraphRunContext` is generic in the state type of the graph it's used in, [`StateT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT\").",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "GraphRunContext"
  },
  "728d5bbe56a490300b081cb98e69b58293cc07b331fa072abdf095e1c92743a9": {
    "text": "[`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\") — return value to indicate the graph run should end.\n\n`End` is generic in the graph return type of the graph it's used in, [`RunEndT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT\").",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "End"
  },
  "2332fabe73c77c402eed46800a2f5c8cd5dc325ac10383184d45cb8324cac331": {
    "text": "Subclasses of [`BaseNode`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode\") define nodes for execution in the graph.\n\nNodes, which are generally [`dataclass`es](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass \"https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass\"), generally consist of:\n\n* fields containing any parameters required/optional when calling the node\n* the business logic to execute the node, in the [`run`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run\") method\n* return annotations of the [`run`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run\") method, which are read by `pydantic-graph` to determine the outgoing edges of the node\n\nNodes are generic in:\n\n* **state**, which must have the same type as the state of graphs they're included in, [`StateT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT\") has a default of `None`, so if you're not using state you can omit this generic parameter, see [stateful graphs](#stateful-graphs \"#stateful-graphs\") for more information\n* **deps**, which must have the same type as the deps of the graph they're included in, [`DepsT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.DepsT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.DepsT\") has a default of `None`, so if you're not using deps you can omit this generic parameter, see [dependency injection](#dependency-injection \"#dependency-injection\") for more information\n* **graph return type** — this only applies if the node returns [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\"). [`RunEndT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT\") has a default of [Never](https://docs.python.org/3/library/typing.html#typing.Never \"https://docs.python.org/3/library/typing.html#typing.Never\") so this generic parameter can be omitted if the node doesn't return `End`, but must be included if it does.\n\nHere's an example of a start or intermediate node in a graph — it can't end the run as it doesn't return [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\"):\n\nintermediate\\_node.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, GraphRunContext\n\n\n@dataclass\nclass MyNode(BaseNode[MyState]):  # (1)!\n    foo: int  # (2)!\n\n    async def run(\n        self,\n        ctx: GraphRunContext[MyState],  # (3)!\n    ) -> AnotherNode:  # (4)!\n        ...\n        return AnotherNode()\n```\n\n1. State in this example is `MyState` (not shown), hence `BaseNode` is parameterized with `MyState`. This node can't end the run, so the `RunEndT` generic parameter is omitted and defaults to `Never`.\n2. `MyNode` is a dataclass and has a single field `foo`, an `int`.\n3. The `run` method takes a `GraphRunContext` parameter, again parameterized with state `MyState`.\n4. The return type of the `run` method is `AnotherNode` (not shown), this is used to determine the outgoing edges of the node.\n\nWe could extend `MyNode` to optionally end the run if `foo` is divisible by 5:\n\nintermediate\\_or\\_end\\_node.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, GraphRunContext\n\n\n@dataclass\nclass MyNode(BaseNode[MyState, None, int]):  # (1)!\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[MyState],\n    ) -> AnotherNode | End[int]:  # (2)!\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return AnotherNode()\n```\n\n1. We parameterize the node with the return type (`int` in this case) as well as state. Because generic parameters are positional-only, we have to include `None` as the second parameter representing deps.\n2. The return type of the `run` method is now a union of `AnotherNode` and `End[int]`, this allows the node to end the run if `foo` is divisible by 5.",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Nodes"
  },
  "10a28a8a31ecc9d7d06ad03c461b60eb43b65a65aeaae202c919db7444362808": {
    "text": "[`Graph`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph\") — this is the execution graph itself, made up of a set of [node classes](#nodes \"#nodes\") (i.e., `BaseNode` subclasses).\n\n`Graph` is generic in:\n\n* **state** the state type of the graph, [`StateT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.StateT\")\n* **deps** the deps type of the graph, [`DepsT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.DepsT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.DepsT\")\n* **graph return type** the return type of the graph run, [`RunEndT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT\")\n\nHere's an example of a simple graph:\n\ngraph\\_example.py\n\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, None, int]):  # (1)!\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext,\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode):  # (2)!\n    foo: int\n\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\n        return DivisibleBy5(self.foo + 1)\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])  # (3)!\nresult = fives_graph.run_sync(DivisibleBy5(4))  # (4)!\nprint(result.output)\n#> 5\n```\n\n1. The `DivisibleBy5` node is parameterized with `None` for the state param and `None` for the deps param as this graph doesn't use state or deps, and `int` as it can end the run.\n2. The `Increment` node doesn't return `End`, so the `RunEndT` generic parameter is omitted, state can also be omitted as the graph doesn't use state.\n3. The graph is created with a sequence of nodes.\n4. The graph is run synchronously with [`run_sync`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run_sync \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run_sync\"). The initial node is `DivisibleBy5(4)`. Because the graph doesn't use external state or deps, we don't pass `state` or `deps`.\n\n*(This example is complete, it can be run \"as is\")*\n\nA [mermaid diagram](#mermaid-diagrams \"#mermaid-diagrams\") for this graph can be generated with the following code:\n\ngraph\\_example\\_diagram.py\n\n```\nfrom graph_example import DivisibleBy5, fives_graph\n\nfives_graph.mermaid_code(start_node=DivisibleBy5)\n```\n\n```\n---\ntitle: fives_graph\n---\nstateDiagram-v2\n  [*] --> DivisibleBy5\n  DivisibleBy5 --> Increment\n  DivisibleBy5 --> [*]\n  Increment --> DivisibleBy5\n```\n\nIn order to visualize a graph within a `jupyter-notebook`, `IPython.display` needs to be used:\n\njupyter\\_display\\_mermaid.py\n\n```\nfrom graph_example import DivisibleBy5, fives_graph\nfrom IPython.display import Image, display\n\ndisplay(Image(fives_graph.mermaid_image(start_node=DivisibleBy5)))\n```",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Graph"
  },
  "de98c893d3c5332049aba4dbc951c0abdb22b3fb2c6fa36d147ed7ab875e4882": {
    "text": "The \"state\" concept in `pydantic-graph` provides an optional way to access and mutate an object (often a `dataclass` or Pydantic model) as nodes run in a graph. If you think of Graphs as a production line, then your state is the engine being passed along the line and built up by each node as the graph is run.\n\n`pydantic-graph` provides state persistence, with the state recorded after each node is run. (See [State Persistence](#state-persistence \"#state-persistence\").)\n\nHere's an example of a graph which represents a vending machine where the user may insert coins and select a product to purchase.\n\nvending\\_machine.py\n\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom rich.prompt import Prompt\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass MachineState:  # (1)!\n    user_balance: float = 0.0\n    product: str | None = None\n\n\n@dataclass\nclass InsertCoin(BaseNode[MachineState]):  # (3)!\n    async def run(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted:  # (16)!\n        return CoinsInserted(float(Prompt.ask('Insert coins')))  # (4)!\n\n\n@dataclass\nclass CoinsInserted(BaseNode[MachineState]):\n    amount: float  # (5)!\n\n    async def run(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> SelectProduct | Purchase:  # (17)!\n        ctx.state.user_balance += self.amount  # (6)!\n        if ctx.state.product is not None:  # (7)!\n            return Purchase(ctx.state.product)\n        else:\n            return SelectProduct()\n\n\n@dataclass\nclass SelectProduct(BaseNode[MachineState]):\n    async def run(self, ctx: GraphRunContext[MachineState]) -> Purchase:\n        return Purchase(Prompt.ask('Select product'))\n\n\nPRODUCT_PRICES = {  # (2)!\n    'water': 1.25,\n    'soda': 1.50,\n    'crisps': 1.75,\n    'chocolate': 2.00,\n}\n\n\n@dataclass\nclass Purchase(BaseNode[MachineState, None, None]):  # (18)!\n    product: str\n\n    async def run(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> End | InsertCoin | SelectProduct:\n        if price := PRODUCT_PRICES.get(self.product):  # (8)!\n            ctx.state.product = self.product  # (9)!\n            if ctx.state.user_balance >= price:  # (10)!\n                ctx.state.user_balance -= price\n                return End(None)\n            else:\n                diff = price - ctx.state.user_balance\n                print(f'Not enough money for {self.product}, need {diff:0.2f} more')\n                #> Not enough money for crisps, need 0.75 more\n                return InsertCoin()  # (11)!\n        else:\n            print(f'No such product: {self.product}, try again')\n            return SelectProduct()  # (12)!\n\n\nvending_machine_graph = Graph(  # (13)!\n    nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\n)\n\n\nasync def main():\n    state = MachineState()  # (14)!\n    await vending_machine_graph.run(InsertCoin(), state=state)  # (15)!\n    print(f'purchase successful item={state.product} change={state.user_balance:0.2f}')\n    #> purchase successful item=crisps change=0.25\n```\n\n1. The state of the vending machine is defined as a dataclass with the user's balance and the product they've selected, if any.\n2. A dictionary of products mapped to prices.\n3. The `InsertCoin` node, [`BaseNode`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode\") is parameterized with `MachineState` as that's the state used in this graph.\n4. The `InsertCoin` node prompts the user to insert coins. We keep things simple by just entering a monetary amount as a float. Before you start thinking this is a toy too since it's using [rich's `Prompt.ask`](https://rich.readthedocs.io/en/stable/reference/prompt.html#rich.prompt.PromptBase.ask \"https://rich.readthedocs.io/en/stable/reference/prompt.html#rich.prompt.PromptBase.ask\") within nodes, see [below](#example-human-in-the-loop \"#example-human-in-the-loop\") for how control flow can be managed when nodes require external input.\n5. The `CoinsInserted` node; again this is a [`dataclass`](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass \"https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass\") with one field `amount`.\n6. Update the user's balance with the amount inserted.\n7. If the user has already selected a product, go to `Purchase`, otherwise go to `SelectProduct`.\n8. In the `Purchase` node, look up the price of the product if the user entered a valid product.\n9. If the user did enter a valid product, set the product in the state so we don't revisit `SelectProduct`.\n10. If the balance is enough to purchase the product, adjust the balance to reflect the purchase and return [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\") to end the graph. We're not using the run return type, so we call `End` with `None`.\n11. If the balance is insufficient, go to `InsertCoin` to prompt the user to insert more coins.\n12. If the product is invalid, go to `SelectProduct` to prompt the user to select a product again.\n13. The graph is created by passing a list of nodes to [`Graph`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph\"). Order of nodes is not important, but it can affect how [diagrams](#mermaid-diagrams \"#mermaid-diagrams\") are displayed.\n14. Initialize the state. This will be passed to the graph run and mutated as the graph runs.\n15. Run the graph with the initial state. Since the graph can be run from any node, we must pass the start node — in this case, `InsertCoin`. [`Graph.run`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run\") returns a [`GraphRunResult`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRunResult \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRunResult\") that provides the final data and a history of the run.\n16. The return type of the node's [`run`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run\") method is important as it is used to determine the outgoing edges of the node. This information in turn is used to render [mermaid diagrams](#mermaid-diagrams \"#mermaid-diagrams\") and is enforced at runtime to detect misbehavior as soon as possible.\n17. The return type of `CoinsInserted`'s [`run`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode.run\") method is a union, meaning multiple outgoing edges are possible.\n18. Unlike other nodes, `Purchase` can end the run, so the [`RunEndT`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.RunEndT\") generic parameter must be set. In this case it's `None` since the graph run return type is `None`.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nA [mermaid diagram](#mermaid-diagrams \"#mermaid-diagrams\") for this graph can be generated with the following code:\n\nvending\\_machine\\_diagram.py\n\n```\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin)\n```\n\nThe diagram generated by the above code is:\n\n```\n---\ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```\n\nSee [below](#mermaid-diagrams \"#mermaid-diagrams\") for more information on generating diagrams.",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Stateful Graphs"
  },
  "c3f7384fe58778539c82dfc502e7187d518440016f058df5dae45b14cf146bc6": {
    "text": "So far we haven't shown an example of a Graph that actually uses Pydantic AI or GenAI at all.\n\nIn this example, one agent generates a welcome email to a user and the other agent provides feedback on the email.\n\nThis graph has a very simple structure:\n\n```\n---\ntitle: feedback_graph\n---\nstateDiagram-v2\n  [*] --> WriteEmail\n  WriteEmail --> Feedback\n  Feedback --> WriteEmail\n  Feedback --> [*]\n```\n\ngenai\\_email\\_feedback.py\n\n```\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass, field\n\nfrom pydantic import BaseModel, EmailStr\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_ai.messages import ModelMessage\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass User:\n    name: str\n    email: EmailStr\n    interests: list[str]\n\n\n@dataclass\nclass Email:\n    subject: str\n    body: str\n\n\n@dataclass\nclass State:\n    user: User\n    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\nemail_writer_agent = Agent(\n    'google-gla:gemini-1.5-pro',\n    output_type=Email,\n    system_prompt='Write a welcome email to our tech blog.',\n)\n\n\n@dataclass\nclass WriteEmail(BaseNode[State]):\n    email_feedback: str | None = None\n\n    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\n        if self.email_feedback:\n            prompt = (\n                f'Rewrite the email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}\\n'\n                f'Feedback: {self.email_feedback}'\n            )\n        else:\n            prompt = (\n                f'Write a welcome email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}'\n            )\n\n        result = await email_writer_agent.run(\n            prompt,\n            message_history=ctx.state.write_agent_messages,\n        )\n        ctx.state.write_agent_messages += result.new_messages()\n        return Feedback(result.output)\n\n\nclass EmailRequiresWrite(BaseModel):\n    feedback: str\n\n\nclass EmailOk(BaseModel):\n    pass\n\n\nfeedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\n    'openai:gpt-4o',\n    output_type=EmailRequiresWrite | EmailOk,  # type: ignore\n    system_prompt=(\n        'Review the email and provide feedback, email must reference the users specific interests.'\n    ),\n)\n\n\n@dataclass\nclass Feedback(BaseNode[State, None, Email]):\n    email: Email\n\n    async def run(\n        self,\n        ctx: GraphRunContext[State],\n    ) -> WriteEmail | End[Email]:\n        prompt = format_as_xml({'user': ctx.state.user, 'email': self.email})\n        result = await feedback_agent.run(prompt)\n        if isinstance(result.output, EmailRequiresWrite):\n            return WriteEmail(email_feedback=result.output.feedback)\n        else:\n            return End(self.email)\n\n\nasync def main():\n    user = User(\n        name='John Doe',\n        email='john.joe@example.com',\n        interests=['Haskel', 'Lisp', 'Fortran'],\n    )\n    state = State(user)\n    feedback_graph = Graph(nodes=(WriteEmail, Feedback))\n    result = await feedback_graph.run(WriteEmail(), state=state)\n    print(result.output)\n    \"\"\"\n    Email(\n        subject='Welcome to our tech blog!',\n        body='Hello John, Welcome to our tech blog! ...',\n    )\n    \"\"\"\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "GenAI Example"
  },
  "02c43a70d3a07631277b084d24e796d9d0fcbcfd41c0535167c7afaf33aa6614": {
    "text": "Sometimes you want direct control or insight into each node as the graph executes. The easiest way to do that is with the [`Graph.iter`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter\") method, which returns a **context manager** that yields a [`GraphRun`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun\") object. The `GraphRun` is an async-iterable over the nodes of your graph, allowing you to record or modify them as they execute.\n\nHere's an example:\n\ncount\\_down.py\n\n```\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom pydantic_graph import Graph, BaseNode, End, GraphRunContext\n\n\n@dataclass\nclass CountDownState:\n    counter: int\n\n\n@dataclass\nclass CountDown(BaseNode[CountDownState, None, int]):\n    async def run(self, ctx: GraphRunContext[CountDownState]) -> CountDown | End[int]:\n        if ctx.state.counter <= 0:\n            return End(ctx.state.counter)\n        ctx.state.counter -= 1\n        return CountDown()\n\n\ncount_down_graph = Graph(nodes=[CountDown])\n\n\nasync def main():\n    state = CountDownState(counter=3)\n    async with count_down_graph.iter(CountDown(), state=state) as run:  # (1)!\n        async for node in run:  # (2)!\n            print('Node:', node)\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: End(data=0)\n    print('Final output:', run.result.output)  # (3)!\n    #> Final output: 0\n```\n\n1. `Graph.iter(...)` returns a [`GraphRun`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun\").\n2. Here, we step through each node as it is executed.\n3. Once the graph returns an [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\"), the loop ends, and `run.result` becomes a [`GraphRunResult`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRunResult \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRunResult\") containing the final outcome (`0` here).",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Using `Graph.iter` for `async for` iteration"
  },
  "aa1f8e36dbc09047dc4297bba03827fdb21651b8a9bf3fa4fbc98a8a313c3ec5": {
    "text": "Alternatively, you can drive iteration manually with the [`GraphRun.next`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun.next \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun.next\") method, which allows you to pass in whichever node you want to run next. You can modify or selectively skip nodes this way.\n\nBelow is a contrived example that stops whenever the counter is at 2, ignoring any node runs beyond that:\n\ncount\\_down\\_next.py\n\n```\nfrom pydantic_graph import End, FullStatePersistence\nfrom count_down import CountDown, CountDownState, count_down_graph\n\n\nasync def main():\n    state = CountDownState(counter=5)\n    persistence = FullStatePersistence()  # (7)!\n    async with count_down_graph.iter(\n        CountDown(), state=state, persistence=persistence\n    ) as run:\n        node = run.next_node  # (1)!\n        while not isinstance(node, End):  # (2)!\n            print('Node:', node)\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            if state.counter == 2:\n                break  # (3)!\n            node = await run.next(node)  # (4)!\n\n        print(run.result)  # (5)!\n        #> None\n\n        for step in persistence.history:  # (6)!\n            print('History Step:', step.state, step.state)\n            #> History Step: CountDownState(counter=5) CountDownState(counter=5)\n            #> History Step: CountDownState(counter=4) CountDownState(counter=4)\n            #> History Step: CountDownState(counter=3) CountDownState(counter=3)\n            #> History Step: CountDownState(counter=2) CountDownState(counter=2)\n```\n\n1. We start by grabbing the first node that will be run in the agent's graph.\n2. The agent run is finished once an `End` node has been produced; instances of `End` cannot be passed to `next`.\n3. If the user decides to stop early, we break out of the loop. The graph run won't have a real final result in that case (`run.result` remains `None`).\n4. At each step, we call `await run.next(node)` to run it and get the next node (or an `End`).\n5. Because we did not continue the run until it finished, the `result` is not set.\n6. The run's history is still populated with the steps we executed so far.\n7. Use [`FullStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.FullStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.FullStatePersistence\") so we can show the history of the run, see [State Persistence](#state-persistence \"#state-persistence\") below for more information.",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Using `GraphRun.next(node)` manually"
  },
  "937d899221b029fd7e319b9e9026961380116debe2f0bd5119bb2b418ff73865": {
    "text": "One of the biggest benefits of finite state machine (FSM) graphs is how they simplify the handling of interrupted execution. This might happen for a variety of reasons:\n\n* the state machine logic might fundamentally need to be paused — e.g. the returns workflow for an e-commerce order needs to wait for the item to be posted to the returns center or because execution of the next node needs input from a user so needs to wait for a new http request,\n* the execution takes so long that the entire graph can't reliably be executed in a single continuous run — e.g. a deep research agent that might take hours to run,\n* you want to run multiple graph nodes in parallel in different processes / hardware instances (note: parallel node execution is not yet supported in `pydantic-graph`, see [#704](https://github.com/pydantic/pydantic-ai/issues/704 \"https://github.com/pydantic/pydantic-ai/issues/704\")).\n\nTrying to make a conventional control flow (i.e., boolean logic and nested function calls) implementation compatible with these usage scenarios generally results in brittle and over-complicated spaghetti code, with the logic required to interrupt and resume execution dominating the implementation.\n\nTo allow graph runs to be interrupted and resumed, `pydantic-graph` provides state persistence — a system for snapshotting the state of a graph run before and after each node is run, allowing a graph run to be resumed from any point in the graph.\n\n`pydantic-graph` includes three state persistence implementations:\n\n* [`SimpleStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.SimpleStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.SimpleStatePersistence\") — Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default.\n* [`FullStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.FullStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.in_mem.FullStatePersistence\") — In memory state persistence that hold a list of snapshots.\n* [`FileStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence\") — File-based state persistence that saves snapshots to a JSON file.\n\nIn production applications, developers should implement their own state persistence by subclassing [`BaseStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence\") abstract base class, which might persist runs in a relational database like PostgresQL.\n\nAt a high level the role of `StatePersistence` implementations is to store and retrieve [`NodeSnapshot`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.NodeSnapshot \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.NodeSnapshot\") and [`EndSnapshot`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.EndSnapshot \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.EndSnapshot\") objects.\n\n[`graph.iter_from_persistence()`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence\") may be used to run the graph based on the state stored in persistence.\n\nWe can run the `count_down_graph` from [above](#iterating-over-a-graph \"#iterating-over-a-graph\"), using [`graph.iter_from_persistence()`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence\") and [`FileStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence\").\n\nAs you can see in this code, `run_node` requires no external application state (apart from state persistence) to be run, meaning graphs can easily be executed by distributed execution and queueing systems.\n\ncount\\_down\\_from\\_persistence.py\n\n```\nfrom pathlib import Path\n\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.file import FileStatePersistence\n\nfrom count_down import CountDown, CountDownState, count_down_graph\n\n\nasync def main():\n    run_id = 'run_abc123'\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))  # (1)!\n    state = CountDownState(counter=5)\n    await count_down_graph.initialize(  # (2)!\n        CountDown(), state=state, persistence=persistence\n    )\n\n    done = False\n    while not done:\n        done = await run_node(run_id)\n\n\nasync def run_node(run_id: str) -> bool:  # (3)!\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))\n    async with count_down_graph.iter_from_persistence(persistence) as run:  # (4)!\n        node_or_end = await run.next()  # (5)!\n\n    print('Node:', node_or_end)\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: End(data=0)\n    return isinstance(node_or_end, End)  # (6)!\n```\n\n1. Create a [`FileStatePersistence`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.file.FileStatePersistence\") to use to start the graph.\n2. Call [`graph.initialize()`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.initialize \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.initialize\") to set the initial graph state in the persistence object.\n3. `run_node` is a pure function that doesn't need access to any other process state to run the next node of the graph, except the ID of the run.\n4. Call [`graph.iter_from_persistence()`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.iter_from_persistence\") create a [`GraphRun`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun\") object that will run the next node of the graph from the state stored in persistence. This will return either a node or an `End` object.\n5. [`graph.run()`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph.run\") will return either a [node](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode\") or an [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\") object.\n6. Check if the node is an [`End`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.End \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.End\") object, if it is, the graph run is complete.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "State Persistence"
  },
  "9c6bc313ab53e5b45434aba06b79c0ccde66dc89f8bdf2845af15352b30d9f56": {
    "text": "As noted above, state persistence allows graphs to be interrupted and resumed. One use case of this is to allow user input to continue.\n\nIn this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong.\n\nInstead of running the entire graph in a single process invocation, we run the graph by running the process repeatedly, optionally providing an answer to the question as a command line argument.\n\n`ai_q_and_a_graph.py` — `question_graph` definition\n\nai\\_q\\_and\\_a\\_graph.py\n\n```\nfrom __future__ import annotations as _annotations\n\nfrom typing import Annotated\nfrom pydantic_graph import Edge\nfrom dataclasses import dataclass, field\nfrom pydantic import BaseModel\nfrom pydantic_graph import (\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_ai.messages import ModelMessage\n\nask_agent = Agent('openai:gpt-4o', output_type=str, instrument=True)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    \"\"\"Generate question using GPT-4o.\"\"\"\n    docstring_notes = True\n    async def run(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationResult(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-4o',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nai\\_q\\_and\\_a\\_run.py\n\n```\nimport sys\nfrom pathlib import Path\n\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.file import FileStatePersistence\nfrom pydantic_ai.messages import ModelMessage  # noqa: F401\n\nfrom ai_q_and_a_graph import Ask, question_graph, Evaluate, QuestionState, Answer\n\n\nasync def main():\n    answer: str | None = sys.argv[1] if len(sys.argv) > 1 else None  # (1)!\n    persistence = FileStatePersistence(Path('question_graph.json'))  # (2)!\n    persistence.set_graph_types(question_graph)  # (3)!\n\n    if snapshot := await persistence.load_next():  # (4)!\n        state = snapshot.state\n        assert answer is not None\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()  # (5)!\n\n    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()  # (6)!\n            if isinstance(node, End):  # (7)!\n                print('END:', node.data)\n                history = await persistence.load_all()  # (8)!\n                print([e.node for e in history])\n                break\n            elif isinstance(node, Answer):  # (9)!\n                print(node.question)\n                #> What is the capital of France?\n                break\n            # otherwise just continue\n```\n\n1. Get the user's answer from the command line, if provided. See [question graph example](../examples/question-graph/ \"../examples/question-graph/\") for a complete example.\n2. Create a state persistence instance the `'question_graph.json'` file may or may not already exist.\n3. Since we're using the [persistence interface](../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence\") outside a graph, we need to call [`set_graph_types`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.set_graph_types \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.set_graph_types\") to set the graph generic types `StateT` and `RunEndT` for the persistence instance. This is necessary to allow the persistence instance to know how to serialize and deserialize graph nodes.\n4. If we're run the graph before, [`load_next`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.load_next \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.load_next\") will return a snapshot of the next node to run, here we use `state` from that snapshot, and create a new `Evaluate` node with the answer provided on the command line.\n5. If the graph hasn't been run before, we create a new `QuestionState` and start with the `Ask` node.\n6. Call [`GraphRun.next()`](../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun.next \"../api/pydantic_graph/graph/#pydantic_graph.graph.GraphRun.next\") to run the node. This will return either a node or an `End` object.\n7. If the node is an `End` object, the graph run is complete. The `data` field of the `End` object contains the comment returned by the `evaluate_agent` about the correct answer.\n8. To demonstrate the state persistence, we call [`load_all`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.load_all \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.BaseStatePersistence.load_all\") to get all the snapshots from the persistence instance. This will return a list of [`Snapshot`](../api/pydantic_graph/persistence/#pydantic_graph.persistence.Snapshot \"../api/pydantic_graph/persistence/#pydantic_graph.persistence.Snapshot\") objects.\n9. If the node is an `Answer` object, we print the question and break out of the loop to end the process and wait for user input.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nFor a complete example of this graph, see the [question graph example](../examples/question-graph/ \"../examples/question-graph/\").",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Example: Human in the loop."
  },
  "a41515ded81d53b1168de4943cf21d11c611398854c8a688833acda2a39ab582": {
    "text": "As with Pydantic AI, `pydantic-graph` supports dependency injection via a generic parameter on [`Graph`](../api/pydantic_graph/graph/#pydantic_graph.graph.Graph \"../api/pydantic_graph/graph/#pydantic_graph.graph.Graph\") and [`BaseNode`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.BaseNode\"), and the [`GraphRunContext.deps`](../api/pydantic_graph/nodes/#pydantic_graph.nodes.GraphRunContext.deps \"../api/pydantic_graph/nodes/#pydantic_graph.nodes.GraphRunContext.deps\") field.\n\nAs an example of dependency injection, let's modify the `DivisibleBy5` example [above](#graph \"#graph\") to use a [`ProcessPoolExecutor`](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor \"https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor\") to run the compute load in a separate process (this is a contrived example, `ProcessPoolExecutor` wouldn't actually improve performance in this example):\n\ndeps\\_example.py\n\n```\nfrom __future__ import annotations\n\nimport asyncio\nfrom concurrent.futures import ProcessPoolExecutor\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, FullStatePersistence, Graph, GraphRunContext\n\n\n@dataclass\nclass GraphDeps:\n    executor: ProcessPoolExecutor\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, GraphDeps, int]):\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[None, GraphDeps],\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode[None, GraphDeps]):\n    foo: int\n\n    async def run(self, ctx: GraphRunContext[None, GraphDeps]) -> DivisibleBy5:\n        loop = asyncio.get_running_loop()\n        compute_result = await loop.run_in_executor(\n            ctx.deps.executor,\n            self.compute,\n        )\n        return DivisibleBy5(compute_result)\n\n    def compute(self) -> int:\n        return self.foo + 1\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\n\n\nasync def main():\n    with ProcessPoolExecutor() as executor:\n        deps = GraphDeps(executor)\n        result = await fives_graph.run(DivisibleBy5(3), deps=deps, persistence=FullStatePersistence())\n    print(result.output)\n    #> 5\n    # the full history is quite verbose (see below), so we'll just print the summary\n    print([item.node for item in result.persistence.history])\n    \"\"\"\n    [\n        DivisibleBy5(foo=3),\n        Increment(foo=3),\n        DivisibleBy5(foo=4),\n        Increment(foo=4),\n        DivisibleBy5(foo=5),\n        End(data=5),\n    ]\n    \"\"\"\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Dependency Injection"
  },
  "6ec905e6980442e49bbfab926b8ebd89f6474ac479357e12a1aaa8c674a74b6c": {
    "text": "Pydantic Graph can generate [mermaid](https://mermaid.js.org/ \"https://mermaid.js.org/\") [`stateDiagram-v2`](https://mermaid.js.org/syntax/stateDiagram.html \"https://mermaid.js.org/syntax/stateDiagram.html\") diagrams for graphs, as shown above.\n\nThese diagrams can be generated with:\n\nBeyond the diagrams shown above, you can also customize mermaid diagrams with the following options:\n\nPutting that together, we can edit the last [`ai_q_and_a_graph.py`](#example-human-in-the-loop \"#example-human-in-the-loop\") example to:\n\n* add labels to some edges\n* add a note to the `Ask` node\n* highlight the `Answer` node\n* save the diagram as a `PNG` image to file\n\nai\\_q\\_and\\_a\\_graph\\_extra.py\n\n```\nfrom typing import Annotated\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext, Edge\n\nask_agent = Agent('openai:gpt-4o', output_type=str, instrument=True)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    \"\"\"Generate question using GPT-4o.\"\"\"\n    docstring_notes = True\n    async def run(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationResult(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-4o',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n```\n\n*(This example is not complete and cannot be run directly)*\n\nThis would generate an image that looks like this:\n\n```\n---\ntitle: question_graph\n---\nstateDiagram-v2\n  Ask --> Answer: Ask the question\n  note right of Ask\n    Judge the answer.\n    Decide on next step.\n  end note\n  Answer --> Evaluate\n  Evaluate --> Reprimand\n  Evaluate --> [*]: success\n  Reprimand --> Ask\n\nclassDef highlighted fill:#fdff32\nclass Answer highlighted\n```",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Mermaid Diagrams"
  },
  "a1d88f669b7f5a02899442f44dac222d953a0abd2360fbde3389e029f79c328b": {
    "text": "You can specify the direction of the state diagram using one of the following values:\n\n* `'TB'`: Top to bottom, the diagram flows vertically from top to bottom.\n* `'LR'`: Left to right, the diagram flows horizontally from left to right.\n* `'RL'`: Right to left, the diagram flows horizontally from right to left.\n* `'BT'`: Bottom to top, the diagram flows vertically from bottom to top.\n\nHere is an example of how to do this using 'Left to Right' (LR) instead of the default 'Top to Bottom' (TB):\n\nvending\\_machine\\_diagram.py\n\n```\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin, direction='LR')\n```\n\n```\n---\ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  direction LR\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```",
    "source_url": "https://ai.pydantic.dev/graph/",
    "header": "Setting Direction of the State Diagram"
  },
  "bc3645e6e9b9ce1d7cb6435cba62ba3af3791337d0b63a376876a6280477c563": {
    "text": "\"Evals\" refers to evaluating a model's performance for a specific application.\n\nWarning\n\nUnlike unit tests, evals are an emerging art/science; anyone who claims to know for sure exactly how your evals should be defined can safely be ignored.\n\nPydantic Evals is a powerful evaluation framework designed to help you systematically test and evaluate the performance and accuracy of the systems you build, especially when working with LLMs.\n\nWe've designed Pydantic Evals to be useful while not being too opinionated since we (along with everyone else) are still figuring out best practices. We'd love your [feedback](../help/ \"../help/\") on the package and how we can improve it.\n\nIn Beta\n\nPydantic Evals support was [introduced](https://github.com/pydantic/pydantic-ai/pull/935 \"https://github.com/pydantic/pydantic-ai/pull/935\") in v0.0.47 and is currently in beta. The API is subject to change and the documentation is incomplete.",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Evals"
  },
  "b8ad6e96ba196ce3a6321a54b5f98df80479b88cd2b4bef180a79a55ff004e9b": {
    "text": "To install the Pydantic Evals package, run:\n\n`pydantic-evals` does not depend on `pydantic-ai`, but has an optional dependency on `logfire` if you'd like to\nuse OpenTelemetry traces in your evals, or send evaluation results to [logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\").",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Installation"
  },
  "d0c2876b40dafba6a5dc068384828d835c7b3dceebc6152e041ff19105b7cf73": {
    "text": "In Pydantic Evals, everything begins with `Dataset`s and `Case`s:\n\n* [`Case`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Case \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Case\"): A single test scenario corresponding to \"task\" inputs. Can also optionally have a name, expected outputs, metadata, and evaluators.\n* [`Dataset`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset\"): A collection of test cases designed for the evaluation of a specific task or function.\n\nsimple\\_eval\\_dataset.py\n\n```\nfrom pydantic_evals import Case, Dataset\n\ncase1 = Case(\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\ndataset = Dataset(cases=[case1])\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Datasets and Cases"
  },
  "8bc373beaf046c548ff826297d485d30847226207dbd523967d422e5a80d7293": {
    "text": "Evaluators are the components that analyze and score the results of your task when tested against a case.\n\nPydantic Evals includes several built-in evaluators and allows you to create custom evaluators:\n\nsimple\\_eval\\_evaluator.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\nfrom pydantic_evals.evaluators.common import IsInstance\n\nfrom simple_eval_dataset import dataset\n\ndataset.add_evaluator(IsInstance(type_name='str'))  # (1)!\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:  # (2)!\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset.add_evaluator(MyEvaluator())\n```\n\n1. You can add built-in evaluators to a dataset using the [`add_evaluator`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.add_evaluator \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.add_evaluator\") method.\n2. This custom evaluator returns a simple score based on whether the output matches the expected output.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Evaluators"
  },
  "004bb4a5e3b357552c1fd2628b7a1de6aa2ac12bad2cf8d0db7dfb5326a13bd6": {
    "text": "The evaluation process involves running a task against all cases in a dataset:\n\nPutting the above two examples together and using the more declarative `evaluators` kwarg to [`Dataset`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset\"):\n\nsimple\\_eval\\_complete.py\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(  # (1)!\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\n\nclass MyEvaluator(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset = Dataset(\n    cases=[case1],\n    evaluators=[IsInstance(type_name='str'), MyEvaluator()],  # (3)!\n)\n\n\nasync def guess_city(question: str) -> str:  # (4)!\n    return 'Paris'\n\n\nreport = dataset.evaluate_sync(guess_city)  # (5)!\nreport.print(include_input=True, include_output=True, include_durations=False)  # (6)!\n\"\"\"\n                              Evaluation Summary: guess_city\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Case ID     ┃ Inputs                         ┃ Outputs ┃ Scores            ┃ Assertions ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ simple_case │ What is the capital of France? │ Paris   │ MyEvaluator: 1.00 │ ✔          │\n├─────────────┼────────────────────────────────┼─────────┼───────────────────┼────────────┤\n│ Averages    │                                │         │ MyEvaluator: 1.00 │ 100.0% ✔   │\n└─────────────┴────────────────────────────────┴─────────┴───────────────────┴────────────┘\n\"\"\"\n```\n\n1. Create a [test case](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Case \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Case\") as above\n2. Also create a custom evaluator function as above\n3. Create a [`Dataset`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset\") with test cases, also set the [`evaluators`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluators \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluators\") when creating the dataset\n4. Our function to evaluate.\n5. Run the evaluation with [`evaluate_sync`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluate_sync \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluate_sync\"), which runs the function against all test cases in the dataset, and returns an [`EvaluationReport`](../api/pydantic_evals/reporting/#pydantic_evals.reporting.EvaluationReport \"../api/pydantic_evals/reporting/#pydantic_evals.reporting.EvaluationReport\") object.\n6. Print the report with [`print`](../api/pydantic_evals/reporting/#pydantic_evals.reporting.EvaluationReport.print \"../api/pydantic_evals/reporting/#pydantic_evals.reporting.EvaluationReport.print\"), which shows the results of the evaluation, including input and output. We have omitted duration here just to keep the printed output from changing from run to run.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Evaluation Process"
  },
  "b4ed1a4f47ca68a7ce9b969449e3816acc67fc3f67c4b8c93768648af0f3fe6d": {
    "text": "In this example we evaluate a method for generating recipes based on customer orders.\n\njudge\\_recipes.py\n\n```\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\n\n\nclass CustomerOrder(BaseModel):  # (1)!\n    dish_name: str\n    dietary_restriction: str | None = None\n\n\nclass Recipe(BaseModel):\n    ingredients: list[str]\n    steps: list[str]\n\n\nrecipe_agent = Agent(\n    'groq:llama-3.3-70b-versatile',\n    output_type=Recipe,\n    system_prompt=(\n        'Generate a recipe to cook the dish that meets the dietary restrictions.'\n    ),\n)\n\n\nasync def transform_recipe(customer_order: CustomerOrder) -> Recipe:  # (2)!\n    r = await recipe_agent.run(format_as_xml(customer_order))\n    return r.output\n\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](  # (3)!\n    cases=[\n        Case(\n            name='vegetarian_recipe',\n            inputs=CustomerOrder(\n                dish_name='Spaghetti Bolognese', dietary_restriction='vegetarian'\n            ),\n            expected_output=None,  # (4)\n            metadata={'focus': 'vegetarian'},\n            evaluators=(\n                LLMJudge(  # (5)!\n                    rubric='Recipe should not contain meat or animal products',\n                ),\n            ),\n        ),\n        Case(\n            name='gluten_free_recipe',\n            inputs=CustomerOrder(\n                dish_name='Chocolate Cake', dietary_restriction='gluten-free'\n            ),\n            expected_output=None,\n            metadata={'focus': 'gluten-free'},\n            # Case-specific evaluator with a focused rubric\n            evaluators=(\n                LLMJudge(\n                    rubric='Recipe should not contain gluten or wheat products',\n                ),\n            ),\n        ),\n    ],\n    evaluators=[  # (6)!\n        IsInstance(type_name='Recipe'),\n        LLMJudge(\n            rubric='Recipe should have clear steps and relevant ingredients',\n            include_input=True,\n            model='anthropic:claude-3-7-sonnet-latest',  # (7)!\n        ),\n    ],\n)\n\n\nreport = recipe_dataset.evaluate_sync(transform_recipe)\nprint(report)\n\"\"\"\n     Evaluation Summary: transform_recipe\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID            ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ vegetarian_recipe  │ ✔✔✔        │     10ms │\n├────────────────────┼────────────┼──────────┤\n│ gluten_free_recipe │ ✔✔✔        │     10ms │\n├────────────────────┼────────────┼──────────┤\n│ Averages           │ 100.0% ✔   │     10ms │\n└────────────────────┴────────────┴──────────┘\n\"\"\"\n```\n\n1. Define models for our task — Input for recipe generation task and output of the task.\n2. Define our recipe generation function - this is the task we want to evaluate.\n3. Create a dataset with different test cases and different rubrics.\n4. No expected output, we'll let the LLM judge the quality.\n5. Case-specific evaluator with a focused rubric using [`LLMJudge`](../api/pydantic_evals/evaluators/#pydantic_evals.evaluators.LLMJudge \"../api/pydantic_evals/evaluators/#pydantic_evals.evaluators.LLMJudge\").\n6. Dataset-level evaluators that apply to all cases, including a general quality rubric for all recipes\n7. By default `LLMJudge` uses `openai:gpt-4o`, here we use a specific Anthropic model.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Evaluation with `LLMJudge`"
  },
  "e91c4da94548405bd3d621bf0b1237c33c624520e881c00c29cf54681ea5d67f": {
    "text": "Datasets can be saved to and loaded from YAML or JSON files.\n\nsave\\_load\\_dataset\\_example.py\n\n```\nfrom pathlib import Path\n\nfrom judge_recipes import CustomerOrder, Recipe, recipe_dataset\n\nfrom pydantic_evals import Dataset\n\nrecipe_transforms_file = Path('recipe_transform_tests.yaml')\nrecipe_dataset.to_file(recipe_transforms_file)  # (1)!\nprint(recipe_transforms_file.read_text())\n\"\"\"",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Saving and Loading Datasets"
  },
  "fd551c774c666de75763c9370f4d9ac5308b4fca66ac9e0e263c6a39da48311c": {
    "text": "cases:\n- name: vegetarian_recipe\n  inputs:\n    dish_name: Spaghetti Bolognese\n    dietary_restriction: vegetarian\n  metadata:\n    focus: vegetarian\n  evaluators:\n  - LLMJudge: Recipe should not contain meat or animal products\n- name: gluten_free_recipe\n  inputs:\n    dish_name: Chocolate Cake\n    dietary_restriction: gluten-free\n  metadata:\n    focus: gluten-free\n  evaluators:\n  - LLMJudge: Recipe should not contain gluten or wheat products\nevaluators:\n- IsInstance: Recipe\n- LLMJudge:\n    rubric: Recipe should have clear steps and relevant ingredients\n    model: anthropic:claude-3-7-sonnet-latest\n    include_input: true\n\"\"\"",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "yaml-language-server: $schema=recipe_transform_tests_schema.json"
  },
  "81514e3d42000e0ccde04d046a0fc819e2cda7694cc9e44c752ac59112036d66": {
    "text": "loaded_dataset = Dataset[CustomerOrder, Recipe, dict].from_file(recipe_transforms_file)\n\nprint(f'Loaded dataset with {len(loaded_dataset.cases)} cases')\n#> Loaded dataset with 2 cases\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Load dataset from file"
  },
  "0b6178b04c3feabd11cdd1145aa5b13ee758b1d0a983d0e9c04ca1730552b66b": {
    "text": "You can control concurrency during evaluation (this might be useful to prevent exceeding a rate limit):\n\nparallel\\_evaluation\\_example.py\n\n```\nimport asyncio\nimport time\n\nfrom pydantic_evals import Case, Dataset",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Parallel Evaluation"
  },
  "cafb65fda144ace0fa8838d74b7c773c6f0c38931dc62998ebef5e16ba9c2552": {
    "text": "dataset = Dataset(\n    cases=[\n        Case(\n            name=f'case_{i}',\n            inputs=i,\n            expected_output=i * 2,\n        )\n        for i in range(5)\n    ]\n)\n\n\nasync def double_number(input_value: int) -> int:\n    \"\"\"Function that simulates work by sleeping for a tenth of a second before returning double the input.\"\"\"\n    await asyncio.sleep(0.1)  # Simulate work\n    return input_value * 2",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Create a dataset with multiple test cases"
  },
  "785fb60426f6e50c9a138cf42119f678f197ff909ce67b4991cc01d46374aaca": {
    "text": "t0 = time.time()\nreport_default = dataset.evaluate_sync(double_number)\nprint(f'Evaluation took less than 0.5s: {time.time() - t0 < 0.5}')\n#> Evaluation took less than 0.5s: True\n\nreport_default.print(include_input=True, include_output=True, include_durations=False)  # (1)!\n\"\"\"\n      Evaluation Summary:\n         double_number\n┏━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n┃ Case ID  ┃ Inputs ┃ Outputs ┃\n┡━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n│ case_0   │ 0      │ 0       │\n├──────────┼────────┼─────────┤\n│ case_1   │ 1      │ 2       │\n├──────────┼────────┼─────────┤\n│ case_2   │ 2      │ 4       │\n├──────────┼────────┼─────────┤\n│ case_3   │ 3      │ 6       │\n├──────────┼────────┼─────────┤\n│ case_4   │ 4      │ 8       │\n├──────────┼────────┼─────────┤\n│ Averages │        │         │\n└──────────┴────────┴─────────┘\n\"\"\"",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Run evaluation with unlimited concurrency"
  },
  "1bf9f0151f927dc283603c6db7cbc77a7df4adee624604e9ef3204b0803c04d9": {
    "text": "t0 = time.time()\nreport_limited = dataset.evaluate_sync(double_number, max_concurrency=1)\nprint(f'Evaluation took more than 0.5s: {time.time() - t0 > 0.5}')\n#> Evaluation took more than 0.5s: True\n\nreport_limited.print(include_input=True, include_output=True, include_durations=False)  # (2)!\n\"\"\"\n      Evaluation Summary:\n         double_number\n┏━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n┃ Case ID  ┃ Inputs ┃ Outputs ┃\n┡━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n│ case_0   │ 0      │ 0       │\n├──────────┼────────┼─────────┤\n│ case_1   │ 1      │ 2       │\n├──────────┼────────┼─────────┤\n│ case_2   │ 2      │ 4       │\n├──────────┼────────┼─────────┤\n│ case_3   │ 3      │ 6       │\n├──────────┼────────┼─────────┤\n│ case_4   │ 4      │ 8       │\n├──────────┼────────┼─────────┤\n│ Averages │        │         │\n└──────────┴────────┴─────────┘\n\"\"\"\n```\n\n1. We have omitted duration here just to keep the printed output from changing from run to run.\n2. We have omitted duration here just to keep the printed output from changing from run to run.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Run evaluation with limited concurrency"
  },
  "ad2b0567fe66698796dc74402e08d7298d5812c645b785fc6e60b44233480601": {
    "text": "Pydantic Evals integrates with OpenTelemetry for tracing.\n\nThe [`EvaluatorContext`](../api/pydantic_evals/evaluators/#pydantic_evals.evaluators.EvaluatorContext \"../api/pydantic_evals/evaluators/#pydantic_evals.evaluators.EvaluatorContext\") includes a property called `span_tree`\nwhich returns a [`SpanTree`](../api/pydantic_evals/otel/#pydantic_evals.otel.SpanTree \"../api/pydantic_evals/otel/#pydantic_evals.otel.SpanTree\"). The `SpanTree` provides a way to query and analyze\nthe spans generated during function execution. This provides a way to access the results of instrumentation during\nevaluation.\n\nNote\n\nIf you just want to write unit tests that ensure that specific spans are produced during calls to your evaluation\ntask, it's usually better to just use the `logfire.testing.capfire` fixture directly.\n\nThere are two main ways this is useful.\n\nopentelemetry\\_example.py\n\n```\nimport asyncio\nfrom typing import Any\n\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator\nfrom pydantic_evals.evaluators.context import EvaluatorContext\nfrom pydantic_evals.otel.span_tree import SpanQuery\n\nlogfire.configure(  # ensure that an OpenTelemetry tracer is configured\n    send_to_logfire='if-token-present'\n)\n\n\nclass SpanTracingEvaluator(Evaluator[str, str]):\n    \"\"\"Evaluator that analyzes the span tree generated during function execution.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> dict[str, Any]:\n        # Get the span tree from the context\n        span_tree = ctx.span_tree\n        if span_tree is None:\n            return {'has_spans': False, 'performance_score': 0.0}\n\n        # Find all spans with \"processing\" in the name\n        processing_spans = span_tree.find(lambda node: 'processing' in node.name)\n\n        # Calculate total processing time\n        total_processing_time = sum(\n            (span.duration.total_seconds() for span in processing_spans), 0.0\n        )\n\n        # Check for error spans\n        error_query: SpanQuery = {'name_contains': 'error'}\n        has_errors = span_tree.any(error_query)\n\n        # Calculate a performance score (lower is better)\n        performance_score = 1.0 if total_processing_time < 1.0 else 0.5\n\n        return {\n            'has_spans': True,\n            'has_errors': has_errors,\n            'performance_score': 0 if has_errors else performance_score,\n        }\n\n\nasync def process_text(text: str) -> str:\n    \"\"\"Function that processes text with OpenTelemetry instrumentation.\"\"\"\n    with logfire.span('process_text'):\n        # Simulate initial processing\n        with logfire.span('text_processing'):\n            await asyncio.sleep(0.1)\n            processed = text.strip().lower()\n\n        # Simulate additional processing\n        with logfire.span('additional_processing'):\n            if 'error' in processed:\n                with logfire.span('error_handling'):\n                    logfire.error(f'Error detected in text: {text}')\n                    return f'Error processing: {text}'\n            await asyncio.sleep(0.2)\n            processed = processed.replace(' ', '_')\n\n        return f'Processed: {processed}'",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "OpenTelemetry Integration"
  },
  "add6f1da498cfe56e8290785b740b96a36e47faa6f190b66dcddb6d245872867": {
    "text": "dataset = Dataset(\n    cases=[\n        Case(\n            name='normal_text',\n            inputs='Hello World',\n            expected_output='Processed: hello_world',\n        ),\n        Case(\n            name='text_with_error',\n            inputs='Contains error marker',\n            expected_output='Error processing: Contains error marker',\n        ),\n    ],\n    evaluators=[SpanTracingEvaluator()],\n)",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Create test cases"
  },
  "661e0476a3b7ce1b1d3280cf083b95fc693c723ca88c6066bf0ed4600ce8bed6": {
    "text": "report = dataset.evaluate_sync(process_text)",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Run evaluation - spans are automatically captured since logfire is configured"
  },
  "88946b4e016c85e077544ef693ef71bf933a9c6d7ea6653c5aa999b2446fb864": {
    "text": "report.print(include_input=True, include_output=True, include_durations=False)  # (1)!\n\"\"\"\n                                              Evaluation Summary: process_text\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Case ID         ┃ Inputs                ┃ Outputs                                 ┃ Scores                   ┃ Assertions ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ normal_text     │ Hello World           │ Processed: hello_world                  │ performance_score: 1.00  │ ✔✗         │\n├─────────────────┼───────────────────────┼─────────────────────────────────────────┼──────────────────────────┼────────────┤\n│ text_with_error │ Contains error marker │ Error processing: Contains error marker │ performance_score: 0     │ ✔✔         │\n├─────────────────┼───────────────────────┼─────────────────────────────────────────┼──────────────────────────┼────────────┤\n│ Averages        │                       │                                         │ performance_score: 0.500 │ 75.0% ✔    │\n└─────────────────┴───────────────────────┴─────────────────────────────────────────┴──────────────────────────┴────────────┘\n\"\"\"\n```\n\n1. We have omitted duration here just to keep the printed output from changing from run to run.\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Print the report"
  },
  "cfd864e30cf0ebaef7ed1f45f8ab742febf94c534c375792f279fbf700b9ba61": {
    "text": "Pydantic Evals allows you to generate test datasets using LLMs with [`generate_dataset`](../api/pydantic_evals/generation/#pydantic_evals.generation.generate_dataset \"../api/pydantic_evals/generation/#pydantic_evals.generation.generate_dataset\").\n\nDatasets can be generated in either JSON or YAML format, in both cases a JSON schema file is generated alongside the dataset and referenced in the dataset, so you should get type checking and auto-completion in your editor.\n\ngenerate\\_dataset\\_example.py\n\n```\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\n\nclass QuestionInputs(BaseModel, use_attribute_docstrings=True):  # (1)!\n    \"\"\"Model for question inputs.\"\"\"\n\n    question: str\n    \"\"\"A question to answer\"\"\"\n    context: str | None = None\n    \"\"\"Optional context for the question\"\"\"\n\n\nclass AnswerOutput(BaseModel, use_attribute_docstrings=True):  # (2)!\n    \"\"\"Model for expected answer outputs.\"\"\"\n\n    answer: str\n    \"\"\"The answer to the question\"\"\"\n    confidence: float = Field(ge=0, le=1)\n    \"\"\"Confidence level (0-1)\"\"\"\n\n\nclass MetadataType(BaseModel, use_attribute_docstrings=True):  # (3)!\n    \"\"\"Metadata model for test cases.\"\"\"\n\n    difficulty: str\n    \"\"\"Difficulty level (easy, medium, hard)\"\"\"\n    category: str\n    \"\"\"Question category\"\"\"\n\n\nasync def main():\n    dataset = await generate_dataset(  # (4)!\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.yaml')\n    dataset.to_file(output_file)  # (5)!\n    print(output_file.read_text())\n    \"\"\"\n    # yaml-language-server: $schema=questions_cases_schema.json\n    cases:\n    - name: Easy Capital Question\n      inputs:\n        question: What is the capital of France?\n      metadata:\n        difficulty: easy\n        category: Geography\n      expected_output:\n        answer: Paris\n        confidence: 0.95\n      evaluators:\n      - EqualsExpected\n    - name: Challenging Landmark Question\n      inputs:\n        question: Which world-famous landmark is located on the banks of the Seine River?\n      metadata:\n        difficulty: hard\n        category: Landmarks\n      expected_output:\n        answer: Eiffel Tower\n        confidence: 0.9\n      evaluators:\n      - EqualsExpected\n    \"\"\"\n```\n\n1. Define the schema for the inputs to the task.\n2. Define the schema for the expected outputs of the task.\n3. Define the schema for the metadata of the test cases.\n4. Call [`generate_dataset`](../api/pydantic_evals/generation/#pydantic_evals.generation.generate_dataset \"../api/pydantic_evals/generation/#pydantic_evals.generation.generate_dataset\") to create a [`Dataset`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset\") with 2 cases confirming to the schema.\n5. Save the dataset to a YAML file, this will also write `questions_cases_schema.json` with the schema JSON schema for `questions_cases.yaml` to make editing easier. The magic `yaml-language-server` comment is supported by at least vscode, jetbrains/pycharm (more details [here](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema \"https://github.com/redhat-developer/yaml-language-server#using-inlined-schema\")).\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main(answer))` to run `main`)*\n\nYou can also write datasets as JSON files:\n\ngenerate\\_dataset\\_example\\_json.py\n\n```\nfrom pathlib import Path\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\nfrom generate_dataset_example import AnswerOutput, MetadataType, QuestionInputs\n\n\nasync def main():\n    dataset = await generate_dataset(  # (1)!\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.json')\n    dataset.to_file(output_file)  # (2)!\n    print(output_file.read_text())\n    \"\"\"\n    {\n      \"$schema\": \"questions_cases_schema.json\",\n      \"cases\": [\n        {\n          \"name\": \"Easy Capital Question\",\n          \"inputs\": {\n            \"question\": \"What is the capital of France?\"\n          },\n          \"metadata\": {\n            \"difficulty\": \"easy\",\n            \"category\": \"Geography\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Paris\",\n            \"confidence\": 0.95\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        },\n        {\n          \"name\": \"Challenging Landmark Question\",\n          \"inputs\": {\n            \"question\": \"Which world-famous landmark is located on the banks of the Seine River?\"\n          },\n          \"metadata\": {\n            \"difficulty\": \"hard\",\n            \"category\": \"Landmarks\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Eiffel Tower\",\n            \"confidence\": 0.9\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        }\n      ]\n    }\n    \"\"\"\n```\n\n1. Generate the [`Dataset`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset\") exactly as above.\n2. Save the dataset to a JSON file, this will also write `questions_cases_schema.json` with th JSON schema for `questions_cases.json`. This time the `$schema` key is included in the JSON file to define the schema for IDEs to use while you edit the file, there's no formal spec for this, but it works in vscode and pycharm and is discussed at length in [json-schema-org/json-schema-spec#828](https://github.com/json-schema-org/json-schema-spec/issues/828 \"https://github.com/json-schema-org/json-schema-spec/issues/828\").\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main(answer))` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Generating Test Datasets"
  },
  "dc98687bc4ee9d7961a586bbeff2c71544a4f006b00ccd4ccfd7f4bab0482b19": {
    "text": "Pydantic Evals is implemented using OpenTelemetry to record traces of the evaluation process. These traces contain all\nthe information included in the terminal output as attributes, but also include full tracing from the executions of the\nevaluation task function.\n\nYou can send these traces to any OpenTelemetry-compatible backend, including [Pydantic Logfire](https://logfire.pydantic.dev/docs \"https://logfire.pydantic.dev/docs\").\n\nAll you need to do is configure Logfire via `logfire.configure`:\n\nlogfire\\_integration.py\n\n```\nimport logfire\nfrom judge_recipes import recipe_dataset, transform_recipe\n\nlogfire.configure(\n    send_to_logfire='if-token-present',  # (1)!\n    environment='development',  # (2)!\n    service_name='evals',  # (3)!\n)\n\nrecipe_dataset.evaluate_sync(transform_recipe)\n```\n\n1. The `send_to_logfire` argument controls when traces are sent to Logfire. You can set it to `'if-token-present'` to send data to Logfire only if the `LOGFIRE_TOKEN` environment variable is set. See the [Logfire configuration docs](https://logfire.pydantic.dev/docs/reference/configuration/ \"https://logfire.pydantic.dev/docs/reference/configuration/\") for more details.\n2. The `environment` argument sets the environment for the traces. It's a good idea to set this to `'development'` when running tests or evaluations and sending data to a project with production data, to make it easier to filter these traces out while reviewing data from your production environment(s).\n3. The `service_name` argument sets the service name for the traces. This is displayed in the Logfire UI to help you identify the source of the associated spans.\n\nLogfire has some special integration with Pydantic Evals traces, including a table view of the evaluation results\non the evaluation root span (which is generated in each call to [`Dataset.evaluate`](../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluate \"../api/pydantic_evals/dataset/#pydantic_evals.dataset.Dataset.evaluate\")):\n\nand a detailed view of the inputs and outputs for the execution of each case:\n\nIn addition, any OpenTelemetry spans generated during the evaluation process will be sent to Logfire, allowing you to\nvisualize the full execution of the code called during the evaluation process:\n\nThis can be especially helpful when attempting to write evaluators that make use of the `span_tree` property of the\n[`EvaluatorContext`](../api/pydantic_evals/evaluators/#pydantic_evals.evaluators.EvaluatorContext \"../api/pydantic_evals/evaluators/#pydantic_evals.evaluators.EvaluatorContext\"), as described in the\n[OpenTelemetry Integration](#opentelemetry-integration \"#opentelemetry-integration\") section above.\n\nThis allows you to write evaluations that depend on information about which code paths were executed during the call to\nthe task function without needing to manually instrument the code being evaluated, as long as the code being evaluated\nis already adequately instrumented with OpenTelemetry. In the case of Pydantic AI agents, for example, this can be used\nto ensure specific tools are (or are not) called during the execution of specific cases.\n\nUsing OpenTelemetry in this way also means that all data used to evaluate the task executions will be accessible in\nthe traces produced by production runs of the code, making it straightforward to perform the same evaluations on\nproduction data.",
    "source_url": "https://ai.pydantic.dev/evals/",
    "header": "Integration with Logfire"
  },
  "0b7f231a7def5d9863a3fdf942da87d3bdb301aaf80d3dcd350a147deac6605b": {
    "text": "Some LLMs are now capable of understanding audio, video, image and document content.",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "Image, Audio, Video & Document Input"
  },
  "b570b082477ed95a9293a0f5b2260794a1b880a95cc82e11257c8abfadd61d72": {
    "text": "Info\n\nSome models do not support image input. Please check the model's documentation to confirm whether it supports image input.\n\nIf you have a direct URL for the image, you can use [`ImageUrl`](../api/messages/#pydantic_ai.messages.ImageUrl \"../api/messages/#pydantic_ai.messages.ImageUrl\"):\n\nimage\\_input.py\n\n```\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='openai:gpt-4o')\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        ImageUrl(url='https://iili.io/3Hs4FMg.png'),\n    ]\n)\nprint(result.output)",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "Image Input"
  },
  "7f9cf17e7b488b15a7df75fbb4d2ba35eceed9c4349f74b0434b2fbf8355be73": {
    "text": "```\n\nIf you have the image locally, you can also use [`BinaryContent`](../api/messages/#pydantic_ai.messages.BinaryContent \"../api/messages/#pydantic_ai.messages.BinaryContent\"):\n\nlocal\\_image\\_input.py\n\n```\nimport httpx\n\nfrom pydantic_ai import Agent, BinaryContent\n\nimage_response = httpx.get('https://iili.io/3Hs4FMg.png')  # Pydantic logo\n\nagent = Agent(model='openai:gpt-4o')\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        BinaryContent(data=image_response.content, media_type='image/png'),  # (1)!\n    ]\n)\nprint(result.output)",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "> This is the logo for Pydantic, a data validation and settings management library in Python."
  },
  "5ee6ac90ffe3a1fbe8599ac0b2213b7f383be41aa3dd515ad2b64160cf832237": {
    "text": "```\n\n1. To ensure the example is runnable we download this image from the web, but you can also use `Path().read_bytes()` to read a local file's contents.",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "> This is the logo for Pydantic, a data validation and settings management library in Python."
  },
  "a217247bccd51ab1eea72252b202006d7ffabb36af4739ef34e102514881629f": {
    "text": "Info\n\nSome models do not support audio input. Please check the model's documentation to confirm whether it supports audio input.\n\nYou can provide audio input using either [`AudioUrl`](../api/messages/#pydantic_ai.messages.AudioUrl \"../api/messages/#pydantic_ai.messages.AudioUrl\") or [`BinaryContent`](../api/messages/#pydantic_ai.messages.BinaryContent \"../api/messages/#pydantic_ai.messages.BinaryContent\"). The process is analogous to the examples above.",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "Audio Input"
  },
  "d376edb01ce2c53188335b7ac4d548285193f3748ac460cc2af4c09f60576983": {
    "text": "Info\n\nSome models do not support video input. Please check the model's documentation to confirm whether it supports video input.\n\nYou can provide video input using either [`VideoUrl`](../api/messages/#pydantic_ai.messages.VideoUrl \"../api/messages/#pydantic_ai.messages.VideoUrl\") or [`BinaryContent`](../api/messages/#pydantic_ai.messages.BinaryContent \"../api/messages/#pydantic_ai.messages.BinaryContent\"). The process is analogous to the examples above.",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "Video Input"
  },
  "f00561d175c3a9abbbd12a7ee8469ee4f88d324ae6c840fd9a79bdf90f9d8eb9": {
    "text": "Info\n\nSome models do not support document input. Please check the model's documentation to confirm whether it supports document input.\n\nYou can provide document input using either [`DocumentUrl`](../api/messages/#pydantic_ai.messages.DocumentUrl \"../api/messages/#pydantic_ai.messages.DocumentUrl\") or [`BinaryContent`](../api/messages/#pydantic_ai.messages.BinaryContent \"../api/messages/#pydantic_ai.messages.BinaryContent\"). The process is similar to the examples above.\n\nIf you have a direct URL for the document, you can use [`DocumentUrl`](../api/messages/#pydantic_ai.messages.DocumentUrl \"../api/messages/#pydantic_ai.messages.DocumentUrl\"):\n\ndocument\\_input.py\n\n```\nfrom pydantic_ai import Agent, DocumentUrl\n\nagent = Agent(model='anthropic:claude-3-sonnet')\nresult = agent.run_sync(\n    [\n        'What is the main content of this document?',\n        DocumentUrl(url='https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf'),\n    ]\n)\nprint(result.output)",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "Document Input"
  },
  "4baa1eb89a43d8b0a649d89fd0733caeaaf23cee65b58c2c0511438dbf619ab1": {
    "text": "```\n\nThe supported document formats vary by model.\n\nYou can also use [`BinaryContent`](../api/messages/#pydantic_ai.messages.BinaryContent \"../api/messages/#pydantic_ai.messages.BinaryContent\") to pass document data directly:\n\nbinary\\_content\\_input.py\n\n```\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\n\npdf_path = Path('document.pdf')\nagent = Agent(model='anthropic:claude-3-sonnet')\nresult = agent.run_sync(\n    [\n        'What is the main content of this document?',\n        BinaryContent(data=pdf_path.read_bytes(), media_type='application/pdf'),\n    ]\n)\nprint(result.output)",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "> This document is the technical report introducing Gemini 1.5, Google's latest large language model..."
  },
  "9c59ccd39c2f5365ea0d31155ddd42c5b5292d7583aaad6508988d1940c34941": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "> The document discusses..."
  },
  "374fb98be3a99456b005965d28adb5923f0573ae4c97e30515bb872f2a1393a1": {
    "text": "As a general rule, when you provide a URL using any of `ImageUrl`, `AudioUrl`, `VideoUrl` or `DocumentUrl`, Pydantic AI downloads the file content and then sends it as part of the API request.\n\nThe situation is different for certain models:\n\n* [`AnthropicModel`](../api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModel \"../api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModel\"): if you provide a PDF document via `DocumentUrl`, the URL is sent directly in the API request, so no download happens on the user side.\n* [`GoogleModel`](../api/models/google/#pydantic_ai.models.google.GoogleModel \"../api/models/google/#pydantic_ai.models.google.GoogleModel\") on Vertex AI: any URL provided using `ImageUrl`, `AudioUrl`, `VideoUrl`, or `DocumentUrl` is sent as-is in the API request and no data is downloaded beforehand.\n\nSee the [Gemini API docs for Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#filedata \"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#filedata\") to learn more about supported URLs, formats and limitations:\n\n* Cloud Storage bucket URIs (with protocol `gs://`)\n* Public HTTP(S) URLs\n* Public YouTube video URL (maximum one URL per request)\n\nHowever, because of crawling restrictions, it may happen that Gemini can't access certain URLs. In that case, you can instruct Pydantic AI to download the file content and send that instead of the URL by setting the boolean flag `force_download` to `True`. This attribute is available on all objects that inherit from [`FileUrl`](../api/messages/#pydantic_ai.messages.FileUrl \"../api/messages/#pydantic_ai.messages.FileUrl\").\n\n* [`GoogleModel`](../api/models/google/#pydantic_ai.models.google.GoogleModel \"../api/models/google/#pydantic_ai.models.google.GoogleModel\") on GLA: YouTube video URLs are sent directly in the request to the model.",
    "source_url": "https://ai.pydantic.dev/input/",
    "header": "User-side download vs. direct file URL"
  },
  "366421bacc2dfd5f237f653cd6257e9719f5d46eda116f58d582224c47ecf20c": {
    "text": "Thinking (or reasoning) is the process by which a model works through a problem step-by-step before\nproviding its final answer.\n\nThis capability is typically disabled by default and depends on the specific model being used.\nSee the sections below for how to enable thinking for each provider.\n\nInternally, if the model doesn't provide thinking objects, Pydantic AI will convert thinking blocks\n(`\"<think>...\"</think>\"`) in provider-specific text parts to `ThinkingPart`s. We have also made\nthe decision not to send `ThinkingPart`s back to the provider in multi-turn conversations -\nthis helps save costs for users. In the future, we plan to add a setting to customize this behavior.",
    "source_url": "https://ai.pydantic.dev/thinking/",
    "header": "Thinking"
  },
  "acb981e4d9b5a37d98f464bf0b1d8213da986ee56fcd915f430a267f2b8091a1": {
    "text": "When using the [`OpenAIChatModel`](../api/models/openai/#pydantic_ai.models.openai.OpenAIChatModel \"../api/models/openai/#pydantic_ai.models.openai.OpenAIChatModel\"), thinking objects are not created\nby default. However, the text content may contain `\"<think>\"` tags. When this happens, Pydantic AI will\nconvert them to [`ThinkingPart`](../api/messages/#pydantic_ai.messages.ThinkingPart \"../api/messages/#pydantic_ai.messages.ThinkingPart\") objects.\n\nIn contrast, the [`OpenAIResponsesModel`](../api/models/openai/#pydantic_ai.models.openai.OpenAIResponsesModel \"../api/models/openai/#pydantic_ai.models.openai.OpenAIResponsesModel\") does\ngenerate thinking parts. To enable this functionality, you need to set the `openai_reasoning_effort` and\n`openai_reasoning_summary` fields in the\n[`OpenAIResponsesModelSettings`](../api/models/openai/#pydantic_ai.models.openai.OpenAIResponsesModelSettings \"../api/models/openai/#pydantic_ai.models.openai.OpenAIResponsesModelSettings\").\n\nopenai\\_thinking\\_part.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('o3-mini')\nsettings = OpenAIResponsesModelSettings(\n    openai_reasoning_effort='low',\n    openai_reasoning_summary='detailed',\n)\nagent = Agent(model, model_settings=settings)\n...\n```",
    "source_url": "https://ai.pydantic.dev/thinking/",
    "header": "OpenAI"
  },
  "fc0d46f7047f37769913f4b254be2911e1171e8377ab6b34c2c4f011d5124ef1": {
    "text": "Unlike other providers, Anthropic includes a signature in the thinking part. This signature is used to\nensure that the thinking part has not been tampered with. To enable thinking, use the `anthropic_thinking`\nfield in the [`AnthropicModelSettings`](../api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModelSettings \"../api/models/anthropic/#pydantic_ai.models.anthropic.AnthropicModelSettings\").\n\nanthropic\\_thinking\\_part.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings\n\nmodel = AnthropicModel('claude-3-7-sonnet-latest')\nsettings = AnthropicModelSettings(\n    anthropic_thinking={'type': 'enabled', 'budget_tokens': 1024},\n)\nagent = Agent(model, model_settings=settings)\n...\n```",
    "source_url": "https://ai.pydantic.dev/thinking/",
    "header": "Anthropic"
  },
  "999bd147fa289e4d7d5944bd82374d4275cf69399de0b75caf723777513cc48c": {
    "text": "Groq supports different formats to receive thinking parts:\n\n* `\"raw\"`: The thinking part is included in the text content with the `\"<think>\"` tag.\n* `\"hidden\"`: The thinking part is not included in the text content.\n* `\"parsed\"`: The thinking part has its own [`ThinkingPart`](../api/messages/#pydantic_ai.messages.ThinkingPart \"../api/messages/#pydantic_ai.messages.ThinkingPart\") object.\n\nTo enable thinking, use the `groq_reasoning_format` field in the\n[`GroqModelSettings`](../api/models/groq/#pydantic_ai.models.groq.GroqModelSettings \"../api/models/groq/#pydantic_ai.models.groq.GroqModelSettings\"):\n\ngroq\\_thinking\\_part.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel, GroqModelSettings\n\nmodel = GroqModel('qwen-qwq-32b')\nsettings = GroqModelSettings(groq_reasoning_format='parsed')\nagent = Agent(model, model_settings=settings)\n...\n```",
    "source_url": "https://ai.pydantic.dev/thinking/",
    "header": "Groq"
  },
  "69f0e009e46348229e7f98c8e02d239c8402c2878d36a8288f22985c1ed33031": {
    "text": "To enable thinking, use the `google_thinking_config` field in the\n[`GoogleModelSettings`](../api/models/google/#pydantic_ai.models.google.GoogleModelSettings \"../api/models/google/#pydantic_ai.models.google.GoogleModelSettings\").\n\ngoogle\\_thinking\\_part.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel = GoogleModel('gemini-2.5-pro-preview-03-25')\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\nagent = Agent(model, model_settings=settings)\n...\n```",
    "source_url": "https://ai.pydantic.dev/thinking/",
    "header": "Google"
  },
  "49f7a6ae7e404f4b6fbb1de0d24a069eeccad32d692d4cd424f4250b40a08e88": {
    "text": "Neither Mistral nor Cohere generate thinking parts.",
    "source_url": "https://ai.pydantic.dev/thinking/",
    "header": "Mistral / Cohere"
  },
  "241cbc502bdfe9957b0bbac5a03b127d632ca90dbc5491b48a21d29da54bc7e6": {
    "text": "The `direct` module provides low-level methods for making imperative requests to LLMs where the only abstraction is input and output schema translation, enabling you to use all models with the same API.\n\nThese methods are thin wrappers around the [`Model`](../api/models/base/#pydantic_ai.models.Model \"../api/models/base/#pydantic_ai.models.Model\") implementations, offering a simpler interface when you don't need the full functionality of an [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\").\n\nThe following functions are available:",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "Direct Model Requests"
  },
  "d8208f008c2d7cdb5dc41cfe5a17e76df92f69d7077070878c9c70ac149dc349": {
    "text": "Here's a simple example demonstrating how to use the direct API to make a basic request:\n\ndirect\\_basic.py\n\n```\nfrom pydantic_ai.direct import model_request_sync\nfrom pydantic_ai.messages import ModelRequest",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "Basic Example"
  },
  "8c4834fdd14dd4aada98b24c2f848309ca1704aa6b8993334bac4da52560a628": {
    "text": "model_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\nprint(model_response.usage)\n#> RequestUsage(input_tokens=56, output_tokens=7)\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can also use the direct API to work with function/tool calling.\n\nEven here we can use Pydantic to generate the JSON schema for the tool:\n\n```\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import ToolDefinition\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.messages import ModelRequest\nfrom pydantic_ai.models import ModelRequestParameters\n\n\nclass Divide(BaseModel):\n    \"\"\"Divide two numbers.\"\"\"\n\n    numerator: float\n    denominator: float\n    on_inf: Literal['error', 'infinity'] = 'infinity'\n\n\nasync def main():\n    # Make a request to the model with tool access\n    model_response = await model_request(\n        'openai:gpt-4.1-nano',\n        [ModelRequest.user_text_prompt('What is 123 / 456?')],\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[\n                ToolDefinition(\n                    name=Divide.__name__.lower(),\n                    description=Divide.__doc__,\n                    parameters_json_schema=Divide.model_json_schema(),\n                )\n            ],\n            allow_text_output=True,  # Allow model to either use tools or respond directly\n        ),\n    )\n    print(model_response)\n    \"\"\"\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='divide',\n                args={'numerator': '123', 'denominator': '456'},\n                tool_call_id='pyd_ai_2e0e396768a14fe482df90a29a78dc7b',\n            )\n        ],\n        usage=RequestUsage(input_tokens=55, output_tokens=7),\n        model_name='gpt-4.1-nano',\n        timestamp=datetime.datetime(...),\n    )\n    \"\"\"\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "Make a synchronous request to the model"
  },
  "5070cf7cc8ace622055ad8ef3cd7d4b16b74cf9a1a0ea1db201e377103711f46": {
    "text": "The direct API is ideal when:\n\n1. You need more direct control over model interactions\n2. You want to implement custom behavior around model requests\n3. You're building your own abstractions on top of model interactions\n\nFor most application use cases, the higher-level [`Agent`](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\") API provides a more convenient interface with additional features such as built-in tool execution, retrying, structured output parsing, and more.",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "When to Use the direct API vs Agent"
  },
  "7711452c28c4e333feda676c60e99f3ade098a4bc7bfcd4ea1751f017773f099": {
    "text": "As with [agents](../api/agent/#pydantic_ai.agent.Agent \"../api/agent/#pydantic_ai.agent.Agent\"), you can enable OpenTelemetry/Logfire instrumentation with just a few extra lines\n\ndirect\\_instrumented.py\n\n```\nimport logfire\n\nfrom pydantic_ai.direct import model_request_sync\nfrom pydantic_ai.messages import ModelRequest\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "OpenTelemetry or Logfire Instrumentation"
  },
  "72a4d6591461984b567087dfa581b2b43ac6af28bd7feb734a6e149c44486897": {
    "text": "model_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can also enable OpenTelemetry on a per call basis:\n\ndirect\\_instrumented.py\n\n```\nimport logfire\n\nfrom pydantic_ai.direct import model_request_sync\nfrom pydantic_ai.messages import ModelRequest\n\nlogfire.configure()",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "Make a synchronous request to the model"
  },
  "22c68a5e667b5bbc086fda41ef0e9612ad52e3efe64316396965e8b66377fea0": {
    "text": "model_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n    instrument=True\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\n```\n\nSee [Debugging and Monitoring](../logfire/ \"../logfire/\") for more details, including how to instrument with plain OpenTelemetry without Logfire.",
    "source_url": "https://ai.pydantic.dev/direct/",
    "header": "Make a synchronous request to the model"
  },
  "e53a78e71ecd40a2f2c1e67406cb814e1e306073202d3b8eeac2e46c1fa91e6c": {
    "text": "Builtin tools are native tools provided by LLM providers that can be used to enhance your agent's capabilities. Unlike [common tools](../common-tools/ \"../common-tools/\"), which are custom implementations that PydanticAI executes, builtin tools are executed directly by the model provider.",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Introduction"
  },
  "aa94c94d90b67129eac3e6336455ebba18cd79fd170d527fbd8fbeb19e9575c1": {
    "text": "PydanticAI supports the following builtin tools:\n\nThese tools are passed to the agent via the `builtin_tools` parameter and are executed by the model provider's infrastructure.\n\nProvider Support\n\nNot all model providers support builtin tools. If you use a builtin tool with an unsupported provider, PydanticAI will raise a [`UserError`](../api/exceptions/#pydantic_ai.exceptions.UserError \"../api/exceptions/#pydantic_ai.exceptions.UserError\") when you try to run the agent.\n\nThe [`WebSearchTool`](../api/builtin_tools/#pydantic_ai.builtin_tools.WebSearchTool \"../api/builtin_tools/#pydantic_ai.builtin_tools.WebSearchTool\") allows your agent to search the web,\nmaking it ideal for queries that require up-to-date data.",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Overview"
  },
  "c29f18a96f589c90e82657c30b587de54c369f120cfec5d47bdb19d3ded0694f": {
    "text": "| Provider | Supported | Notes |\n| --- | --- | --- |\n| OpenAI | ✅ | Full feature support |\n| Anthropic | ✅ | Full feature support |\n| Groq | ✅ | Limited parameter support |\n| Google | ✅ | No parameter support |\n| Bedrock | ❌ | Not supported |\n| Mistral | ❌ | Not supported |\n| Cohere | ❌ | Not supported |\n| HuggingFace | ❌ | Not supported |\n\nGroq Support\n\nTo use web search capabilities with Groq, you need to use the [compound models](https://console.groq.com/docs/compound \"https://console.groq.com/docs/compound\").",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Provider Support"
  },
  "c8536d0c61687bdec3ce1444c52983865fa15e6a91588408fff62efc62ebe118": {
    "text": "web\\_search\\_basic.py\n\n```\nfrom pydantic_ai import Agent, WebSearchTool\n\nagent = Agent('anthropic:claude-sonnet-4-0', builtin_tools=[WebSearchTool()])\n\nresult = agent.run_sync('Give me a sentence with the biggest news in AI this week.')",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Usage"
  },
  "da10a62b7367a39f20c8af8bf4d887b8a96c305b243f0dba4b336e2282f5b384": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "> Scientists have developed a universal AI detector that can identify deepfake videos."
  },
  "e8149d03efdf88d6d4a4767e947d6a3ed939f2731db56b8a3f2f815e52ea374b": {
    "text": "The `WebSearchTool` supports several configuration parameters:\n\nweb\\_search\\_configured.py\n\n```\nfrom pydantic_ai import Agent, WebSearchTool, WebSearchUserLocation\n\nagent = Agent(\n    'anthropic:claude-sonnet-4-0',\n    builtin_tools=[\n        WebSearchTool(\n            search_context_size='high',\n            user_location=WebSearchUserLocation(\n                city='San Francisco',\n                country='US',\n                region='CA',\n                timezone='America/Los_Angeles',\n            ),\n            blocked_domains=['example.com', 'spam-site.net'],\n            allowed_domains=None,  # Cannot use both blocked_domains and allowed_domains with Anthropic\n            max_uses=5,  # Anthropic only: limit tool usage\n        )\n    ],\n)\n\nresult = agent.run_sync('Use the web to get the current time.')",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Configuration Options"
  },
  "9a8b8cb8da6d25dfb72babc3c2b3c8c94ac03df5f127f82e5e62a02f95aeed69": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "> In San Francisco, it's 8:21:41 pm PDT on Wednesday, August 6, 2025."
  },
  "b983562f31a3c3a09917815427c4d600b47293321ff39f6d159eab437698e431": {
    "text": "| Parameter | OpenAI | Anthropic | Groq |\n| --- | --- | --- | --- |\n| `search_context_size` | ✅ | ❌ | ❌ |\n| `user_location` | ✅ | ✅ | ❌ |\n| `blocked_domains` | ❌ | ✅ | ✅ |\n| `allowed_domains` | ❌ | ✅ | ✅ |\n| `max_uses` | ❌ | ✅ | ❌ |\n\nAnthropic Domain Filtering\n\nWith Anthropic, you can only use either `blocked_domains` or `allowed_domains`, not both.\n\nThe [`CodeExecutionTool`](../api/builtin_tools/#pydantic_ai.builtin_tools.CodeExecutionTool \"../api/builtin_tools/#pydantic_ai.builtin_tools.CodeExecutionTool\") enables your agent to execute code\nin a secure environment, making it perfect for computational tasks, data analysis, and mathematical operations.",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Parameter Support by Provider"
  },
  "b8488bd9721a49e2758bb6260dbd5e58ca6f3208e469bb2e35302723d0338bea": {
    "text": "| Provider | Supported |\n| --- | --- |\n| OpenAI | ✅ |\n| Anthropic | ✅ |\n| Google | ✅ |\n| Groq | ❌ |\n| Bedrock | ❌ |\n| Mistral | ❌ |\n| Cohere | ❌ |\n| HuggingFace | ❌ |",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Provider Support"
  },
  "da77bfe8778a8ca4b01102b207afce249f58923a709dc13bcee93f14a3408bc2": {
    "text": "code\\_execution\\_basic.py\n\n```\nfrom pydantic_ai import Agent, CodeExecutionTool\n\nagent = Agent('anthropic:claude-sonnet-4-0', builtin_tools=[CodeExecutionTool()])\n\nresult = agent.run_sync('Calculate the factorial of 15 and show your work')",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Usage"
  },
  "942c32bec46e4db70e50d0d5e215b12bd606fd33fc9e34452c1c22ebc6c06b71": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "> The factorial of 15 is **1,307,674,368,000**."
  },
  "938bfadbdbdf62bce4cbc0b768bb2d6f41863d503322bb6bfde1ab08d6572ce4": {
    "text": "The [`UrlContextTool`](../api/builtin_tools/#pydantic_ai.builtin_tools.UrlContextTool \"../api/builtin_tools/#pydantic_ai.builtin_tools.UrlContextTool\") enables your agent to pull URL contents into its context,\nallowing it to pull up-to-date information from the web.",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "URL Context Tool"
  },
  "75ce604c35cec7917493a4e5cbba15aa71fa32bb9fb27e9dd02d6ff966caccd0": {
    "text": "| Provider | Supported |\n| --- | --- |\n| Google | ✅ |\n| OpenAI | ❌ |\n| Anthropic | ❌ |\n| Groq | ❌ |\n| Bedrock | ❌ |\n| Mistral | ❌ |\n| Cohere | ❌ |\n| HuggingFace | ❌ |",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Provider Support"
  },
  "355d823d558fc38ffb29a17771136134b35f3398116b478c1d5aa6ce813566b5": {
    "text": "url\\_context\\_basic.py\n\n```\nfrom pydantic_ai import Agent, UrlContextTool\n\nagent = Agent('google-gla:gemini-2.5-flash', builtin_tools=[UrlContextTool()])\n\nresult = agent.run_sync('What is this? https://ai.pydantic.dev')",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "Usage"
  },
  "46a08bafbb7e9c9d039dc8f5481c2050f0143f6317b681194acd07681dfc4951": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "> A Python agent framework for building Generative AI applications."
  },
  "36df594782696e85964c44ff816649e6aa9dbd5d05b09892bd6fcc1da84f9a1a": {
    "text": "For complete API documentation, see the [API Reference](../api/builtin_tools/ \"../api/builtin_tools/\").",
    "source_url": "https://ai.pydantic.dev/builtin-tools/",
    "header": "API Reference"
  },
  "2e0871ac0c1135415aff0a78e01724611b35d17acbea65185a3daa7dcb2bee10": {
    "text": "Pydantic AI ships with native tools that can be used to enhance your agent's capabilities.\n\nThe DuckDuckGo search tool allows you to search the web for information. It is built on top of the\n[DuckDuckGo API](https://github.com/deedy5/ddgs \"https://github.com/deedy5/ddgs\").",
    "source_url": "https://ai.pydantic.dev/common-tools/",
    "header": "Introduction"
  },
  "51c72aadbf24909dda729dee4e55f0c40c51aecc6f7375c15d16a42fdc75b2e5": {
    "text": "To use [`duckduckgo_search_tool`](../api/common_tools/#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool \"../api/common_tools/#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool\"), you need to install\n[`pydantic-ai-slim`](../install/#slim-install \"../install/#slim-install\") with the `duckduckgo` optional group:",
    "source_url": "https://ai.pydantic.dev/common-tools/",
    "header": "Installation"
  },
  "a2efdab4e341ec7c786889441de3b339b9d70a112a9da3362b363ac187ece378": {
    "text": "Here's an example of how you can use the DuckDuckGo search tool with an agent:\n\nduckduckgo\\_search.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n\nagent = Agent(\n    'openai:o3-mini',\n    tools=[duckduckgo_search_tool()],\n    system_prompt='Search DuckDuckGo for the given query and return the results.',\n)\n\nresult = agent.run_sync(\n    'Can you list the top five highest-grossing animated films of 2025?'\n)\nprint(result.output)\n\"\"\"\nI looked into several sources on animated box‐office performance in 2025, and while detailed\nrankings can shift as more money is tallied, multiple independent reports have already\nhighlighted a couple of record‐breaking shows. For example:\n\n• Ne Zha 2 – News outlets (Variety, Wikipedia's \"List of animated feature films of 2025\", and others)\n    have reported that this Chinese title not only became the highest‑grossing animated film of 2025\n    but also broke records as the highest‑grossing non‑English animated film ever. One article noted\n    its run exceeded US$1.7 billion.\n• Inside Out 2 – According to data shared on Statista and in industry news, this Pixar sequel has been\n    on pace to set new records (with some sources even noting it as the highest‑grossing animated film\n    ever, as of January 2025).\n\nBeyond those two, some entertainment trade sites (for example, a Just Jared article titled\n\"Top 10 Highest-Earning Animated Films at the Box Office Revealed\") have begun listing a broader\ntop‑10. Although full consolidated figures can sometimes differ by source and are updated daily during\na box‑office run, many of the industry trackers have begun to single out five films as the biggest\nearners so far in 2025.\n\nUnfortunately, although multiple articles discuss the \"top animated films\" of 2025, there isn't yet a\nsingle, universally accepted list with final numbers that names the complete top five. (Box‑office\nrankings, especially mid‑year, can be fluid as films continue to add to their totals.)\n\nBased on what several sources note so far, the two undisputed leaders are:\n1. Ne Zha 2\n2. Inside Out 2\n\nThe remaining top spots (3–5) are reported by some outlets in their \"Top‑10 Animated Films\"\nlists for 2025 but the titles and order can vary depending on the source and the exact cut‑off\ndate of the data. For the most up‑to‑date and detailed ranking (including the 3rd, 4th, and 5th\nhighest‑grossing films), I recommend checking resources like:\n• Wikipedia's \"List of animated feature films of 2025\" page\n• Box‑office tracking sites (such as Box Office Mojo or The Numbers)\n• Trade articles like the one on Just Jared\n\nTo summarize with what is clear from the current reporting:\n1. Ne Zha 2\n2. Inside Out 2\n3–5. Other animated films (yet to be definitively finalized across all reporting outlets)\n\nIf you're looking for a final, consensus list of the top five, it may be best to wait until\nthe 2025 year‑end box‑office tallies are in or to consult a regularly updated entertainment industry source.\n\nWould you like help finding a current source or additional details on where to look for the complete updated list?\n\"\"\"\n```\n\nInfo\n\nTavily is a paid service, but they have free credits to explore their product.\n\nYou need to [sign up for an account](https://app.tavily.com/home \"https://app.tavily.com/home\") and get an API key to use the Tavily search tool.\n\nThe Tavily search tool allows you to search the web for information. It is built on top of the [Tavily API](https://tavily.com/ \"https://tavily.com/\").",
    "source_url": "https://ai.pydantic.dev/common-tools/",
    "header": "Usage"
  },
  "787c114547fd57a9ebdd3c0b4e9409e4680b4fdb09f15ab18904a7215b7b6786": {
    "text": "To use [`tavily_search_tool`](../api/common_tools/#pydantic_ai.common_tools.tavily.tavily_search_tool \"../api/common_tools/#pydantic_ai.common_tools.tavily.tavily_search_tool\"), you need to install\n[`pydantic-ai-slim`](../install/#slim-install \"../install/#slim-install\") with the `tavily` optional group:",
    "source_url": "https://ai.pydantic.dev/common-tools/",
    "header": "Installation"
  },
  "0a0e8f1f3dc01d3e29f763f0132348304e408018e65c9b1f0c089ec67faacabe": {
    "text": "Here's an example of how you can use the Tavily search tool with an agent:\n\ntavily\\_search.py\n\n```\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.common_tools.tavily import tavily_search_tool\n\napi_key = os.getenv('TAVILY_API_KEY')\nassert api_key is not None\n\nagent = Agent(\n    'openai:o3-mini',\n    tools=[tavily_search_tool(api_key)],\n    system_prompt='Search Tavily for the given query and return the results.',\n)\n\nresult = agent.run_sync('Tell me the top news in the GenAI world, give me links.')\nprint(result.output)\n\"\"\"\nHere are some of the top recent news articles related to GenAI:\n\n1. How CLEAR users can improve risk analysis with GenAI – Thomson Reuters\n   Read more: https://legal.thomsonreuters.com/blog/how-clear-users-can-improve-risk-analysis-with-genai/\n   (This article discusses how CLEAR's new GenAI-powered tool streamlines risk analysis by quickly summarizing key information from various public data sources.)\n\n2. TELUS Digital Survey Reveals Enterprise Employees Are Entering Sensitive Data Into AI Assistants More Than You Think – FT.com\n   Read more: https://markets.ft.com/data/announce/detail?dockey=600-202502260645BIZWIRE_USPRX____20250226_BW490609-1\n   (This news piece highlights findings from a TELUS Digital survey showing that many enterprise employees use public GenAI tools and sometimes even enter sensitive data.)\n\n3. The Essential Guide to Generative AI – Virtualization Review\n   Read more: https://virtualizationreview.com/Whitepapers/2025/02/SNOWFLAKE-The-Essential-Guide-to-Generative-AI.aspx\n   (This guide provides insights into how GenAI is revolutionizing enterprise strategies and productivity, with input from industry leaders.)\n\nFeel free to click on the links to dive deeper into each story!\n\"\"\"\n```",
    "source_url": "https://ai.pydantic.dev/common-tools/",
    "header": "Usage"
  },
  "5655d880572374eb3e36d14a29efb7007020e84b6e3d1db6139ba32a201ac31e": {
    "text": "Pydantic AI provides retry functionality for HTTP requests made by model providers through custom HTTP transports.\nThis is particularly useful for handling transient failures like rate limits, network timeouts, or temporary server errors.",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "HTTP Request Retries"
  },
  "ca4bd52edd7a8a3dfb5f18294e366ec39f93bf08a8a76435e1f07f1ad13dbda1": {
    "text": "The retry functionality is built on top of the [tenacity](https://github.com/jd/tenacity \"https://github.com/jd/tenacity\") library and integrates\nseamlessly with httpx clients. You can configure retry behavior for any provider that accepts a custom HTTP client.",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Overview"
  },
  "6422ee38af5809086403a7ba4195aab1e8c266565843714615119140779568f8": {
    "text": "To use the retry transports, you need to install `tenacity`, which you can do via the `retries` dependency group:",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Installation"
  },
  "3fd6e47f8387817adf314813ec4953d79b452c130b5cfa53fac121a9c5dd4616": {
    "text": "Here's an example of adding retry functionality with smart retry handling:\n\nsmart\\_retry\\_example.py\n\n```\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_retrying_client():\n    \"\"\"Create a client with smart retry handling for multiple error types.\"\"\"\n\n    def should_retry_status(response):\n        \"\"\"Raise exceptions for retryable HTTP status codes.\"\"\"\n        if response.status_code in (429, 502, 503, 504):\n            response.raise_for_status()  # This will raise HTTPStatusError\n\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            # Retry on HTTP errors and connection issues\n            retry=retry_if_exception_type((HTTPStatusError, ConnectionError)),\n            # Smart waiting: respects Retry-After headers, falls back to exponential backoff\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300\n            ),\n            # Stop after 5 attempts\n            stop=stop_after_attempt(5),\n            # Re-raise the last exception if all retries fail\n            reraise=True\n        ),\n        validate_response=should_retry_status\n    )\n    return AsyncClient(transport=transport)",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Usage Example"
  },
  "2e20069146684503a84f799c16e437709135d24446f7317b60f9c0e376d152bd": {
    "text": "client = create_retrying_client()\nmodel = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Use the retrying client with a model"
  },
  "8ed386e987c9106fac4ddee71e1ff2ca40e03ca969c2eae4b6b18f062d75b760": {
    "text": "The `wait_retry_after` function is a smart wait strategy that automatically respects HTTP `Retry-After` headers:\n\nwait\\_strategy\\_example.py\n\n```\nfrom tenacity import wait_exponential\n\nfrom pydantic_ai.retries import wait_retry_after",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "wait\\_retry\\_after"
  },
  "036916cdc2e3fa37e61bf94edb5f92e26479f60f712c8f099acea8eb3f8ddbdd": {
    "text": "wait_strategy_1 = wait_retry_after()",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Basic usage - respects Retry-After headers, falls back to exponential backoff"
  },
  "d4b97ab4c4d85637cbac926c9935c6e5689107e30e9851cf88a61e253a47ba0d": {
    "text": "wait_strategy_2 = wait_retry_after(\n    fallback_strategy=wait_exponential(multiplier=2, max=120),\n    max_wait=600  # Never wait more than 10 minutes\n)\n```\n\nThis wait strategy:\n\n* Automatically parses `Retry-After` headers from HTTP 429 responses\n* Supports both seconds format (`\"30\"`) and HTTP date format (`\"Wed, 21 Oct 2015 07:28:00 GMT\"`)\n* Falls back to your chosen strategy when no header is present\n* Respects the `max_wait` limit to prevent excessive delays",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Custom configuration"
  },
  "475aaeb673bcab53a253ba12aba277978540dfc34585f236161b12d487714ede": {
    "text": "For asynchronous HTTP clients (recommended for most use cases):\n\nasync\\_transport\\_example.py\n\n```\nfrom httpx import AsyncClient\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig\n\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.\n    Without a response validator, only network errors and timeouts will result in a retry.\n    \"\"\"\n    response.raise_for_status()",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "AsyncTenacityTransport"
  },
  "ea736515411825769051e7ef2bd2c49c3352420c4c9f92040f2a3e236107a5d3": {
    "text": "transport = AsyncTenacityTransport(\n    config=RetryConfig(stop=stop_after_attempt(3), reraise=True),\n    validate_response=validator\n)",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Create the transport"
  },
  "08f32dde8c17715919f68508c4219734714a7e5929931a7272bc5c5e6f1f60ff": {
    "text": "client = AsyncClient(transport=transport)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Create a client using the transport:"
  },
  "f865428cc7da0de05796d365c01ed3b35d9bf6bd0edb29ab2df6173dea55edd4": {
    "text": "For synchronous HTTP clients:\n\nsync\\_transport\\_example.py\n\n```\nfrom httpx import Client\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_ai.retries import RetryConfig, TenacityTransport\n\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.\n    Without a response validator, only network errors and timeouts will result in a retry.\n    \"\"\"\n    response.raise_for_status()",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "TenacityTransport"
  },
  "15da1ca3fd642ae5dd48448a1d10be28a7f9d4342d06e25011f01285a8de4bf0": {
    "text": "transport = TenacityTransport(\n    config=RetryConfig(stop=stop_after_attempt(3), reraise=True),\n    validate_response=validator\n)",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Create the transport"
  },
  "3fdaa393945c41bc167efc791e2070f98a69467d3caa0ee2b1e10706a7d7a9ea": {
    "text": "client = Client(transport=transport)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Create a client using the transport"
  },
  "080c3ce795abc79eff686ba149f6fd08c7b3bff8e6203fd2ffe8eadcb09322a4": {
    "text": "rate\\_limit\\_handling.py\n\n```\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_rate_limit_client():\n    \"\"\"Create a client that respects Retry-After headers from rate limiting responses.\"\"\"\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300  # Don't wait more than 5 minutes\n            ),\n            stop=stop_after_attempt(10),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()  # Raises HTTPStatusError for 4xx/5xx\n    )\n    return AsyncClient(transport=transport)",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Rate Limit Handling with Retry-After Support"
  },
  "27d1391ecc3b4d4278bc58f6edd74de481e598dfaba658dcc7cbb4b3afaf9a01": {
    "text": "client = create_rate_limit_client()",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Example usage"
  },
  "f3ebadbaeb7db9ef8dc401ae620d88f827f948ab4bc99dbf650e0229d1c66cd2": {
    "text": "```\n\nThe `wait_retry_after` function automatically detects `Retry-After` headers in 429 (rate limit) responses and waits for the specified time. If no header is present, it falls back to exponential backoff.",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Client is now ready to use with any HTTP requests and will respect Retry-After headers"
  },
  "7cc4f1fcdecc663bd181746c2b67768e8f8407c88acde88092548ea8be4584f6": {
    "text": "network\\_error\\_handling.py\n\n```\nimport httpx\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig\n\n\ndef create_network_resilient_client():\n    \"\"\"Create a client that handles network errors with retries.\"\"\"\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception_type((\n                httpx.TimeoutException,\n                httpx.ConnectError,\n                httpx.ReadError\n            )),\n            wait=wait_exponential(multiplier=1, max=10),\n            stop=stop_after_attempt(3),\n            reraise=True\n        )\n    )\n    return httpx.AsyncClient(transport=transport)",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Network Error Handling"
  },
  "f0d4e64a767d81bd9a1e4682314e06153e7812892ec18ae3e9d368c24ea49d00": {
    "text": "client = create_network_resilient_client()",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Example usage"
  },
  "0ea6ad15bd2e301e338689b4f606deb0e8c795cd44b4737d3f69e276d0237eb9": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Client will now retry on timeout, connection, and read errors"
  },
  "a3be8acc4555e07926605a61158eec43040a88743fd5302ac5b71f34ddc24e3b": {
    "text": "custom\\_retry\\_logic.py\n\n```\nimport httpx\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_custom_retry_client():\n    \"\"\"Create a client with custom retry logic.\"\"\"\n    def custom_retry_condition(exception):\n        \"\"\"Custom logic to determine if we should retry.\"\"\"\n        if isinstance(exception, httpx.HTTPStatusError):\n            # Retry on server errors but not client errors\n            return 500 <= exception.response.status_code < 600\n        return isinstance(exception, httpx.TimeoutException | httpx.ConnectError)\n\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=custom_retry_condition,\n            # Use wait_retry_after for smart waiting on rate limits,\n            # with custom exponential backoff as fallback\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=2, max=30),\n                max_wait=120\n            ),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    return httpx.AsyncClient(transport=transport)\n\nclient = create_custom_retry_client()",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Custom Retry Logic"
  },
  "d39665f24aa59d3c81b9924788a80beb6f7dbf1cf85531236dc2953f3ff9a8a4": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Client will retry server errors (5xx) and network errors, but not client errors (4xx)"
  },
  "fe5c31669b28c8870df07f9e98b6424dd62cd25eb9412c2ecb57b5945a2f50e3": {
    "text": "The retry transports work with any provider that accepts a custom HTTP client:",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Using with Different Providers"
  },
  "4b60a3b63da20a4abf21cb2bf9fc13a69dc1823f756994a702c5710c16f4b7b8": {
    "text": "openai\\_with\\_retries.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "OpenAI"
  },
  "6c03c80130ecf49b7a6dc60136f9e231f38bf457f3581a9b0af685e61faf5647": {
    "text": "anthropic\\_with\\_retries.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = AnthropicModel('claude-3-5-sonnet-20241022', provider=AnthropicProvider(http_client=client))\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Anthropic"
  },
  "90ceea7c2ce5f72f5f832c75b394f1183f6413c43737e3a701d16e6fa403bc8b": {
    "text": "openai\\_compatible\\_with\\_retries.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel(\n    'your-model-name',  # Replace with actual model name\n    provider=OpenAIProvider(\n        base_url='https://api.example.com/v1',  # Replace with actual API URL\n        api_key='your-api-key',  # Replace with actual API key\n        http_client=client\n    )\n)\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Any OpenAI-Compatible Provider"
  },
  "df1342e0e7990fb665c6ee0eddeabb95118ef6257d3e98e6e1d79abcb92c4c71": {
    "text": "1. **Start Conservative**: Begin with a small number of retries (3-5) and reasonable wait times.\n2. **Use Exponential Backoff**: This helps avoid overwhelming servers during outages.\n3. **Set Maximum Wait Times**: Prevent indefinite delays with reasonable maximum wait times.\n4. **Handle Rate Limits Properly**: Respect `Retry-After` headers when possible.\n5. **Log Retry Attempts**: Add logging to monitor retry behavior in production. (This will be picked up by Logfire automatically if you instrument httpx.)\n6. **Consider Circuit Breakers**: For high-traffic applications, consider implementing circuit breaker patterns.",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Best Practices"
  },
  "ecbd6eb03b0a9a91d7fa0de9b2d02c9af895aa8630a4cc781d8b67698447e5bd": {
    "text": "The retry transports will re-raise the last exception if all retry attempts fail. Make sure to handle these appropriately in your application:\n\nerror\\_handling\\_example.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-4o', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Error Handling"
  },
  "75ed90e0e538e012a64b00ea737e4fb3179630f41928c4800d5ead187a3eed66": {
    "text": "* Retries add latency to requests, especially with exponential backoff\n* Consider the total timeout for your application when configuring retry behavior\n* Monitor retry rates to detect systemic issues\n* Use async transports for better concurrency when handling multiple requests\n\nFor more advanced retry configurations, refer to the [tenacity documentation](https://tenacity.readthedocs.io/ \"https://tenacity.readthedocs.io/\").",
    "source_url": "https://ai.pydantic.dev/retries/",
    "header": "Performance Considerations"
  },
  "b5e49413d119f359ec2df192c5938db9fa50953d12241edc2a1bf360cfb8c1c7": {
    "text": "Pydantic AI allows you to build durable agents that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability. Durable agents have full support for [streaming](../agents/#streaming-all-events \"../agents/#streaming-all-events\") and [MCP](../mcp/client/ \"../mcp/client/\"), with the added benefit of fault tolerance.\n\n[Temporal](https://temporal.io \"https://temporal.io\") is a popular [durable execution](https://docs.temporal.io/evaluate/understanding-temporal#durable-execution \"https://docs.temporal.io/evaluate/understanding-temporal#durable-execution\") platform that's natively supported by Pydantic AI.\nThe integration only uses Pydantic AI's public interface, so it can also serve as a reference for how to integrate with another durable execution systems.",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Durable Execution with Temporal"
  },
  "88d1f3f1cf4a045cefad1b37435b3090fad3666fb5e4dd5b6af448b1609e8095": {
    "text": "In Temporal's durable execution implementation, a program that crashes or encounters an exception while interacting with a model or API will retry until it can successfully complete.\n\nTemporal relies primarily on a replay mechanism to recover from failures.\nAs the program makes progress, Temporal saves key inputs and decisions, allowing a re-started program to pick up right where it left off.\n\nThe key to making this work is to separate the application's repeatable (deterministic) and non-repeatable (non-deterministic) parts:\n\n1. Deterministic pieces, termed [**workflows**](https://docs.temporal.io/workflow-definition \"https://docs.temporal.io/workflow-definition\"), execute the same way when re-run with the same inputs.\n2. Non-deterministic pieces, termed [**activities**](https://docs.temporal.io/activities \"https://docs.temporal.io/activities\"), can run arbitrary code, performing I/O and any other operations.\n\nWorkflow code can run for extended periods and, if interrupted, resume exactly where it left off.\nCritically, workflow code generally *cannot* include any kind of I/O, over the network, disk, etc.\nActivity code faces no restrictions on I/O or external interactions, but if an activity fails part-way through it is restarted from the beginning.\n\nNote\n\nIf you are familiar with celery, it may be helpful to think of Temporal activities as similar to celery tasks, but where you wait for the task to complete and obtain its result before proceeding to the next step in the workflow.\nHowever, Temporal workflows and activities offer a great deal more flexibility and functionality than celery tasks.\n\nSee the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks \"https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks\") for more information\n\nIn the case of Pydantic AI agents, integration with Temporal means that [model requests](../models/ \"../models/\"), [tool calls](../tools/ \"../tools/\") that may require I/O, and [MCP server communication](../mcp/client/ \"../mcp/client/\") all need to be offloaded to Temporal activities due to their I/O requirements, while the logic that coordinates them (i.e. the agent run) lives in the workflow. Code that handles a scheduled job or web request can then execute the workflow, which will in turn execute the activities as needed.\n\nThe diagram below shows the overall architecture of an agentic application in Temporal.\nThe Temporal Server is responsible for tracking program execution and making sure the associated state is preserved reliably (i.e., stored to an internal database, and possibly replicated across cloud regions).\nTemporal Server manages data in encrypted form, so all data processing occurs on the Worker, which runs the workflow and activities.\n\n```\n            +---------------------+\n            |   Temporal Server   |      (Stores workflow state,\n            +---------------------+       schedules activities,\n                     ^                    persists progress)\n                     |\n        Save state,  |   Schedule Tasks,\n        progress,    |   load state on resume\n        timeouts     |\n                     |\n+------------------------------------------------------+\n|                      Worker                          |\n|   +----------------------------------------------+   |\n|   |              Workflow Code                   |   |\n|   |       (Agent Run Loop)                       |   |\n|   +----------------------------------------------+   |\n|          |          |                |               |\n|          v          v                v               |\n|   +-----------+ +------------+ +-------------+       |\n|   | Activity  | | Activity   | |  Activity   |       |\n|   | (Tool)    | | (MCP Tool) | | (Model API) |       |\n|   +-----------+ +------------+ +-------------+       |\n|         |           |                |               |\n+------------------------------------------------------+\n          |           |                |\n          v           v                v\n      [External APIs, services, databases, etc.]\n```\n\nSee the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks \"https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks\") for more information.",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Durable Execution"
  },
  "b506761b1178f938f4bb54ec7a1cf0da1f28f774385fcccab43cbf2b0abd5346": {
    "text": "Any agent can be wrapped in a [`TemporalAgent`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent\") to get a durable agent that can be used inside a deterministic Temporal workflow, by automatically offloading all work that requires I/O (namely model requests, tool calls, and MCP server communication) to non-deterministic activities.\n\nAt the time of wrapping, the agent's [model](../models/ \"../models/\") and [toolsets](../toolsets/ \"../toolsets/\") (including function tools registered on the agent and MCP servers) are frozen, activities are dynamically created for each, and the original model and toolsets are wrapped to call on the worker to execute the corresponding activities instead of directly performing the actions inside the workflow. The original agent can still be used as normal outside the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\nHere is a simple but complete example of wrapping an agent for durable execution, creating a Temporal workflow with durable execution logic, connecting to a Temporal server, and running the workflow from non-durable code. All it requires is a Temporal server to be [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally \"https://github.com/temporalio/temporal#download-and-start-temporal-server-locally\"):\n\n```\nbrew install temporal\ntemporal server start-dev\n```\n\ntemporal\\_agent.py\n\n```\nimport uuid\n\nfrom temporalio import workflow\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.temporal import (\n    AgentPlugin,\n    PydanticAIPlugin,\n    TemporalAgent,\n)\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (10)!\n)\n\ntemporal_agent = TemporalAgent(agent)  # (1)!\n\n\n@workflow.defn\nclass GeographyWorkflow:  # (2)!\n    @workflow.run\n    async def run(self, prompt: str) -> str:\n        result = await temporal_agent.run(prompt)  # (3)!\n        return result.output\n\n\nasync def main():\n    client = await Client.connect(  # (4)!\n        'localhost:7233',  # (5)!\n        plugins=[PydanticAIPlugin()],  # (6)!\n    )\n\n    async with Worker(  # (7)!\n        client,\n        task_queue='geography',\n        workflows=[GeographyWorkflow],\n        plugins=[AgentPlugin(temporal_agent)],  # (8)!\n    ):\n        output = await client.execute_workflow(  # (9)!\n            GeographyWorkflow.run,\n            args=['What is the capital of Mexico?'],\n            id=f'geography-{uuid.uuid4()}',\n            task_queue='geography',\n        )\n        print(output)\n        #> Mexico City (Ciudad de México, CDMX)\n```\n\n1. The original `Agent` cannot be used inside a deterministic Temporal workflow, but the `TemporalAgent` can.\n2. As explained above, the workflow represents a deterministic piece of code that can use non-deterministic activities for operations that require I/O.\n3. [`TemporalAgent.run()`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent.run \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent.run\") works just like [`Agent.run()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run \"../api/agent/#pydantic_ai.agent.AbstractAgent.run\"), but it will automatically offload model requests, tool calls, and MCP server communication to Temporal activities.\n4. We connect to the Temporal server which keeps track of workflow and activity execution.\n5. This assumes the Temporal server is [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally \"https://github.com/temporalio/temporal#download-and-start-temporal-server-locally\").\n6. The [`PydanticAIPlugin`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.PydanticAIPlugin \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.PydanticAIPlugin\") tells Temporal to use Pydantic for serialization and deserialization, and to treat [`UserError`](../api/exceptions/#pydantic_ai.exceptions.UserError \"../api/exceptions/#pydantic_ai.exceptions.UserError\") exceptions as non-retryable.\n7. We start the worker that will listen on the specified task queue and run workflows and activities. In a real world application, this might be run in a separate service.\n8. The [`AgentPlugin`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.AgentPlugin \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.AgentPlugin\") registers the `TemporalAgent`'s activities with the worker.\n9. We call on the server to execute the workflow on a worker that's listening on the specified task queue.\n10. The agent's `name` is used to uniquely identify its activities.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nIn a real world application, the agent, workflow, and worker are typically defined separately from the code that calls for a workflow to be executed.\nBecause Temporal workflows need to be defined at the top level of the file and the `TemporalAgent` instance is needed inside the workflow and when starting the worker (to register the activities), it needs to be defined at the top level of the file as well.\n\nFor more information on how to use Temporal in Python applications, see their [Python SDK guide](https://docs.temporal.io/develop/python \"https://docs.temporal.io/develop/python\").",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Durable Agent"
  },
  "8fdf020dd0bd897a0c29ef2a335be0294bcfdee70ba69349c5604849f9a8a2cc": {
    "text": "There are a few considerations specific to agents and toolsets when using Temporal for durable execution. These are important to understand to ensure that your agents and toolsets work correctly with Temporal's workflow and activity model.\n\nTo ensure that Temporal knows what code to run when an activity fails or is interrupted and then restarted, even if your code is changed in between, each activity needs to have a name that's stable and unique.\n\nWhen `TemporalAgent` dynamically creates activities for the wrapped agent's model requests and toolsets (specifically those that implement their own tool listing and calling, i.e. [`FunctionToolset`](../api/toolsets/#pydantic_ai.toolsets.FunctionToolset \"../api/toolsets/#pydantic_ai.toolsets.FunctionToolset\") and [`MCPServer`](../api/mcp/#pydantic_ai.mcp.MCPServer \"../api/mcp/#pydantic_ai.mcp.MCPServer\")), their names are derived from the agent's [`name`](../api/agent/#pydantic_ai.agent.AbstractAgent.name \"../api/agent/#pydantic_ai.agent.AbstractAgent.name\") and the toolsets' [`id`s](../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.id \"../api/toolsets/#pydantic_ai.toolsets.AbstractToolset.id\"). These fields are normally optional, but are required to be set when using Temporal. They should not be changed once the durable agent has been deployed to production as this would break active workflows.\n\nOther than that, any agent and toolset will just work!",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Temporal Integration Considerations"
  },
  "e46a95341c3a17c9a9f95717f27fcd284b2c4f25c314dafb0457eb4fc08d1cb4": {
    "text": "As workflows and activities run in separate processes, any values passed between them need to be serializable. As these payloads are stored in the workflow execution event history, Temporal limits their size to 2MB.\n\nTo account for these limitations, tool functions and the [event stream handler](#streaming \"#streaming\") running inside activities receive a limited version of the agent's [`RunContext`](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\"), and it's your responsibility to make sure that the [dependencies](../dependencies/ \"../dependencies/\") object provided to [`TemporalAgent.run()`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent.run \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent.run\") can be serialized using Pydantic.\n\nSpecifically, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, and `run_step` fields are available by default, and trying to access `model`, `usage`, `prompt`, `messages`, or `tracer` will raise an error.\nIf you need one or more of these attributes to be available inside activities, you can create a [`TemporalRunContext`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalRunContext \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalRunContext\") subclass with custom `serialize_run_context` and `deserialize_run_context` class methods and pass it to [`TemporalAgent`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent\") as `run_context_type`.",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Agent Run Context and Dependencies"
  },
  "7f9d35ac4dff98bd5524025e88cf965d0a9ce8979761e39f9ccc219b9331f0be": {
    "text": "Because Temporal activities cannot stream output directly to the activity call site, [`Agent.run_stream()`](../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream \"../api/agent/#pydantic_ai.agent.AbstractAgent.run_stream\") and [`Agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\") are not supported.\n\nInstead, you can implement streaming by setting an [`event_stream_handler`](../api/agent/#pydantic_ai.agent.EventStreamHandler \"../api/agent/#pydantic_ai.agent.EventStreamHandler\") on the `Agent` or `TemporalAgent` instance and using [`TemporalAgent.run()`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent.run \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.TemporalAgent.run\") inside the workflow.\nThe event stream handler function will receive the agent [run context](../api/tools/#pydantic_ai.tools.RunContext \"../api/tools/#pydantic_ai.tools.RunContext\") and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../agents/#streaming-all-events \"../agents/#streaming-all-events\").\n\nAs the streaming model request activity, workflow, and workflow execution call all take place in separate processes, passing data between them requires some care:\n\n* To get data from the workflow call site or workflow to the event stream handler, you can use a [dependencies object](#agent-run-context-and-dependencies \"#agent-run-context-and-dependencies\").\n* To get data from the event stream handler to the workflow, workflow call site, or a frontend, you need to use an external system that the event stream handler can write to and the event consumer can read from, like a message queue. You can use the dependency object to make sure the same connection string or other unique ID is available in all the places that need it.",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Streaming"
  },
  "07017dd677052091993168e1fe59e0150d1172f8e662d2ef8c634e498bf51ed6": {
    "text": "Temporal activity configuration, like timeouts and retry policies, can be customized by passing [`temporalio.workflow.ActivityConfig`](https://python.temporal.io/temporalio.workflow.ActivityConfig.html \"https://python.temporal.io/temporalio.workflow.ActivityConfig.html\") objects to the `TemporalAgent` constructor:\n\n* `activity_config`: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.\n* `model_activity_config`: The Temporal activity config to use for model request activities. This is merged with the base activity config.\n* `toolset_activity_config`: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.\n* `tool_activity_config`: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.\n  This is merged with the base and toolset-specific activity configs.\n\n  If a tool does not use I/O, you can specify `False` to disable using an activity. Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Activity Configuration"
  },
  "f7c3ee6f19ff88d98405d17521076c14f043eee8343350c7d8ab531103b20018": {
    "text": "On top of the automatic retries for request failures that Temporal will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper `Retry-After` handling.\n\nWhen using Temporal, it's recommended to not use [HTTP Request Retries](../retries/ \"../retries/\") and to turn off your provider API client's own retry logic, for example by setting `max_retries=0` on a [custom `OpenAIProvider` API client](../models/openai/#custom-openai-client \"../models/openai/#custom-openai-client\").\n\nYou can customize Temporal's retry policy using [activity configuration](#activity-configuration \"#activity-configuration\").",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Activity Retries"
  },
  "c2de1074ef87b1a85f913700a0f55f9017857b1f0b59e30d1b72b23e7f6f5d02": {
    "text": "Temporal generates telemetry events and metrics for each workflow and activity execution, and Pydantic AI generates events for each agent run, model request and tool call. These can be sent to [Pydantic Logfire](../logfire/ \"../logfire/\") to get a complete picture of what's happening in your application.\n\nTo use Logfire with Temporal, you need to pass a [`LogfirePlugin`](../api/durable_exec/#pydantic_ai.durable_exec.temporal.LogfirePlugin \"../api/durable_exec/#pydantic_ai.durable_exec.temporal.LogfirePlugin\") object to Temporal's `Client.connect()`:\n\nlogfire\\_plugin.py\n\n```\nfrom temporalio.client import Client\n\nfrom pydantic_ai.durable_exec.temporal import LogfirePlugin, PydanticAIPlugin\n\n\nasync def main():\n    client = await Client.connect(\n        'localhost:7233',\n        plugins=[PydanticAIPlugin(), LogfirePlugin()],\n    )\n```\n\nBy default, the `LogfirePlugin` will instrument Temporal (including metrics) and Pydantic AI and send all data to Logfire. To customize Logfire configuration and instrumentation, you can pass a `logfire_setup` function to the `LogfirePlugin` constructor and return a custom `Logfire` instance (i.e. the result of `logfire.configure()`). To disable sending Temporal metrics to Logfire, you can pass `metrics=False` to the `LogfirePlugin` constructor.",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Observability with Logfire"
  },
  "5704cd2ff5ddda3d17596f7ebfd169da5d43647dd53a0dec48d963f8f5716d56": {
    "text": "When `logfire.info` is used inside an activity and the `pandas` package is among your project's dependencies, you may encounter the following error which seems to be the result of an import race condition:\n\n```\nAttributeError: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)\n```\n\nTo fix this, you can use the [`temporalio.workflow.unsafe.imports_passed_through()`](https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through \"https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through\") context manager to proactively import the package and not have it be reloaded in the workflow sandbox:\n\ntemporal\\_activity.py\n\n```\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    import pandas\n```",
    "source_url": "https://ai.pydantic.dev/temporal/",
    "header": "Pandas"
  },
  "26ec14b48fa62b447a185acb8f9d7de272171faa0461849bbaaccf16cec72a64": {
    "text": "Pydantic AI supports [Model Context Protocol (MCP)](https://modelcontextprotocol.io \"https://modelcontextprotocol.io\") in three ways:\n\n1. [Agents](../agents/ \"../agents/\") act as an MCP Client, connecting to MCP servers to use their tools, [learn more …](client/ \"client/\")\n2. Agents can be used within MCP servers, [learn more …](server/ \"server/\")\n3. As part of Pydantic AI, we're building a number of MCP servers, [see below](#mcp-servers \"#mcp-servers\")",
    "source_url": "https://ai.pydantic.dev/mcp/",
    "header": "Model Context Protocol (MCP)"
  },
  "4967edceccfc269ceebc54e716d0e2296f2272ea0d956daac890db75a6f80041": {
    "text": "The Model Context Protocol is a standardized protocol that allow AI applications (including programmatic agents like Pydantic AI, coding agents like [cursor](https://www.cursor.com/ \"https://www.cursor.com/\"), and desktop applications like [Claude Desktop](https://claude.ai/download \"https://claude.ai/download\")) to connect to external tools and services using a common interface.\n\nAs with other protocols, the dream of MCP is that a wide range of applications can speak to each other without the need for specific integrations.\n\nThere is a great list of MCP servers at [github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers \"https://github.com/modelcontextprotocol/servers\").\n\nSome examples of what this means:\n\n* Pydantic AI could use a web search service implemented as an MCP server to implement a deep research agent\n* Cursor could connect to the [Pydantic Logfire](https://github.com/pydantic/logfire-mcp \"https://github.com/pydantic/logfire-mcp\") MCP server to search logs, traces and metrics to gain context while fixing a bug\n* Pydantic AI, or any other MCP client could connect to our [Run Python](run-python/ \"run-python/\") MCP server to run arbitrary Python code in a sandboxed environment",
    "source_url": "https://ai.pydantic.dev/mcp/",
    "header": "What is MCP?"
  },
  "0e639ba9bef99636a1d2c5ecc49b4a57de28a3428b14531fb14def0d5c71a53c": {
    "text": "To add functionality to Pydantic AI while making it as widely usable as possible, we're implementing some functionality as MCP servers.\n\nSo far, we've only implemented one MCP server as part of Pydantic AI:\n\n* [Run Python](run-python/ \"run-python/\"): A sandboxed Python interpreter that can run arbitrary code, with a focus on security and safety.",
    "source_url": "https://ai.pydantic.dev/mcp/",
    "header": "MCP Servers"
  },
  "758f62553ae4e0f124046e3428bd00d40ed19dab95da3c70070b00879b89035f": {
    "text": "Pydantic AI can act as an [MCP client](https://modelcontextprotocol.io/quickstart/client \"https://modelcontextprotocol.io/quickstart/client\"), connecting to MCP servers\nto use their tools.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Client"
  },
  "42f98366dacb639274ca0719d52df114bfbc76b5833932014d51502de1109054": {
    "text": "You need to either install [`pydantic-ai`](../../install/ \"../../install/\"), or[`pydantic-ai-slim`](../../install/#slim-install \"../../install/#slim-install\") with the `mcp` optional group:",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Install"
  },
  "bd41665b9840992795c25f49f4c2a2ad318ad9e0bff5a513b94278b0eb4d52e9": {
    "text": "Pydantic AI comes with two ways to connect to MCP servers:\n\nExamples of all three are shown below; [mcp-run-python](../run-python/ \"../run-python/\") is used as the MCP server in all examples.\n\nEach MCP server instance is a [toolset](../../toolsets/ \"../../toolsets/\") and can be registered with an [`Agent`](../../api/agent/#pydantic_ai.agent.Agent \"../../api/agent/#pydantic_ai.agent.Agent\") using the `toolsets` argument.\n\nYou can use the [`async with agent`](../../api/agent/#pydantic_ai.agent.Agent.__aenter__ \"../../api/agent/#pydantic_ai.agent.Agent.__aenter__\") context manager to open and close connections to all registered servers (and in the case of stdio servers, start and stop the subprocesses) around the context where they'll be used in agent runs. You can also use [`async with server`](../../api/mcp/#pydantic_ai.mcp.MCPServer.__aenter__ \"../../api/mcp/#pydantic_ai.mcp.MCPServer.__aenter__\") to manage the connection or subprocess of a specific server, for example if you'd like to use it with multiple agents. If you don't explicitly enter one of these context managers to set up the server, this will be done automatically when it's needed (e.g. to list the available tools or call a specific tool), but it's more efficient to do so around the entire context where you expect the servers to be used.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Usage"
  },
  "f8d7e167f11b3e719f792378e6588d7343e4820eb660336c95a06f123facee5a": {
    "text": "[`MCPServerStreamableHTTP`](../../api/mcp/#pydantic_ai.mcp.MCPServerStreamableHTTP \"../../api/mcp/#pydantic_ai.mcp.MCPServerStreamableHTTP\") connects over HTTP using the\n[Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http \"https://modelcontextprotocol.io/introduction#streamable-http\") transport to a server.\n\nNote\n\n[`MCPServerStreamableHTTP`](../../api/mcp/#pydantic_ai.mcp.MCPServerStreamableHTTP \"../../api/mcp/#pydantic_ai.mcp.MCPServerStreamableHTTP\") requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI.\n\nBefore creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport.\n\nstreamable\\_http\\_server.py\n\n```\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='streamable-http')\n```\n\nThen we can create the client:\n\nmcp\\_streamable\\_http\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')  # (1)!\nagent = Agent('openai:gpt-4o', toolsets=[server])  # (2)!\n\nasync def main():\n    async with agent:  # (3)!\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. Define the MCP server with the URL used to connect.\n2. Create an agent with the MCP server attached.\n3. Create a client session to connect to the server.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n**What's happening here?**\n\n* The model is receiving the prompt \"how many days between 2000-01-01 and 2025-03-18?\"\n* The model decides \"Oh, I've got this `run_python_code` tool, that will be a good way to answer this question\", and writes some python code to calculate the answer.\n* The model returns a tool call\n* Pydantic AI sends the tool call to the MCP server using the SSE transport\n* The model is called again with the return value of running the code\n* The model returns the final answer\n\nYou can visualise this clearly, and even see the code that's run by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs \"https://logfire.pydantic.dev/docs\"):\n\nmcp\\_sse\\_client\\_logfire.py\n\n```\nimport logfire\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n```\n\nWill display as follows:",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Streamable HTTP Client"
  },
  "ae4a524e3b29f76ddae9c48c64d64a9a92641e1b4e01511880cc115121913be6": {
    "text": "[`MCPServerSSE`](../../api/mcp/#pydantic_ai.mcp.MCPServerSSE \"../../api/mcp/#pydantic_ai.mcp.MCPServerSSE\") connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse \"https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse\") to a server.\n\nNote\n\n[`MCPServerSSE`](../../api/mcp/#pydantic_ai.mcp.MCPServerSSE \"../../api/mcp/#pydantic_ai.mcp.MCPServerSSE\") requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI.\n\nThe name \"HTTP\" is used since this implementation will be adapted in future to use the new\n[Streamable HTTP](https://github.com/modelcontextprotocol/specification/pull/206 \"https://github.com/modelcontextprotocol/specification/pull/206\") currently in development.\n\nBefore creating the SSE client, we need to run the server (docs [here](../run-python/ \"../run-python/\")):\n\nterminal (run sse server)\n\n```\ndeno run \\\n  -N -R=node_modules -W=node_modules --node-modules-dir=auto \\\n  jsr:@pydantic/mcp-run-python sse\n```\n\nmcp\\_sse\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE(url='http://localhost:3001/sse')  # (1)!\nagent = Agent('openai:gpt-4o', toolsets=[server])  # (2)!\n\n\nasync def main():\n    async with agent:  # (3)!\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. Define the MCP server with the URL used to connect.\n2. Create an agent with the MCP server attached.\n3. Create a client session to connect to the server.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "SSE Client"
  },
  "2627e87878f23257fb5952b63a2cb0363175fdb05a9a15218a4afbdfa32dd8d7": {
    "text": "The other transport offered by MCP is the [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio \"https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio\") where the server is run as a subprocess and communicates with the client over `stdin` and `stdout`. In this case, you'd use the [`MCPServerStdio`](../../api/mcp/#pydantic_ai.mcp.MCPServerStdio \"../../api/mcp/#pydantic_ai.mcp.MCPServerStdio\") class.\n\nmcp\\_stdio\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ]\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n\nasync def main():\n    async with agent:\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. See [MCP Run Python](../run-python/ \"../run-python/\") for more information.\n\nThe MCP servers provide the ability to set a `process_tool_call` which allows\nthe customisation of tool call requests and their responses.\n\nA common use case for this is to inject metadata to the requests which the server\ncall needs.\n\nmcp\\_process\\_tool\\_call.py\n\n```\nfrom typing import Any\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.mcp import CallToolFunc, MCPServerStdio, ToolResult\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def process_tool_call(\n    ctx: RunContext[int],\n    call_tool: CallToolFunc,\n    name: str,\n    tool_args: dict[str, Any],\n) -> ToolResult:\n    \"\"\"A tool call processor that passes along the deps.\"\"\"\n    return await call_tool(name, tool_args, {'deps': ctx.deps})\n\n\nserver = MCPServerStdio('python', ['mcp_server.py'], process_tool_call=process_tool_call)\nagent = Agent(\n    model=TestModel(call_tools=['echo_deps']),\n    deps_type=int,\n    toolsets=[server]\n)\n\n\nasync def main():\n    async with agent:\n        result = await agent.run('Echo with deps set to 42', deps=42)\n    print(result.output)\n    #> {\"echo_deps\":{\"echo\":\"This is an echo message\",\"deps\":42}}\n```\n\nWhen connecting to multiple MCP servers that might provide tools with the same name, you can use the `tool_prefix` parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server.\n\nThis allows you to use multiple servers that might have overlapping tool names without conflicts:\n\nmcp\\_tool\\_prefix\\_http\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "MCP \"stdio\" Server"
  },
  "c87bea79579144c82fba26aeda1b0c79c374a678b1feb35fe2a6232d38ca9bce": {
    "text": "weather_server = MCPServerSSE(\n    url='http://localhost:3001/sse',\n    tool_prefix='weather'  # Tools will be prefixed with 'weather_'\n)\n\ncalculator_server = MCPServerSSE(\n    url='http://localhost:3002/sse',\n    tool_prefix='calc'  # Tools will be prefixed with 'calc_'\n)",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Create two servers with different prefixes"
  },
  "752fb4349face2a92ec45d2de37e3e3a8a94607eab47676334c192ea9c9d2dcf": {
    "text": "agent = Agent('openai:gpt-4o', toolsets=[weather_server, calculator_server])\n```",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "- 'calc_get_data'"
  },
  "1efd896fa298e1736b73491cb33a456e79ec2ad6ed81f779a7dc78eb2d145fad": {
    "text": "In some environments you need to tweak how HTTPS connections are established –\nfor example to trust an internal Certificate Authority, present a client\ncertificate for **mTLS**, or (during local development only!) disable\ncertificate verification altogether.\nAll HTTP-based MCP client classes\n([`MCPServerStreamableHTTP`](../../api/mcp/#pydantic_ai.mcp.MCPServerStreamableHTTP \"../../api/mcp/#pydantic_ai.mcp.MCPServerStreamableHTTP\") and\n[`MCPServerSSE`](../../api/mcp/#pydantic_ai.mcp.MCPServerSSE \"../../api/mcp/#pydantic_ai.mcp.MCPServerSSE\")) expose an `http_client`\nparameter that lets you pass your own pre-configured\n[`httpx.AsyncClient`](https://www.python-httpx.org/async/ \"https://www.python-httpx.org/async/\").\n\nmcp\\_custom\\_tls\\_client.py\n\n```\nimport ssl\n\nimport httpx\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Custom TLS / SSL configuration"
  },
  "c867dc6b6f869a5ec8321a5e89cd794785a1cc5b1d4ccbe7f7b99559c7f877f6": {
    "text": "ssl_ctx = ssl.create_default_context(cafile='/etc/ssl/private/my_company_ca.pem')",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Trust an internal / self-signed CA"
  },
  "d92cf2fae30714a9039ea43ab16c853e4cf26c85330b4039aaa389faeeb71f3e": {
    "text": "ssl_ctx.load_cert_chain(certfile='/etc/ssl/certs/client.crt', keyfile='/etc/ssl/private/client.key',)\n\nhttp_client = httpx.AsyncClient(\n    verify=ssl_ctx,\n    timeout=httpx.Timeout(10.0),\n)\n\nserver = MCPServerSSE(\n    url='http://localhost:3001/sse',\n    http_client=http_client,  # (1)!\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\nasync def main():\n    async with agent:\n        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. When you supply `http_client`, Pydantic AI re-uses this client for every\n   request. Anything supported by **httpx** (`verify`, `cert`, custom\n   proxies, timeouts, etc.) therefore applies to all MCP traffic.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "OPTIONAL: if the server requires **mutual TLS** load your client certificate"
  },
  "b00cb6f7521d056341fb9d1788dbf153fc7b2c81905c760c3669043f59049470": {
    "text": "What is MCP Sampling?\n\nIn MCP [sampling](https://modelcontextprotocol.io/docs/concepts/sampling \"https://modelcontextprotocol.io/docs/concepts/sampling\") is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used.\n\nSampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls.\n\nConfusingly it has nothing to do with the concept of \"sampling\" in observability, or frankly the concept of \"sampling\" in any other domain.\n\nSampling Diagram\n\nHere's a mermaid diagram that may or may not make the data flow clearer:\n\n```\nsequenceDiagram\n    participant LLM\n    participant MCP_Client as MCP client\n    participant MCP_Server as MCP server\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM tool call response\n\n    MCP_Client->>MCP_Server: tool call\n    MCP_Server->>MCP_Client: sampling \"create message\"\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM text response\n\n    MCP_Client->>MCP_Server: sampling response\n    MCP_Server->>MCP_Client: tool call response\n```\n\nPydantic AI supports sampling as both a client and server. See the [server](../server/#mcp-sampling \"../server/#mcp-sampling\") documentation for details on how to use sampling within a server.\n\nSampling is automatically supported by Pydantic AI agents when they act as a client.\n\nTo be able to use sampling, an MCP server instance needs to have a [`sampling_model`](../../api/mcp/#pydantic_ai.mcp.MCPServer.sampling_model \"../../api/mcp/#pydantic_ai.mcp.MCPServer.sampling_model\") set. This can be done either directly on the server using the constructor keyword argument or the property, or by using [`agent.set_mcp_sampling_model()`](../../api/agent/#pydantic_ai.agent.Agent.set_mcp_sampling_model \"../../api/agent/#pydantic_ai.agent.Agent.set_mcp_sampling_model\") to set the agent's model or one specified as an argument as the sampling model on all MCP servers registered with that agent.\n\nLet's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments).\n\nSampling MCP Server\n\ngenerate\\_svg.py\n\n```\nimport re\nfrom pathlib import Path\n\nfrom mcp import SamplingMessage\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.types import TextContent\n\napp = FastMCP()\n\n\n@app.tool()\nasync def image_generator(ctx: Context, subject: str, style: str) -> str:\n    prompt = f'{subject=} {style=}'\n    # `ctx.session.create_message` is the sampling call\n    result = await ctx.session.create_message(\n        [SamplingMessage(role='user', content=TextContent(type='text', text=prompt))],\n        max_tokens=1_024,\n        system_prompt='Generate an SVG image as per the user input',\n    )\n    assert isinstance(result.content, TextContent)\n\n    path = Path(f'{subject}_{style}.svg')\n    # remove triple backticks if the svg was returned within markdown\n    if m := re.search(r'^```\\w*$(.+?)```$', result.content.text, re.S | re.M):\n        path.write_text(m.group(1))\n    else:\n        path.write_text(result.content.text)\n    return f'See {path}'\n\n\nif __name__ == '__main__':\n    # run the server via stdio\n    app.run()\n```\n\nUsing this server with an `Agent` will automatically allow sampling:\n\nsampling\\_mcp\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(command='python', args=['generate_svg.py'])\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n\nasync def main():\n    async with agent:\n        agent.set_mcp_sampling_model()\n        result = await agent.run('Create an image of a robot in a punk style.')\n    print(result.output)\n    #> Image file written to robot_punk.svg.\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can disallow sampling by setting [`allow_sampling=False`](../../api/mcp/#pydantic_ai.mcp.MCPServer.allow_sampling \"../../api/mcp/#pydantic_ai.mcp.MCPServer.allow_sampling\") when creating the server reference, e.g.:\n\nsampling\\_disallowed.py\n\n```\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    command='python',\n    args=['generate_svg.py'],\n    allow_sampling=False,\n)\n```",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "MCP Sampling"
  },
  "852f62b0744c3986114c052be70222da89716966985c02c1b57eaad0d945e0f6": {
    "text": "In MCP, [elicitation](https://modelcontextprotocol.io/docs/concepts/elicitation \"https://modelcontextprotocol.io/docs/concepts/elicitation\") allows a server to request for [structured input](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types \"https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types\") from the client for missing or additional context during a session.\n\nElicitation let models essentially say \"Hold on - I need to know X before i can continue\" rather than requiring everything upfront or taking a shot in the dark.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Elicitation"
  },
  "83b7c6121bc52ddb852f016fb82e3cd989acfa28f2f0ea986e559574774e0bd1": {
    "text": "Elicitation introduces a new protocol message type called [`ElicitRequest`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest \"https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest\"), which is sent from the server to the client when it needs additional information. The client can then respond with an [`ElicitResult`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult \"https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult\") or an `ErrorData` message.\n\nHere's a typical interaction:\n\n* User makes a request to the MCP server (e.g. \"Book a table at that Italian place\")\n* The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\")\n* The server sends an `ElicitRequest` to the client asking for the missing information.\n* The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface).\n* User provides the requested information, `decline` or `cancel` the request.\n* The client sends an `ElicitResult` back to the server with the user's response.\n* With the structured data, the server can continue processing the original request.\n\nThis allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "How Elicitation works"
  },
  "2dc74934bf144102b191133a38e1eba67295197fd69f00efb37036bc2fdfa02c": {
    "text": "To enable elicitation, provide an [`elicitation_callback`](../../api/mcp/#pydantic_ai.mcp.MCPServer.elicitation_callback \"../../api/mcp/#pydantic_ai.mcp.MCPServer.elicitation_callback\") function when creating your MCP server instance:\n\nrestaurant\\_server.py\n\n```\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom pydantic import BaseModel, Field\n\nmcp = FastMCP(name='Restaurant Booking')\n\n\nclass BookingDetails(BaseModel):\n    \"\"\"Schema for restaurant booking information.\"\"\"\n\n    restaurant: str = Field(description='Choose a restaurant')\n    party_size: int = Field(description='Number of people', ge=1, le=8)\n    date: str = Field(description='Reservation date (DD-MM-YYYY)')\n\n\n@mcp.tool()\nasync def book_table(ctx: Context) -> str:\n    \"\"\"Book a restaurant table with user input.\"\"\"\n    # Ask user for booking details using Pydantic schema\n    result = await ctx.elicit(message='Please provide your booking details:', schema=BookingDetails)\n\n    if result.action == 'accept' and result.data:\n        booking = result.data\n        return f'✅ Booked table for {booking.party_size} at {booking.restaurant} on {booking.date}'\n    elif result.action == 'decline':\n        return 'No problem! Maybe another time.'\n    else:  # cancel\n        return 'Booking cancelled.'\n\n\nif __name__ == '__main__':\n    mcp.run(transport='stdio')\n```\n\nThis server demonstrates elicitation by requesting structured booking details from the client when the `book_table` tool is called. Here's how to create a client that handles these elicitation requests:\n\nclient\\_example.py\n\n```\nimport asyncio\nfrom typing import Any\n\nfrom mcp.client.session import ClientSession\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\n\nasync def handle_elicitation(\n    context: RequestContext[ClientSession, Any, Any],\n    params: ElicitRequestParams,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP server.\"\"\"\n    print(f'\\n{params.message}')\n\n    if not params.requestedSchema:\n        response = input('Response: ')\n        return ElicitResult(action='accept', content={'response': response})\n\n    # Collect data for each field\n    properties = params.requestedSchema['properties']\n    data = {}\n\n    for field, info in properties.items():\n        description = info.get('description', field)\n\n        value = input(f'{description}: ')\n\n        # Convert to proper type based on JSON schema\n        if info.get('type') == 'integer':\n            data[field] = int(value)\n        else:\n            data[field] = value\n\n    # Confirm\n    confirm = input('\\nConfirm booking? (y/n/c): ').lower()\n\n    if confirm == 'y':\n        print('Booking details:', data)\n        return ElicitResult(action='accept', content=data)\n    elif confirm == 'n':\n        return ElicitResult(action='decline')\n    else:\n        return ElicitResult(action='cancel')",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Setting up Elicitation"
  },
  "d69a0157ef44ab72fdb7ee5ec99c7b18a3bb1a817b1805aa11d6b29a67f0ee6f": {
    "text": "restaurant_server = MCPServerStdio(\n    command='python', args=['restaurant_server.py'], elicitation_callback=handle_elicitation\n)",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Set up MCP server connection"
  },
  "26b21d68709dd12ef1b72864fa622d62c86859222e15a100d67466e9c12c0e36": {
    "text": "agent = Agent('openai:gpt-4o', toolsets=[restaurant_server])\n\n\nasync def main():\n    \"\"\"Run the agent to book a restaurant table.\"\"\"\n    async with agent:\n        result = await agent.run('Book me a table')\n        print(f'\\nResult: {result.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Create agent"
  },
  "dcf5eb89af314c4e7352a64f4c147991b47b879fe9b2d2cefc07caa29c0c98ef": {
    "text": "MCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types \"https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types\") for details.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Supported Schema Types"
  },
  "b4ccd208d04997fddff7a4bb5f5567f38d7f94287b8f0285c019344f40f5fc34": {
    "text": "MCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations \"https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations\") for details.",
    "source_url": "https://ai.pydantic.dev/mcp/client/",
    "header": "Security"
  },
  "52748567eb69b386d5e904187fdc8b883d1925d58ffac03e2ede3c5dfb6d3181": {
    "text": "Pydantic AI models can also be used within MCP Servers.",
    "source_url": "https://ai.pydantic.dev/mcp/server/",
    "header": "Server"
  },
  "8392f220de21ba4695257085fc58afba0b412d77ed3bda18d6b7249056002535": {
    "text": "Here's a simple example of a [Python MCP server](https://github.com/modelcontextprotocol/python-sdk \"https://github.com/modelcontextprotocol/python-sdk\") using Pydantic AI within a tool call:\n\nmcp\\_server.py\n\n```\nfrom mcp.server.fastmcp import FastMCP\n\nfrom pydantic_ai import Agent\n\nserver = FastMCP('Pydantic AI Server')\nserver_agent = Agent(\n    'anthropic:claude-3-5-haiku-latest', system_prompt='always reply in rhyme'\n)\n\n\n@server.tool()\nasync def poet(theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}')\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()\n```",
    "source_url": "https://ai.pydantic.dev/mcp/server/",
    "header": "MCP Server"
  },
  "348e5d316f9466c786980cd03f004072484517d15c8f38994b2789507a2a4001": {
    "text": "This server can be queried with any MCP client. Here is an example using the Python SDK directly:\n\nmcp\\_client.py\n\n```\nimport asyncio\nimport os\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n\nasync def client():\n    server_params = StdioServerParameters(\n        command='python', args=['mcp_server.py'], env=os.environ\n    )\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            \"\"\"\n            Oh, socks, those garments soft and sweet,\n            That nestle softly 'round our feet,\n            From cotton, wool, or blended thread,\n            They keep our toes from feeling dread.\n            \"\"\"\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```",
    "source_url": "https://ai.pydantic.dev/mcp/server/",
    "header": "Simple client"
  },
  "c37a488007da48b31b943acce98ba006e9645c474106826647abf6b9ca3d7fd4": {
    "text": "What is MCP Sampling?\n\nSee the [MCP client docs](../client/#mcp-sampling \"../client/#mcp-sampling\") for details of what MCP sampling is, and how you can support it when using Pydantic AI as an MCP client.\n\nWhen Pydantic AI agents are used within MCP servers, they can use sampling via [`MCPSamplingModel`](../../api/models/mcp-sampling/#pydantic_ai.models.mcp_sampling.MCPSamplingModel \"../../api/models/mcp-sampling/#pydantic_ai.models.mcp_sampling.MCPSamplingModel\").\n\nWe can extend the above example to use sampling so instead of connecting directly to the LLM, the agent calls back through the MCP client to make LLM calls.\n\nmcp\\_server\\_sampling.py\n\n```\nfrom mcp.server.fastmcp import Context, FastMCP\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mcp_sampling import MCPSamplingModel\n\nserver = FastMCP('Pydantic AI Server with sampling')\nserver_agent = Agent(system_prompt='always reply in rhyme')\n\n\n@server.tool()\nasync def poet(ctx: Context, theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}', model=MCPSamplingModel(session=ctx.session))\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()  # run the server over stdio\n```\n\nThe [above](#simple-client \"#simple-client\") client does not support sampling, so if you tried to use it with this server you'd get an error.\n\nThe simplest way to support sampling in an MCP client is to [use](../client/#mcp-sampling \"../client/#mcp-sampling\") a Pydantic AI agent as the client, but if you wanted to support sampling with the vanilla MCP SDK, you could do so like this:\n\nmcp\\_client\\_sampling.py\n\n```\nimport asyncio\nfrom typing import Any\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import (\n    CreateMessageRequestParams,\n    CreateMessageResult,\n    ErrorData,\n    TextContent,\n)\n\n\nasync def sampling_callback(\n    context: RequestContext[ClientSession, Any], params: CreateMessageRequestParams\n) -> CreateMessageResult | ErrorData:\n    print('sampling system prompt:', params.systemPrompt)\n    #> sampling system prompt: always reply in rhyme\n    print('sampling messages:', params.messages)\n    \"\"\"\n    sampling messages:\n    [\n        SamplingMessage(\n            role='user',\n            content=TextContent(\n                type='text',\n                text='write a poem about socks',\n                annotations=None,\n                meta=None,\n            ),\n        )\n    ]\n    \"\"\"\n\n    # TODO get the response content by calling an LLM...\n    response_content = 'Socks for a fox.'\n\n    return CreateMessageResult(\n        role='assistant',\n        content=TextContent(type='text', text=response_content),\n        model='fictional-llm',\n    )\n\n\nasync def client():\n    server_params = StdioServerParameters(command='python', args=['mcp_server_sampling.py'])\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write, sampling_callback=sampling_callback) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            #> Socks for a fox.\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```\n\n*(This example is complete, it can be run \"as is\")*",
    "source_url": "https://ai.pydantic.dev/mcp/server/",
    "header": "MCP Sampling"
  },
  "f60adc68a9478bc29e0ecf141dc12f4fb4ed8511bcf20a79629a0936acb1fed8": {
    "text": "The **MCP Run Python** package is an MCP server that allows agents to execute Python code in a secure, sandboxed environment. It uses [Pyodide](https://pyodide.org/ \"https://pyodide.org/\") to run Python code in a JavaScript environment with [Deno](https://deno.com/ \"https://deno.com/\"), isolating execution from the host system.",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "MCP Run Python"
  },
  "44c20bf418152b4ab519e6a349976ee2325d2cb029d9df6481fdf8443a7aeed7": {
    "text": "* **Secure Execution**: Run Python code in a sandboxed WebAssembly environment\n* **Package Management**: Automatically detects and installs required dependencies\n* **Complete Results**: Captures standard output, standard error, and return values\n* **Asynchronous Support**: Runs async code properly\n* **Error Handling**: Provides detailed error reports for debugging",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "Features"
  },
  "5f89b81ecfd568f0c847c8a2ae5ba3207357110d0d12ecc19147c026ecda22e9": {
    "text": "Switch from npx to deno\n\nWe previously distributed `mcp-run-python` as an `npm` package to use via `npx`.\nWe now recommend using `deno` instead as it provides better sandboxing and security.\n\nThe MCP Run Python server is distributed as a [JSR package](https://jsr.io/@pydantic/mcp-run-python \"https://jsr.io/@pydantic/mcp-run-python\") and can be run directly using [`deno run`](https://deno.com/ \"https://deno.com/\"):\n\nterminal\n\n```\ndeno run \\\n  -N -R=node_modules -W=node_modules --node-modules-dir=auto \\\n  jsr:@pydantic/mcp-run-python [stdio|streamable_http|sse|warmup]\n```\n\nwhere:\n\n* `-N -R=node_modules -W=node_modules` (alias of\n  `--allow-net --allow-read=node_modules --allow-write=node_modules`) allows\n  network access and read+write access to `./node_modules`. These are required\n  so Pyodide can download and cache the Python standard library and packages\n* `--node-modules-dir=auto` tells deno to use a local `node_modules` directory\n* `stdio` runs the server with the\n  [Stdio MCP transport](https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#stdio \"https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#stdio\") — suitable for\n  running the process as a subprocess locally\n* `streamable_http` runs the server with the\n  [Streamable HTTP MCP transport](https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#streamable-http \"https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#streamable-http\") -\n  suitable for running the server as an HTTP server to connect locally or remotely. This supports stateful requests, but does not require the client to hold a stateful connection like SSE\n* `sse` runs the server with the\n  [SSE MCP transport](https://modelcontextprotocol.io/specification/2024-11-05/basic/transports#http-with-sse \"https://modelcontextprotocol.io/specification/2024-11-05/basic/transports#http-with-sse\") —\n  suitable for running the server as an HTTP server to connect locally or remotely. Note that the SSE transport has been\n  [deprecated in newer MCP protocol versions](https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#backwards-compatibility \"https://modelcontextprotocol.io/specification/2025-06-18/basic/transports#backwards-compatibility\")\n  and is there to maintain backwards compatibility.\n* `warmup` will run a minimal Python script to download and cache the Python\n  standard library. This is also useful to check the server is running\n  correctly.\n\nUsage of `jsr:@pydantic/mcp-run-python` with Pydantic AI is described in the [client](../client/#mcp-stdio-server \"../client/#mcp-stdio-server\") documentation.",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "Installation"
  },
  "0a9dc24cb05254619e7120ee8c6b3b3dd72b00c29b72f07a990fa347448a48b4": {
    "text": "As well as using this server with Pydantic AI, it can be connected to other MCP clients. For clarity, in this example we connect directly using the [Python MCP client](https://github.com/modelcontextprotocol/python-sdk \"https://github.com/modelcontextprotocol/python-sdk\").\n\nmcp\\_run\\_python.py\n\n```\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\ncode = \"\"\"\nimport numpy\na = numpy.array([1, 2, 3])\nprint(a)\na\n\"\"\"\nserver_params = StdioServerParameters(\n    command='deno',\n    args=[\n        'run',\n        '-N',\n        '-R=node_modules',\n        '-W=node_modules',\n        '--node-modules-dir=auto',\n        'jsr:@pydantic/mcp-run-python',\n        'stdio',\n    ],\n)\n\n\nasync def main():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            tools = await session.list_tools()\n            print(len(tools.tools))\n            #> 1\n            print(repr(tools.tools[0].name))\n            #> 'run_python_code'\n            print(repr(tools.tools[0].inputSchema))\n            \"\"\"\n            {'type': 'object', 'properties': {'python_code': {'type': 'string', 'description': 'Python code to run'}}, 'required': ['python_code'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n            \"\"\"\n            result = await session.call_tool('run_python_code', {'python_code': code})\n            print(result.content[0].text)\n            \"\"\"\n            <status>success</status>\n            <dependencies>[\"numpy\"]</dependencies>\n            <output>\n            [1 2 3]\n            </output>\n            <return_value>\n            [\n              1,\n              2,\n              3\n            ]\n            </return_value>\n            \"\"\"\n```\n\nIf an exception occurs, `status` will be `install-error` or `run-error` and `return_value` will be replaced\nby `error` which will include the traceback and exception message.",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "Direct Usage"
  },
  "bbeb1170cecb90fd07737dd0fc771f7f051247f934bbb66f05e48ea2043f5984": {
    "text": "Dependencies are installed when code is run.\n\nDependencies can be defined in one of two ways:",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "Dependencies"
  },
  "7d9ee66b948214ab76c2801cbb04ad933f4df0484faa8e60bf29c885d52f4511": {
    "text": "If there's no metadata, dependencies are inferred from imports in the code,\nas shown in the example [above](#direct-usage \"#direct-usage\").\n\nAs introduced in PEP 723, explained [here](https://packaging.python.org/en/latest/specifications/inline-script-metadata/#inline-script-metadata \"https://packaging.python.org/en/latest/specifications/inline-script-metadata/#inline-script-metadata\"), and popularized by [uv](https://docs.astral.sh/uv/guides/scripts/#declaring-script-dependencies \"https://docs.astral.sh/uv/guides/scripts/#declaring-script-dependencies\") — dependencies can be defined in a comment at the top of the file.\n\nThis allows use of dependencies that aren't imported in the code, and is more explicit.\n\ninline\\_script\\_metadata.py\n\n```\nfrom mcp import ClientSession\nfrom mcp.client.stdio import stdio_client",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "Inferred from imports"
  },
  "31035f854f0496da30460df1961848420da6b8779dbfa429360a1cfd1f4cf600": {
    "text": "from mcp_run_python import server_params\n\ncode = \"\"\"\\",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "using `server_params` from the above example."
  },
  "a22997fd571ad7d33aa03ef3afd71429912a46d8a541bed6ec2893ad98d59ec8": {
    "text": "import pydantic\n\nclass Model(pydantic.BaseModel):\n    email: pydantic.EmailStr\n\nprint(Model(email='hello@pydantic.dev'))\n\"\"\"\n\n\nasync def main():\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            result = await session.call_tool('run_python_code', {'python_code': code})\n            print(result.content[0].text)\n            \"\"\"\n            <status>success</status>\n            <dependencies>[\"pydantic\",\"email-validator\"]</dependencies>\n            <output>\n            email='hello@pydantic.dev'\n            </output>\n            \"\"\"\n```\n\nIt also allows versions to be pinned for non-binary packages (Pyodide only supports a single version for the binary packages it supports, like `pydantic` and `numpy`).\n\nE.g. you could set the dependencies to\n\n```",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "///"
  },
  "504010d327b6416b6f4ffbdc99b785ff27945dbb69cdacbc1b6bcba1e8cef0e3": {
    "text": "```",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "///"
  },
  "735a73a3f4f73c878dc600c7df8216b00d22f0ce078f20d77781e60e3fc03545": {
    "text": "MCP Run Python supports emitting stdout and stderr from the python execution as [MCP logging messages](https://github.com/modelcontextprotocol/specification/blob/eb4abdf2bb91e0d5afd94510741eadd416982350/docs/specification/draft/server/utilities/logging.md?plain=1 \"https://github.com/modelcontextprotocol/specification/blob/eb4abdf2bb91e0d5afd94510741eadd416982350/docs/specification/draft/server/utilities/logging.md?plain=1\").\n\nFor logs to be emitted you must set the logging level when connecting to the server. By default, the log level is set to the highest level, `emergency`.\n\nCurrently, it's not possible to demonstrate this due to a bug in the Python MCP Client, see [modelcontextprotocol/python-sdk#201](https://github.com/modelcontextprotocol/python-sdk/issues/201#issuecomment-2727663121 \"https://github.com/modelcontextprotocol/python-sdk/issues/201#issuecomment-2727663121\").",
    "source_url": "https://ai.pydantic.dev/mcp/run-python/",
    "header": "Logging"
  },
  "c833848c3ec54eb73611b1ad18a23d8840115580bb4909739c7275177bb51d7d": {
    "text": "The [Agent2Agent (A2A) Protocol](https://google.github.io/A2A/ \"https://google.github.io/A2A/\") is an open standard introduced by Google that enables\ncommunication and interoperability between AI agents, regardless of the framework or vendor they are built on.\n\nAt Pydantic, we built the [FastA2A](#fasta2a \"#fasta2a\") library to make it easier to implement the A2A protocol in Python.\n\nWe also built a convenience method that expose Pydantic AI agents as A2A servers - let's have a quick look at how to use it:\n\nagent\\_to\\_a2a.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\napp = agent.to_a2a()\n```\n\n*You can run the example with `uvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000`*\n\nThis will expose the agent as an A2A server, and you can start sending requests to it.\n\nSee more about [exposing Pydantic AI agents as A2A servers](#pydantic-ai-agent-to-a2a-server \"#pydantic-ai-agent-to-a2a-server\").",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "Agent2Agent (A2A) Protocol"
  },
  "f75d20e6015f13464025f7affc17f94de08eb3e7ffaefd44b11766452eabd4f4": {
    "text": "**FastA2A** is an agentic framework agnostic implementation of the A2A protocol in Python.\nThe library is designed to be used with any agentic framework, and is **not exclusive to Pydantic AI**.",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "FastA2A"
  },
  "d91aec1c169f8d023c052920994baccb1202eb95f3f0c279a7e573059a9945c9": {
    "text": "**FastA2A** is built on top of [Starlette](https://www.starlette.io \"https://www.starlette.io\"), which means it's fully compatible with any ASGI server.\n\nGiven the nature of the A2A protocol, it's important to understand the design before using it, as a developer\nyou'll need to provide some components:\n\n* [`Storage`](../api/fasta2a/#fasta2a.Storage \"../api/fasta2a/#fasta2a.Storage\"): to save and load tasks, as well as store context for conversations\n* [`Broker`](../api/fasta2a/#fasta2a.Broker \"../api/fasta2a/#fasta2a.Broker\"): to schedule tasks\n* [`Worker`](../api/fasta2a/#fasta2a.Worker \"../api/fasta2a/#fasta2a.Worker\"): to execute tasks\n\nLet's have a look at how those components fit together:\n\n```\nflowchart TB\n    Server[\"HTTP Server\"] <--> |Sends Requests/<br>Receives Results| TM\n\n    subgraph CC[Core Components]\n        direction RL\n        TM[\"TaskManager<br>(coordinates)\"] --> |Schedules Tasks| Broker\n        TM <--> Storage\n        Broker[\"Broker<br>(queues & schedules)\"] <--> Storage[\"Storage<br>(persistence)\"]\n        Broker --> |Delegates Execution| Worker\n    end\n\n    Worker[\"Worker<br>(implementation)\"]\n```\n\nFastA2A allows you to bring your own [`Storage`](../api/fasta2a/#fasta2a.Storage \"../api/fasta2a/#fasta2a.Storage\"), [`Broker`](../api/fasta2a/#fasta2a.Broker \"../api/fasta2a/#fasta2a.Broker\") and [`Worker`](../api/fasta2a/#fasta2a.Worker \"../api/fasta2a/#fasta2a.Worker\").",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "Design"
  },
  "b6bde8b11e2b3b7c8c5af26cf3827387dd6c77d9d605f6822ae710f984d72bb6": {
    "text": "In the A2A protocol:\n\n* **Task**: Represents one complete execution of an agent. When a client sends a message to the agent, a new task is created. The agent runs until completion (or failure), and this entire execution is considered one task. The final output is stored as a task artifact.\n* **Context**: Represents a conversation thread that can span multiple tasks. The A2A protocol uses a `context_id` to maintain conversation continuity:\n* When a new message is sent without a `context_id`, the server generates a new one\n* Subsequent messages can include the same `context_id` to continue the conversation\n* All tasks sharing the same `context_id` have access to the complete message history",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "Understanding Tasks and Context"
  },
  "3dbf0ab3f451e6f3dc6793878566ffd2f25d3e545d5e33bb4c87af7b04aad874": {
    "text": "The [`Storage`](../api/fasta2a/#fasta2a.Storage \"../api/fasta2a/#fasta2a.Storage\") component serves two purposes:\n\n1. **Task Storage**: Stores tasks in A2A protocol format, including their status, artifacts, and message history\n2. **Context Storage**: Stores conversation context in a format optimized for the specific agent implementation\n\nThis design allows for agents to store rich internal state (e.g., tool calls, reasoning traces) as well as store task-specific A2A-formatted messages and artifacts.\n\nFor example, a Pydantic AI agent might store its complete internal message format (including tool calls and responses) in the context storage, while storing only the A2A-compliant messages in the task history.",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "Storage Architecture"
  },
  "be907b4b976fdfe10c42133903d4d17bfebf9106ff46830114c39412e9767b28": {
    "text": "FastA2A is available on PyPI as [`fasta2a`](https://pypi.org/project/fasta2a/ \"https://pypi.org/project/fasta2a/\") so installation is as simple as:\n\nThe only dependencies are:\n\nYou can install Pydantic AI with the `a2a` extra to include **FastA2A**:",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "Installation"
  },
  "6edd9c52a657bba53bb2b2011bdba87b288334773107522a9d6434a3353e6579": {
    "text": "To expose a Pydantic AI agent as an A2A server, you can use the `to_a2a` method:\n\nagent\\_to\\_a2a.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\napp = agent.to_a2a()\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server.\n\n```\nuvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000\n```\n\nSince the goal of `to_a2a` is to be a convenience method, it accepts the same arguments as the [`FastA2A`](../api/fasta2a/#fasta2a.FastA2A \"../api/fasta2a/#fasta2a.FastA2A\") constructor.\n\nWhen using `to_a2a()`, Pydantic AI automatically:\n\n* Stores the complete conversation history (including tool calls and responses) in the context storage\n* Ensures that subsequent messages with the same `context_id` have access to the full conversation history\n* Persists agent results as A2A artifacts:\n* String results become `TextPart` artifacts and also appear in the message history\n* Structured data (Pydantic models, dataclasses, tuples, etc.) become `DataPart` artifacts with the data wrapped as `{\"result\": <your_data>}`\n* Artifacts include metadata with type information and JSON schema when available",
    "source_url": "https://ai.pydantic.dev/a2a/",
    "header": "Pydantic AI Agent to A2A Server"
  },
  "9980689ebc2bab00ac5dcb968331c63836eec60c380da170f4f11526b787dd35": {
    "text": "The [Agent User Interaction (AG-UI) Protocol](https://docs.ag-ui.com/introduction \"https://docs.ag-ui.com/introduction\") is an open standard introduced by the\n[CopilotKit](https://webflow.copilotkit.ai/blog/introducing-ag-ui-the-protocol-where-agents-meet-users \"https://webflow.copilotkit.ai/blog/introducing-ag-ui-the-protocol-where-agents-meet-users\")\nteam that standardises how frontend applications communicate with AI agents, with support for streaming, frontend tools, shared state, and custom events.\n\nNote\n\nThe AG-UI integration was originally built by the team at [Rocket Science](https://www.rocketscience.gg/ \"https://www.rocketscience.gg/\") and contributed in collaboration with the Pydantic AI and CopilotKit teams. Thanks Rocket Science!",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Agent User Interaction (AG-UI) Protocol"
  },
  "2b1248183d6c4762acf1c0da571d6231c2603e603f926a8f11364ca4e384cdf7": {
    "text": "The only dependencies are:\n\nYou can install Pydantic AI with the `ag-ui` extra to ensure you have all the\nrequired AG-UI dependencies:\n\nTo run the examples you'll also need:\n\n* [uvicorn](https://www.uvicorn.org/ \"https://www.uvicorn.org/\") or another ASGI compatible server",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Installation"
  },
  "bae9e6dea67f7da04f3d9468bdf3efccf8bf032b8dc07bf142af7fffa91d53a8": {
    "text": "There are three ways to run a Pydantic AI agent based on AG-UI run input with streamed AG-UI events as output, from most to least flexible. If you're using a Starlette-based web framework like FastAPI, you'll typically want to use the second method.\n\n1. [`run_ag_ui()`](../api/ag_ui/#pydantic_ai.ag_ui.run_ag_ui \"../api/ag_ui/#pydantic_ai.ag_ui.run_ag_ui\") takes an agent and an AG-UI [`RunAgentInput`](https://docs.ag-ui.com/sdk/python/core/types#runagentinput \"https://docs.ag-ui.com/sdk/python/core/types#runagentinput\") object, and returns a stream of AG-UI events encoded as strings. It also takes optional [`Agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\") arguments including `deps`. Use this if you're using a web framework not based on Starlette (e.g. Django or Flask) or want to modify the input or output some way.\n2. [`handle_ag_ui_request()`](../api/ag_ui/#pydantic_ai.ag_ui.handle_ag_ui_request \"../api/ag_ui/#pydantic_ai.ag_ui.handle_ag_ui_request\") takes an agent and a Starlette request (e.g. from FastAPI) coming from an AG-UI frontend, and returns a streaming Starlette response of AG-UI events that you can return directly from your endpoint. It also takes optional [`Agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\") arguments including `deps`, that you can vary for each request (e.g. based on the authenticated user).\n3. [`Agent.to_ag_ui()`](../api/agent/#pydantic_ai.agent.AbstractAgent.to_ag_ui \"../api/agent/#pydantic_ai.agent.AbstractAgent.to_ag_ui\") returns an ASGI application that handles every AG-UI request by running the agent. It also takes optional [`Agent.iter()`](../api/agent/#pydantic_ai.agent.Agent.iter \"../api/agent/#pydantic_ai.agent.Agent.iter\") arguments including `deps`, but these will be the same for each request, with the exception of the AG-UI state that's injected as described under [state management](#state-management \"#state-management\"). This ASGI app can be [mounted](https://fastapi.tiangolo.com/advanced/sub-applications/ \"https://fastapi.tiangolo.com/advanced/sub-applications/\") at a given path in an existing FastAPI app.",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Usage"
  },
  "10e956479f3d05a0bace9f9af4b1efdfbc6a0122bca132fa640acea8b7844c6e": {
    "text": "This example uses [`run_ag_ui()`](../api/ag_ui/#pydantic_ai.ag_ui.run_ag_ui \"../api/ag_ui/#pydantic_ai.ag_ui.run_ag_ui\") and performs its own request parsing and response generation.\nThis can be modified to work with any web framework.\n\nrun\\_ag\\_ui.py\n\n```\nimport json\nfrom http import HTTPStatus\n\nfrom ag_ui.core import RunAgentInput\nfrom fastapi import FastAPI\nfrom fastapi.requests import Request\nfrom fastapi.responses import Response, StreamingResponse\nfrom pydantic import ValidationError\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import SSE_CONTENT_TYPE, run_ag_ui\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\n\napp = FastAPI()\n\n\n@app.post('/')\nasync def run_agent(request: Request) -> Response:\n    accept = request.headers.get('accept', SSE_CONTENT_TYPE)\n    try:\n        run_input = RunAgentInput.model_validate(await request.json())\n    except ValidationError as e:  # pragma: no cover\n        return Response(\n            content=json.dumps(e.json()),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    event_stream = run_ag_ui(agent, run_input, accept=accept)\n\n    return StreamingResponse(event_stream, media_type=accept)\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\nThis will expose the agent as an AG-UI server, and your frontend can start sending requests to it.",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Handle run input and output directly"
  },
  "36d952d95cd1e75877cf834690f794a0fabfb9f14d4de482de5191a48907bedc": {
    "text": "This example uses [`handle_ag_ui_request()`](../api/ag_ui/#pydantic_ai.ag_ui.run_ag_ui \"../api/ag_ui/#pydantic_ai.ag_ui.run_ag_ui\") to directly handle a FastAPI request and return a response. Something analogous to this will work with any Starlette-based web framework.\n\nhandle\\_ag\\_ui\\_request.py\n\n```\nfrom fastapi import FastAPI\nfrom starlette.requests import Request\nfrom starlette.responses import Response\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import handle_ag_ui_request\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\n\napp = FastAPI()\n\n@app.post('/')\nasync def run_agent(request: Request) -> Response:\n    return await handle_ag_ui_request(agent, request)\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```\nuvicorn handle_ag_ui_request:app\n```\n\nThis will expose the agent as an AG-UI server, and your frontend can start sending requests to it.",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Handle a Starlette request"
  },
  "a5379f7d759e039de3a725c8078f60d744fff014de617118ecba810b961f0e86": {
    "text": "This example uses [`Agent.to_ag_ui()`](../api/agent/#pydantic_ai.agent.AbstractAgent.to_ag_ui \"../api/agent/#pydantic_ai.agent.AbstractAgent.to_ag_ui\") to turn the agent into a stand-alone ASGI application:\n\nagent\\_to\\_ag\\_ui.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='Be fun!')\napp = agent.to_ag_ui()\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```\nuvicorn agent_to_ag_ui:app\n```\n\nThis will expose the agent as an AG-UI server, and your frontend can start sending requests to it.",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Stand-alone ASGI app"
  },
  "6be66c919805524d4179e09d344b880d1c18e34d3f92f99efa20da61aab63481": {
    "text": "The Pydantic AI AG-UI integration supports all features of the spec:\n\nThe integration receives messages in the form of a\n[`RunAgentInput`](https://docs.ag-ui.com/sdk/python/core/types#runagentinput \"https://docs.ag-ui.com/sdk/python/core/types#runagentinput\") object\nthat describes the details of the requested agent run including message history, state, and available tools.\n\nThese are converted to Pydantic AI types and passed to the agent's run method. Events from the agent, including tool calls, are converted to AG-UI events and streamed back to the caller as Server-Sent Events (SSE).\n\nA user request may require multiple round trips between client UI and Pydantic AI\nserver, depending on the tools and events needed.",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Design"
  },
  "92b3a64a45d37493577a25d9b64831af1bb7840bb2e87e5f5488226ff7d85c2b": {
    "text": "The integration provides full support for\n[AG-UI state management](https://docs.ag-ui.com/concepts/state \"https://docs.ag-ui.com/concepts/state\"), which enables\nreal-time synchronization between agents and frontend applications.\n\nIn the example below we have document state which is shared between the UI and\nserver using the [`StateDeps`](../api/ag_ui/#pydantic_ai.ag_ui.StateDeps \"../api/ag_ui/#pydantic_ai.ag_ui.StateDeps\") [dependencies type](../dependencies/ \"../dependencies/\") that can be used to automatically\nvalidate state contained in [`RunAgentInput.state`](https://docs.ag-ui.com/sdk/js/core/types#runagentinput \"https://docs.ag-ui.com/sdk/js/core/types#runagentinput\") using a Pydantic `BaseModel` specified as a generic parameter.\n\nCustom dependencies type with AG-UI state\n\nIf you want to use your own dependencies type to hold AG-UI state as well as other things, it needs to implements the\n[`StateHandler`](../api/ag_ui/#pydantic_ai.ag_ui.StateHandler \"../api/ag_ui/#pydantic_ai.ag_ui.StateHandler\") protocol, meaning it needs to be a [dataclass](https://docs.python.org/3/library/dataclasses.html \"https://docs.python.org/3/library/dataclasses.html\") with a non-optional `state` field. This lets Pydantic AI ensure that state is properly isolated between requests by building a new dependencies object each time.\n\nIf the `state` field's type is a Pydantic `BaseModel` subclass, the raw state dictionary on the request is automatically validated. If not, you can validate the raw value yourself in your dependencies dataclass's `__post_init__` method.\n\nag\\_ui\\_state.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent(\n    'openai:gpt-4.1',\n    instructions='Be fun!',\n    deps_type=StateDeps[DocumentState],\n)\napp = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```\nuvicorn ag_ui_state:app --host 0.0.0.0 --port 9000\n```\n\nAG-UI frontend tools are seamlessly provided to the Pydantic AI agent, enabling rich\nuser experiences with frontend user interfaces.",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "State management"
  },
  "e019145f831a6469e8592193045b544972219ef1c0f47c2639d131ecfe9fe20e": {
    "text": "Pydantic AI tools can send\n[AG-UI events](https://docs.ag-ui.com/concepts/events \"https://docs.ag-ui.com/concepts/events\") simply by defining a tool\nwhich returns a (subclass of)\n[`BaseEvent`](https://docs.ag-ui.com/sdk/python/core/events#baseevent \"https://docs.ag-ui.com/sdk/python/core/events#baseevent\"), which allows\nfor custom events and state updates.\n\nag\\_ui\\_tool\\_events.py\n\n```\nfrom ag_ui.core import CustomEvent, EventType, StateSnapshotEvent\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent(\n    'openai:gpt-4.1',\n    instructions='Be fun!',\n    deps_type=StateDeps[DocumentState],\n)\napp = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n\n\n@agent.tool\nasync def update_state(ctx: RunContext[StateDeps[DocumentState]]) -> StateSnapshotEvent:\n    return StateSnapshotEvent(\n        type=EventType.STATE_SNAPSHOT,\n        snapshot=ctx.deps.state,\n    )\n\n\n@agent.tool_plain\nasync def custom_events() -> list[CustomEvent]:\n    return [\n        CustomEvent(\n            type=EventType.CUSTOM,\n            name='count',\n            value=1,\n        ),\n        CustomEvent(\n            type=EventType.CUSTOM,\n            name='count',\n            value=2,\n        ),\n    ]\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```\nuvicorn ag_ui_tool_events:app --host 0.0.0.0 --port 9000\n```",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Events"
  },
  "b29f90a9e22f55c28a840334c6e1a17152256144f99c946ca02129f9b4b8f881": {
    "text": "For more examples of how to use [`to_ag_ui()`](../api/agent/#pydantic_ai.agent.AbstractAgent.to_ag_ui \"../api/agent/#pydantic_ai.agent.AbstractAgent.to_ag_ui\") see\n[`pydantic_ai_examples.ag_ui`](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/ag_ui \"https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/ag_ui\"),\nwhich includes a server for use with the\n[AG-UI Dojo](https://docs.ag-ui.com/tutorials/debugging#the-ag-ui-dojo \"https://docs.ag-ui.com/tutorials/debugging#the-ag-ui-dojo\").",
    "source_url": "https://ai.pydantic.dev/ag-ui/",
    "header": "Examples"
  },
  "418c0dd4ec5f44665e84aff15441f5d8c1a7d68a14674b20b93a43cb9a891354": {
    "text": "**Pydantic AI** comes with a CLI, `clai` (pronounced \"clay\") which you can use to interact with various LLMs from the command line.\nIt provides a convenient way to chat with language models and quickly get answers right in the terminal.\n\nWe originally developed this CLI for our own use, but found ourselves using it so frequently that we decided to share it as part of the Pydantic AI package.\n\nWe plan to continue adding new features, such as interaction with MCP servers, access to tools, and more.",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Command Line Interface (CLI)"
  },
  "1234094a1b49a372d8eae03406fa0f0d2b4ac09c72840e1d4b8788ab56446e03": {
    "text": "You'll need to set an environment variable depending on the provider you intend to use.\n\nE.g. if you're using OpenAI, set the `OPENAI_API_KEY` environment variable:\n\n```\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\nThen with [`uvx`](https://docs.astral.sh/uv/guides/tools/ \"https://docs.astral.sh/uv/guides/tools/\"), run:\n\nOr to install `clai` globally [with `uv`](https://docs.astral.sh/uv/guides/tools/#installing-tools \"https://docs.astral.sh/uv/guides/tools/#installing-tools\"), run:\n\n```\nuv tool install clai\n...\nclai\n```\n\nOr with `pip`, run:\n\n```\npip install clai\n...\nclai\n```\n\nEither way, running `clai` will start an interactive session where you can chat with the AI model. Special commands available in interactive mode:\n\n* `/exit`: Exit the session\n* `/markdown`: Show the last response in markdown format\n* `/multiline`: Toggle multiline input mode (use Ctrl+D to submit)\n* `/cp`: Copy the last response to clipboard",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Usage"
  },
  "625c8dacbc25e2e9a54deb2933d746bcc03d41eb753ccea3c57eeb977a9927d5": {
    "text": "To get help on the CLI, use the `--help` flag:",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Help"
  },
  "bbb6b52ebad74e45e01e493f2e7afe28e4a9654d3d55aa67b63801a20f5cca40": {
    "text": "You can specify which model to use with the `--model` flag:\n\n```\nuvx clai --model anthropic:claude-sonnet-4-0\n```\n\n(a full list of models available can be printed with `uvx clai --list-models`)",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Choose a model"
  },
  "959180c35063bdaeeddb6fb4b01b0aaae56901bf493277a6bddc3d560b15089c": {
    "text": "You can specify a custom agent using the `--agent` flag with a module path and variable name:\n\ncustom\\_agent.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='You always respond in Italian.')\n```\n\nThen run:\n\n```\nuvx clai --agent custom_agent:agent \"What's the weather today?\"\n```\n\nThe format must be `module:variable` where:\n\n* `module` is the importable Python module path\n* `variable` is the name of the Agent instance in that module\n\nAdditionally, you can directly launch CLI mode from an `Agent` instance using `Agent.to_cli_sync()`:\n\nagent\\_to\\_cli\\_sync.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='You always respond in Italian.')\nagent.to_cli_sync()\n```\n\nYou can also use the async interface with `Agent.to_cli()`:\n\nagent\\_to\\_cli.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4.1', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n```\n\n*(You'll need to add `asyncio.run(main())` to run `main`)*",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Custom Agents"
  },
  "db1609628bcae454c0dd847bcaeed4d21721a28c518565462f9c165f2cdfdc75": {
    "text": "Both `Agent.to_cli()` and `Agent.to_cli_sync()` support a `message_history` parameter, allowing you to continue an existing conversation or provide conversation context:\n\nagent\\_with\\_history.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.messages import (\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\nagent = Agent('openai:gpt-4.1')",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Message History"
  },
  "761187999b28422e2ba7a676be4f63e9b6427059a2eb337073fb3dfdde7eca19": {
    "text": "message_history: list[ModelMessage] = [\n    ModelRequest([UserPromptPart(content='What is 2+2?')]),\n    ModelResponse([TextPart(content='2+2 equals 4.')])\n]",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Create some conversation history"
  },
  "3fccdb7509dddf9b690bc8ec6849f785790476bfe82af6a0b24dc97bc1a4bf3c": {
    "text": "agent.to_cli_sync(message_history=message_history)\n```\n\nThe CLI will start with the provided conversation history, allowing the agent to refer back to previous exchanges and maintain context throughout the session.",
    "source_url": "https://ai.pydantic.dev/cli/",
    "header": "Start CLI with existing conversation context"
  },
  "c0d852ba4e272b1a400478f1f7f2fc5852aaabe0c8ee6a658cb7b180ceb9ac4b": {
    "text": "Examples of how to use Pydantic AI and what it can do.",
    "source_url": "https://ai.pydantic.dev/examples/",
    "header": "Examples"
  },
  "6be96ea2075d8b6d34f2d2368a7f23852f7d783e673da72ec76f5ee52972d6b6": {
    "text": "These examples are distributed with `pydantic-ai` so you can run them either by cloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai \"https://github.com/pydantic/pydantic-ai\") or by simply installing `pydantic-ai` from PyPI with `pip` or `uv`.",
    "source_url": "https://ai.pydantic.dev/examples/",
    "header": "Usage"
  },
  "4703e17639b5b83417341279e80c9dfaf2d77d720f83836710bb036e07294aac": {
    "text": "Either way you'll need to install extra dependencies to run some examples, you just need to install the `examples` optional dependency group.\n\nIf you've installed `pydantic-ai` via pip/uv, you can install the extra dependencies with:\n\nIf you clone the repo, you should instead use `uv sync --extra examples` to install extra dependencies.",
    "source_url": "https://ai.pydantic.dev/examples/",
    "header": "Installing required dependencies"
  },
  "de530f092051b0106e3f810b70bade49a6ea0d61c60afb2c0d7b85fb35195509": {
    "text": "These examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](../models/ \"../models/\") docs for details on how to do this.\n\nTL;DR: in most cases you'll need to set one of the following environment variables:",
    "source_url": "https://ai.pydantic.dev/examples/",
    "header": "Setting model environment variables"
  },
  "010721c482d954752447933d21d632e5cd583a55e54e692ed09d05ca395a1652": {
    "text": "To run the examples (this will work whether you installed `pydantic_ai`, or cloned the repo), run:\n\nFor examples, to run the very simple [`pydantic_model`](pydantic-model/ \"pydantic-model/\") example:\n\nIf you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup:\n\n```\nOPENAI_API_KEY='your-api-key' \\\n  uv run --with \"pydantic-ai[examples]\" \\\n  -m pydantic_ai_examples.pydantic_model\n```\n\n---\n\nYou'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with:",
    "source_url": "https://ai.pydantic.dev/examples/",
    "header": "Running Examples"
  },
  "42db7787d04d829736b4153f934f8257cb272e137da8153ee75898829de659b9": {
    "text": "Example of using Pydantic AI agents with the [AG-UI Dojo](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo \"https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo\") example app.\n\nSee the [AG-UI docs](../../ag-ui/ \"../../ag-ui/\") for more information about the AG-UI integration.\n\nDemonstrates:",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Agent User Interaction (AG-UI)"
  },
  "355a6b92382ceb276a1630f2631a2ed2450820434a3c80ba96bc5cc16646d768": {
    "text": "With [dependencies installed and environment variables set](../#usage \"../#usage\")\nyou will need two command line windows.",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Running the Example"
  },
  "b470c6d8f761c3488301f7e7425e65a17c058fa350666dfd55e369563c0be97c": {
    "text": "Setup your OpenAI API Key\n\n```\nexport OPENAI_API_KEY=<your api key>\n```\n\nStart the Pydantic AI AG-UI example backend.",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Pydantic AI AG-UI backend"
  },
  "d254cb6dc7b538f478c691bc83fb62bb4bd0b570ca84381a1a21f7abaca1852c": {
    "text": "Next run the AG-UI Dojo example frontend.\n\n1. Clone the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui \"https://github.com/ag-ui-protocol/ag-ui\")\n\n   ```\n   git clone https://github.com/ag-ui-protocol/ag-ui.git\n   ```\n2. Change into to the `ag-ui/typescript-sdk` directory\n3. Run the Dojo app following the [official instructions](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup \"https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup\")\n4. Visit [http://localhost:3000/pydantic-ai](http://localhost:3000/pydantic-ai \"http://localhost:3000/pydantic-ai\")\n5. Select View `Pydantic AI` from the sidebar",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "AG-UI Dojo example frontend"
  },
  "c2f9464641e8ed017cc105f38eb130c02383b4d78ab333fa1059f9d946f6b55d": {
    "text": "This demonstrates a basic agent interaction including Pydantic AI server side\ntools and AG-UI client side tools.\n\nIf you've [run the example](#running-the-example \"#running-the-example\"), you can view it at [http://localhost:3000/pydantic-ai/feature/agentic\\_chat](http://localhost:3000/pydantic-ai/feature/agentic_chat \"http://localhost:3000/pydantic-ai/feature/agentic_chat\").\n\n* `time` - Pydantic AI tool to check the current time for a time zone\n* `background` - AG-UI tool to set the background color of the client window",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Agentic Chat"
  },
  "92b161c055baff3e64e8b1157048dd031e71499255e4e47598a51e522af5a74d": {
    "text": "```\nWhat is the time in New York?\n```\n\n```\nChange the background to blue\n```\n\nA complex example which mixes both AG-UI and Pydantic AI tools:\n\n```\nPerform the following steps, waiting for the response of each step before continuing:\n1. Get the time\n2. Set the background to red\n3. Get the time\n4. Report how long the background set took by diffing the two times\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Agent Prompts"
  },
  "4d8de08996f034f111ce6f2ce3658312b1f8485cb6955c6f102c0f3f941310fe": {
    "text": "[ag\\_ui/api/agentic\\_chat.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py\")\n\n```\n\"\"\"Agentic Chat feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\nfrom zoneinfo import ZoneInfo\n\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o-mini')\napp = agent.to_ag_ui()\n\n\n@agent.tool_plain\nasync def current_time(timezone: str = 'UTC') -> str:\n    \"\"\"Get the current time in ISO format.\n\n    Args:\n        timezone: The timezone to use.\n\n    Returns:\n        The current time in ISO format string.\n    \"\"\"\n    tz: ZoneInfo = ZoneInfo(timezone)\n    return datetime.now(tz=tz).isoformat()\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Agentic Chat - Code"
  },
  "26fbe11c6bfd6c76adf46ffc58fac5f3e58a2ec29447e901de3c73613e1d4799": {
    "text": "Demonstrates a long running task where the agent sends updates to the frontend\nto let the user know what's happening.\n\nIf you've [run the example](#running-the-example \"#running-the-example\"), you can view it at [http://localhost:3000/pydantic-ai/feature/agentic\\_generative\\_ui](http://localhost:3000/pydantic-ai/feature/agentic_generative_ui \"http://localhost:3000/pydantic-ai/feature/agentic_generative_ui\").",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Agentic Generative UI"
  },
  "fa49c8b8d38b98c9dc8e7af1b10c8edd5bacd09e595344dade7a22507f354fcb": {
    "text": "[ag\\_ui/api/agentic\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py\")\n\n```\n\"\"\"Agentic Generative UI feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom textwrap import dedent\nfrom typing import Any, Literal\n\nfrom pydantic import BaseModel, Field\n\nfrom ag_ui.core import EventType, StateDeltaEvent, StateSnapshotEvent\nfrom pydantic_ai import Agent\n\nStepStatus = Literal['pending', 'completed']\n\n\nclass Step(BaseModel):\n    \"\"\"Represents a step in a plan.\"\"\"\n\n    description: str = Field(description='The description of the step')\n    status: StepStatus = Field(\n        default='pending',\n        description='The status of the step (e.g., pending, completed)',\n    )\n\n\nclass Plan(BaseModel):\n    \"\"\"Represents a plan with multiple steps.\"\"\"\n\n    steps: list[Step] = Field(default_factory=list, description='The steps in the plan')\n\n\nclass JSONPatchOp(BaseModel):\n    \"\"\"A class representing a JSON Patch operation (RFC 6902).\"\"\"\n\n    op: Literal['add', 'remove', 'replace', 'move', 'copy', 'test'] = Field(\n        description='The operation to perform: add, remove, replace, move, copy, or test',\n    )\n    path: str = Field(description='JSON Pointer (RFC 6901) to the target location')\n    value: Any = Field(\n        default=None,\n        description='The value to apply (for add, replace operations)',\n    )\n    from_: str | None = Field(\n        default=None,\n        alias='from',\n        description='Source path (for move, copy operations)',\n    )\n\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    instructions=dedent(\n        \"\"\"\n        When planning use tools only, without any other messages.\n        IMPORTANT:\n        - Use the `create_plan` tool to set the initial state of the steps\n        - Use the `update_plan_step` tool to update the status of each step\n        - Do NOT repeat the plan or summarise it in a message\n        - Do NOT confirm the creation or updates in a message\n        - Do NOT ask the user for additional information or next steps\n\n        Only one plan can be active at a time, so do not call the `create_plan` tool\n        again until all the steps in current plan are completed.\n        \"\"\"\n    ),\n)\n\n\n@agent.tool_plain\nasync def create_plan(steps: list[str]) -> StateSnapshotEvent:\n    \"\"\"Create a plan with multiple steps.\n\n    Args:\n        steps: List of step descriptions to create the plan.\n\n    Returns:\n        StateSnapshotEvent containing the initial state of the steps.\n    \"\"\"\n    plan: Plan = Plan(\n        steps=[Step(description=step) for step in steps],\n    )\n    return StateSnapshotEvent(\n        type=EventType.STATE_SNAPSHOT,\n        snapshot=plan.model_dump(),\n    )\n\n\n@agent.tool_plain\nasync def update_plan_step(\n    index: int, description: str | None = None, status: StepStatus | None = None\n) -> StateDeltaEvent:\n    \"\"\"Update the plan with new steps or changes.\n\n    Args:\n        index: The index of the step to update.\n        description: The new description for the step.\n        status: The new status for the step.\n\n    Returns:\n        StateDeltaEvent containing the changes made to the plan.\n    \"\"\"\n    changes: list[JSONPatchOp] = []\n    if description is not None:\n        changes.append(\n            JSONPatchOp(\n                op='replace', path=f'/steps/{index}/description', value=description\n            )\n        )\n    if status is not None:\n        changes.append(\n            JSONPatchOp(op='replace', path=f'/steps/{index}/status', value=status)\n        )\n    return StateDeltaEvent(\n        type=EventType.STATE_DELTA,\n        delta=changes,\n    )\n\n\napp = agent.to_ag_ui()\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Agentic Generative UI - Code"
  },
  "4f5d6c97fc96a850f9b24d75ab1b5a9a0fa678cf2c22b01d84726676baab69d4": {
    "text": "Demonstrates simple human in the loop workflow where the agent comes up with a\nplan and the user can approve it using checkboxes.\n\n* `generate_task_steps` - AG-UI tool to generate and confirm steps",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Human in the Loop"
  },
  "ff6f0a07e27c941236ed7893593d8ac3db1ddb9d3a11699d3489d25179a7238e": {
    "text": "[ag\\_ui/api/human\\_in\\_the\\_loop.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py\")\n\n```\n\"\"\"Human in the Loop Feature.\n\nNo special handling is required for this feature.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom textwrap import dedent\n\nfrom pydantic_ai import Agent\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    instructions=dedent(\n        \"\"\"\n        When planning tasks use tools only, without any other messages.\n        IMPORTANT:\n        - Use the `generate_task_steps` tool to display the suggested steps to the user\n        - Never repeat the plan, or send a message detailing steps\n        - If accepted, confirm the creation of the plan and the number of selected (enabled) steps only\n        - If not accepted, ask the user for more information, DO NOT use the `generate_task_steps` tool again\n        \"\"\"\n    ),\n)\n\napp = agent.to_ag_ui()\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Human in the Loop - Code"
  },
  "d1c09a520825a145b7e8c1c558268cb6c3e01f296484cf23a85495420895f67a": {
    "text": "Demonstrates how to use the predictive state updates feature to update the state\nof the UI based on agent responses, including user interaction via user\nconfirmation.\n\nIf you've [run the example](#running-the-example \"#running-the-example\"), you can view it at [http://localhost:3000/pydantic-ai/feature/predictive\\_state\\_updates](http://localhost:3000/pydantic-ai/feature/predictive_state_updates \"http://localhost:3000/pydantic-ai/feature/predictive_state_updates\").",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Predictive State Updates"
  },
  "742c90cd9159d1c27ebfe2421be0f2a0dd412301cd9091399ed691e5e23eb692": {
    "text": "* `write_document` - AG-UI tool to write the document to a window\n* `document_predict_state` - Pydantic AI tool that enables document state\n  prediction for the `write_document` tool\n\nThis also shows how to use custom instructions based on shared state information.",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Story Tools"
  },
  "adee1a1348aa58a8943021f0a66369539eb9d46397f0970f336b5a770db52fbd": {
    "text": "Starting document text\n\nAgent prompt\n\n```\nHelp me complete my story about bruce the dog, is should be no longer than a sentence.\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Story Example"
  },
  "c5ad85e1fb23e549d0140c54a4743a7a054bbecb57565271165c287a4d76fa6f": {
    "text": "[ag\\_ui/api/predictive\\_state\\_updates.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py\")\n\n```\n\"\"\"Predictive State feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom textwrap import dedent\n\nfrom pydantic import BaseModel\n\nfrom ag_ui.core import CustomEvent, EventType\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent('openai:gpt-4o-mini', deps_type=StateDeps[DocumentState])",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Predictive State Updates - Code"
  },
  "18ee5fd0c42ac71d75225c47027e5c8ef67d8edd6415b990130aa3288a3b1703": {
    "text": "@agent.tool_plain\nasync def document_predict_state() -> list[CustomEvent]:\n    \"\"\"Enable document state prediction.\n\n    Returns:\n        CustomEvent containing the event to enable state prediction.\n    \"\"\"\n    return [\n        CustomEvent(\n            type=EventType.CUSTOM,\n            name='PredictState',\n            value=[\n                {\n                    'state_key': 'document',\n                    'tool': 'write_document',\n                    'tool_argument': 'document',\n                },\n            ],\n        ),\n    ]\n\n\n@agent.instructions()\nasync def story_instructions(ctx: RunContext[StateDeps[DocumentState]]) -> str:\n    \"\"\"Provide instructions for writing document if present.\n\n    Args:\n        ctx: The run context containing document state information.\n\n    Returns:\n        Instructions string for the document writing agent.\n    \"\"\"\n    return dedent(\n        f\"\"\"You are a helpful assistant for writing documents.\n\n        Before you start writing, you MUST call the `document_predict_state`\n        tool to enable state prediction.\n\n        To present the document to the user for review, you MUST use the\n        `write_document` tool.\n\n        When you have written the document, DO NOT repeat it as a message.\n        If accepted briefly summarize the changes you made, 2 sentences\n        max, otherwise ask the user to clarify what they want to change.\n\n        This is the current document:\n\n        {ctx.deps.state.document}\n        \"\"\"\n    )\n\n\napp = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "event stream, single events and iterables of events are supported."
  },
  "a1b9fb2d7c5d64672005e20990876d201860b28a2c2f78b17783330213b8da07": {
    "text": "Demonstrates how to use the shared state between the UI and the agent.\n\nState sent to the agent is detected by a function based instruction. This then\nvalidates the data using a custom pydantic model before using to create the\ninstructions for the agent to follow and send to the client using a AG-UI tool.\n\nIf you've [run the example](#running-the-example \"#running-the-example\"), you can view it at [http://localhost:3000/pydantic-ai/feature/shared\\_state](http://localhost:3000/pydantic-ai/feature/shared_state \"http://localhost:3000/pydantic-ai/feature/shared_state\").\n\n* `display_recipe` - AG-UI tool to display the recipe in a graphical format",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Shared State"
  },
  "70880bc2db03d44ba82b2c49836b18e1e752c5fc736253fa4a99d6815a55aec3": {
    "text": "1. Customise the basic settings of your recipe\n2. Click `Improve with AI`",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Recipe Example"
  },
  "a820f58a0dd89dea1b16fa2e005b00b8f50fbff22fe193717a54abac0deb1f13": {
    "text": "[ag\\_ui/api/shared\\_state.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/shared_state.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/shared_state.py\")\n\n```\n\"\"\"Shared State feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom enum import StrEnum\nfrom textwrap import dedent\n\nfrom pydantic import BaseModel, Field\n\nfrom ag_ui.core import EventType, StateSnapshotEvent\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass SkillLevel(StrEnum):\n    \"\"\"The level of skill required for the recipe.\"\"\"\n\n    BEGINNER = 'Beginner'\n    INTERMEDIATE = 'Intermediate'\n    ADVANCED = 'Advanced'\n\n\nclass SpecialPreferences(StrEnum):\n    \"\"\"Special preferences for the recipe.\"\"\"\n\n    HIGH_PROTEIN = 'High Protein'\n    LOW_CARB = 'Low Carb'\n    SPICY = 'Spicy'\n    BUDGET_FRIENDLY = 'Budget-Friendly'\n    ONE_POT_MEAL = 'One-Pot Meal'\n    VEGETARIAN = 'Vegetarian'\n    VEGAN = 'Vegan'\n\n\nclass CookingTime(StrEnum):\n    \"\"\"The cooking time of the recipe.\"\"\"\n\n    FIVE_MIN = '5 min'\n    FIFTEEN_MIN = '15 min'\n    THIRTY_MIN = '30 min'\n    FORTY_FIVE_MIN = '45 min'\n    SIXTY_PLUS_MIN = '60+ min'\n\n\nclass Ingredient(BaseModel):\n    \"\"\"A class representing an ingredient in a recipe.\"\"\"\n\n    icon: str = Field(\n        default='ingredient',\n        description=\"The icon emoji (not emoji code like '\\x1f35e', but the actual emoji like 🥕) of the ingredient\",\n    )\n    name: str\n    amount: str\n\n\nclass Recipe(BaseModel):\n    \"\"\"A class representing a recipe.\"\"\"\n\n    skill_level: SkillLevel = Field(\n        default=SkillLevel.BEGINNER,\n        description='The skill level required for the recipe',\n    )\n    special_preferences: list[SpecialPreferences] = Field(\n        default_factory=list,\n        description='Any special preferences for the recipe',\n    )\n    cooking_time: CookingTime = Field(\n        default=CookingTime.FIVE_MIN, description='The cooking time of the recipe'\n    )\n    ingredients: list[Ingredient] = Field(\n        default_factory=list,\n        description='Ingredients for the recipe',\n    )\n    instructions: list[str] = Field(\n        default_factory=list, description='Instructions for the recipe'\n    )\n\n\nclass RecipeSnapshot(BaseModel):\n    \"\"\"A class representing the state of the recipe.\"\"\"\n\n    recipe: Recipe = Field(\n        default_factory=Recipe, description='The current state of the recipe'\n    )\n\n\nagent = Agent('openai:gpt-4o-mini', deps_type=StateDeps[RecipeSnapshot])\n\n\n@agent.tool_plain\nasync def display_recipe(recipe: Recipe) -> StateSnapshotEvent:\n    \"\"\"Display the recipe to the user.\n\n    Args:\n        recipe: The recipe to display.\n\n    Returns:\n        StateSnapshotEvent containing the recipe snapshot.\n    \"\"\"\n    return StateSnapshotEvent(\n        type=EventType.STATE_SNAPSHOT,\n        snapshot={'recipe': recipe},\n    )\n\n\n@agent.instructions\nasync def recipe_instructions(ctx: RunContext[StateDeps[RecipeSnapshot]]) -> str:\n    \"\"\"Instructions for the recipe generation agent.\n\n    Args:\n        ctx: The run context containing recipe state information.\n\n    Returns:\n        Instructions string for the recipe generation agent.\n    \"\"\"\n    return dedent(\n        f\"\"\"\n        You are a helpful assistant for creating recipes.\n\n        IMPORTANT:\n        - Create a complete recipe using the existing ingredients\n        - Append new ingredients to the existing ones\n        - Use the `display_recipe` tool to present the recipe to the user\n        - Do NOT repeat the recipe in the message, use the tool instead\n        - Do NOT run the `display_recipe` tool multiple times in a row\n\n        Once you have created the updated recipe and displayed it to the user,\n        summarise the changes in one sentence, don't describe the recipe in\n        detail or send it as a message to the user.\n\n        The current state of the recipe is:\n\n        {ctx.deps.state.recipe.model_dump_json(indent=2)}\n        \"\"\",\n    )\n\n\napp = agent.to_ag_ui(deps=StateDeps(RecipeSnapshot()))\n```\n\nDemonstrates customised rendering for tool output with used confirmation.\n\nIf you've [run the example](#running-the-example \"#running-the-example\"), you can view it at [http://localhost:3000/pydantic-ai/feature/tool\\_based\\_generative\\_ui](http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui \"http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui\").\n\n* `generate_haiku` - AG-UI tool to display a haiku in English and Japanese",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Shared State - Code"
  },
  "f479767c27c1b90b7f5d675c84c43d81288d1394782b07a7cc248ce8770b0bff": {
    "text": "```\nGenerate a haiku about formula 1\n```\n\n[ag\\_ui/api/tool\\_based\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py\")\n\n```\n\"\"\"Tool Based Generative UI feature.\n\nNo special handling is required for this feature.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o-mini')\napp = agent.to_ag_ui()\n```",
    "source_url": "https://ai.pydantic.dev/examples/ag-ui/",
    "header": "Haiku Prompt"
  },
  "7a888c118de76404387f89f4ef5b17523c1bd37a1d37c61a8958d6984d3d7bc4": {
    "text": "Simple example of using Pydantic AI to construct a Pydantic model from a text input.\n\nDemonstrates:",
    "source_url": "https://ai.pydantic.dev/examples/pydantic-model/",
    "header": "Pydantic Model"
  },
  "acd85d56580dd4d3188aab199bebc5e4a52535c45f5867b0947787ced1b613fd": {
    "text": "With [dependencies installed and environment variables set](../#usage \"../#usage\"), run:\n\nThis examples uses `openai:gpt-4o` by default, but it works well with other models, e.g. you can run it\nwith Gemini using:\n\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)",
    "source_url": "https://ai.pydantic.dev/examples/pydantic-model/",
    "header": "Running the Example"
  },
  "9afc337a230f231afcaa082f9f068dcbe095f58870e2ae66339a6ce2e309fbb7": {
    "text": "[pydantic\\_model.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/pydantic_model.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/pydantic_model.py\")\n\n```\n\"\"\"Simple example of using Pydantic AI to construct a Pydantic model from a text input.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.pydantic_model\n\"\"\"\n\nimport os\n\nimport logfire\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent",
    "source_url": "https://ai.pydantic.dev/examples/pydantic-model/",
    "header": "Example Code"
  },
  "2de378621cae8c1082be755049d0e8bc6be321b955f5433d4f93dcfab00db3bb": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass MyModel(BaseModel):\n    city: str\n    country: str\n\n\nmodel = os.getenv('PYDANTIC_AI_MODEL', 'openai:gpt-4o')\nprint(f'Using model: {model}')\nagent = Agent(model, output_type=MyModel)\n\nif __name__ == '__main__':\n    result = agent.run_sync('The windy city in the US of A.')\n    print(result.output)\n    print(result.usage())\n```",
    "source_url": "https://ai.pydantic.dev/examples/pydantic-model/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  },
  "3003d23ea9c324123b45deddeccaadec182dc96b0e87c5c4d7ac15d6cbff09df": {
    "text": "Example of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nDemonstrates:\n\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in multiple locations,\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\nthe `get_weather` tool to get the weather for those locations.",
    "source_url": "https://ai.pydantic.dev/examples/weather-agent/",
    "header": "Weather agent"
  },
  "28e4eb1fd8ecb06ceee761b344b493d25a9b7f7e042133fc1656f2375cf53596": {
    "text": "To run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)**:\n\nWith [dependencies installed and environment variables set](../#usage \"../#usage\"), run:",
    "source_url": "https://ai.pydantic.dev/examples/weather-agent/",
    "header": "Running the Example"
  },
  "deabc320b3b8cf446448cdbfd31b964c47355dbbd45ee143285a4d2231544449": {
    "text": "[weather\\_agent.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent.py\")\n\n```\n\"\"\"Example of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in multiple cities,\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\nthe `get_weather` tool to get the weather.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.weather_agent\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext",
    "source_url": "https://ai.pydantic.dev/examples/weather-agent/",
    "header": "Example Code"
  },
  "7b2086e60fde853dd0c5b82c1ecdd32d985db622447e9a5a492c4a6c76cc0146": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n\n\nweather_agent = Agent(\n    'openai:gpt-4.1-mini',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    instructions='Be concise, reply with one sentence.',\n    deps_type=Deps,\n    retries=2,\n)\n\n\nclass LatLng(BaseModel):\n    lat: float\n    lng: float\n\n\n@weather_agent.tool\nasync def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> LatLng:\n    \"\"\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \"\"\"\n    # NOTE: the response here will be random, and is not related to the location description.\n    r = await ctx.deps.client.get(\n        'https://demo-endpoints.pydantic.workers.dev/latlng',\n        params={'location': location_description},\n    )\n    r.raise_for_status()\n    return LatLng.model_validate_json(r.content)\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \"\"\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \"\"\"\n    # NOTE: the responses here will be random, and are not related to the lat and lng.\n    temp_response, descr_response = await asyncio.gather(\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/number',\n            params={'min': 10, 'max': 30},\n        ),\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/weather',\n            params={'lat': lat, 'lng': lng},\n        ),\n    )\n    temp_response.raise_for_status()\n    descr_response.raise_for_status()\n    return {\n        'temperature': f'{temp_response.text} °C',\n        'description': descr_response.text,\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        logfire.instrument_httpx(client, capture_all=True)\n        deps = Deps(client=client)\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        print('Response:', result.output)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```",
    "source_url": "https://ai.pydantic.dev/examples/weather-agent/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  },
  "59192cf44c79cc475d866fe5cb9d3c631003741ad1d53babc1a31e87ef78e8f6": {
    "text": "You can build multi-turn chat applications for your agent with [Gradio](https://www.gradio.app/ \"https://www.gradio.app/\"), a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\n\nHere's what the UI looks like for the weather agent:\n\n```\npip install gradio>=5.9.0\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\n```",
    "source_url": "https://ai.pydantic.dev/examples/weather-agent/",
    "header": "Running the UI"
  },
  "d3bccb29fa00437af2b1e85786ac650360672a9f030a2659f846d4aa3286e460": {
    "text": "[weather\\_agent\\_gradio.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent_gradio.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent_gradio.py\")\n\n```\nfrom __future__ import annotations as _annotations\n\nimport json\n\nfrom httpx import AsyncClient\n\nfrom pydantic_ai.messages import ToolCallPart, ToolReturnPart\nfrom pydantic_ai_examples.weather_agent import Deps, weather_agent\n\ntry:\n    import gradio as gr\nexcept ImportError as e:\n    raise ImportError(\n        'Please install gradio with `pip install gradio`. You must use python>=3.10.'\n    ) from e\n\nTOOL_TO_DISPLAY_NAME = {'get_lat_lng': 'Geocoding API', 'get_weather': 'Weather API'}\n\nclient = AsyncClient()\ndeps = Deps(client=client)\n\n\nasync def stream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\n    chatbot.append({'role': 'user', 'content': prompt})\n    yield gr.Textbox(interactive=False, value=''), chatbot, gr.skip()\n    async with weather_agent.run_stream(\n        prompt, deps=deps, message_history=past_messages\n    ) as result:\n        for message in result.new_messages():\n            for call in message.parts:\n                if isinstance(call, ToolCallPart):\n                    call_args = call.args_as_json_str()\n                    metadata = {\n                        'title': f'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}',\n                    }\n                    if call.tool_call_id is not None:\n                        metadata['id'] = call.tool_call_id\n\n                    gr_message = {\n                        'role': 'assistant',\n                        'content': 'Parameters: ' + call_args,\n                        'metadata': metadata,\n                    }\n                    chatbot.append(gr_message)\n                if isinstance(call, ToolReturnPart):\n                    for gr_message in chatbot:\n                        if (\n                            gr_message.get('metadata', {}).get('id', '')\n                            == call.tool_call_id\n                        ):\n                            gr_message['content'] += (\n                                f'\\nOutput: {json.dumps(call.content)}'\n                            )\n                yield gr.skip(), chatbot, gr.skip()\n        chatbot.append({'role': 'assistant', 'content': ''})\n        async for message in result.stream_text():\n            chatbot[-1]['content'] = message\n            yield gr.skip(), chatbot, gr.skip()\n        past_messages = result.all_messages()\n\n        yield gr.Textbox(interactive=True), gr.skip(), past_messages\n\n\nasync def handle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\n    new_history = chatbot[: retry_data.index]\n    previous_prompt = chatbot[retry_data.index]['content']\n    past_messages = past_messages[: retry_data.index]\n    async for update in stream_from_agent(previous_prompt, new_history, past_messages):\n        yield update\n\n\ndef undo(chatbot, past_messages: list, undo_data: gr.UndoData):\n    new_history = chatbot[: undo_data.index]\n    past_messages = past_messages[: undo_data.index]\n    return chatbot[undo_data.index]['content'], new_history, past_messages\n\n\ndef select_data(message: gr.SelectData) -> str:\n    return message.value['text']\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n        \"\"\"\n<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\n    <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\n    <div>\n        <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\n        <h3 style=\"margin: 0 0 0.5rem 0\">\n            This assistant answer your weather questions.\n        </h3>\n    </div>\n</div>\n\"\"\"\n    )\n    past_messages = gr.State([])\n    chatbot = gr.Chatbot(\n        label='Packing Assistant',\n        type='messages',\n        avatar_images=(None, 'https://ai.pydantic.dev/img/logo-white.svg'),\n        examples=[\n            {'text': 'What is the weather like in Miami?'},\n            {'text': 'What is the weather like in London?'},\n        ],\n    )\n    with gr.Row():\n        prompt = gr.Textbox(\n            lines=1,\n            show_label=False,\n            placeholder='What is the weather like in New York City?',\n        )\n    generation = prompt.submit(\n        stream_from_agent,\n        inputs=[prompt, chatbot, past_messages],\n        outputs=[prompt, chatbot, past_messages],\n    )\n    chatbot.example_select(select_data, None, [prompt])\n    chatbot.retry(\n        handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\n    )\n    chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\n\n\nif __name__ == '__main__':\n    demo.launch()\n```",
    "source_url": "https://ai.pydantic.dev/examples/weather-agent/",
    "header": "UI Code"
  },
  "b8c304efe19ad23c314cac2ea4c5eebb24d4e0190cdf9afe843a4e609acaed2e": {
    "text": "Small but complete example of using Pydantic AI to build a support agent for a bank.\n\nDemonstrates:",
    "source_url": "https://ai.pydantic.dev/examples/bank-support/",
    "header": "Bank support"
  },
  "10d491f7102d0355fcdf9ca31180ad7cc0069ce8ab6a8447030d3e3dc71b4d3c": {
    "text": "With [dependencies installed and environment variables set](../#usage \"../#usage\"), run:\n\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)",
    "source_url": "https://ai.pydantic.dev/examples/bank-support/",
    "header": "Running the Example"
  },
  "c72040794b14cf56360a662b35ddd336ed3fa3bc07e22f5ad81331a3d57f13be": {
    "text": "[bank\\_support.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/bank_support.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/bank_support.py\")\n\n```\n\"\"\"Small but complete example of using Pydantic AI to build a support agent for a bank.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.bank_support\n\"\"\"\n\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_ai import Agent, RunContext\n\n\nclass DatabaseConn:\n    \"\"\"This is a fake database for example purposes.\n\n    In reality, you'd be connecting to an external database\n    (e.g. PostgreSQL) to get information about customers.\n    \"\"\"\n\n    @classmethod\n    async def customer_name(cls, *, id: int) -> str | None:\n        if id == 123:\n            return 'John'\n\n    @classmethod\n    async def customer_balance(cls, *, id: int, include_pending: bool) -> float:\n        if id == 123:\n            if include_pending:\n                return 123.45\n            else:\n                return 100.00\n        else:\n            raise ValueError('Customer not found')\n\n\n@dataclass\nclass SupportDependencies:\n    customer_id: int\n    db: DatabaseConn\n\n\nclass SupportOutput(BaseModel):\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description='Whether to block their card or not')\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\nsupport_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query. '\n        \"Reply using the customer's name.\"\n    ),\n)\n\n\n@support_agent.system_prompt\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n@support_agent.tool\nasync def customer_balance(\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> str:\n    \"\"\"Returns the customer's current account balance.\"\"\"\n    balance = await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n    return f'${balance:.2f}'\n\n\nif __name__ == '__main__':\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = support_agent.run_sync('What is my balance?', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = support_agent.run_sync('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```",
    "source_url": "https://ai.pydantic.dev/examples/bank-support/",
    "header": "Example Code"
  },
  "295af2e992169a201056cb4df366126373aeb2567fb8dab4802e8f8598e0aa20": {
    "text": "Example demonstrating how to use Pydantic AI to generate SQL queries based on user input.\n\nDemonstrates:",
    "source_url": "https://ai.pydantic.dev/examples/sql-gen/",
    "header": "SQL Generation"
  },
  "ccff126fa4bc45f520cb07ab26e963a072ee330b92bd8463d8c4a4b9b1207cfe": {
    "text": "The resulting SQL is validated by running it as an `EXPLAIN` query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker:\n\n```\ndocker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n```\n\n*(we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running)*\n\nWith [dependencies installed and environment variables set](../#usage \"../#usage\"), run:\n\nor to use a custom prompt:\n\nThis model uses `gemini-1.5-flash` by default since Gemini is good at single shot queries of this kind.",
    "source_url": "https://ai.pydantic.dev/examples/sql-gen/",
    "header": "Running the Example"
  },
  "20d6f9ed58f0d5fa74b99441183fbc56f30a5f2d4c9cba8a9113a24d073edf65": {
    "text": "[sql\\_gen.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/sql_gen.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/sql_gen.py\")\n\n```\n\"\"\"Example demonstrating how to use Pydantic AI to generate SQL queries based on user input.\n\nRun postgres with:\n\n    mkdir postgres-data\n    docker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n\nRun with:\n\n    uv run -m pydantic_ai_examples.sql_gen \"show me logs from yesterday, with level 'error'\"\n\"\"\"\n\nimport asyncio\nimport sys\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom datetime import date\nfrom typing import Annotated, Any, TypeAlias\n\nimport asyncpg\nimport logfire\nfrom annotated_types import MinLen\nfrom devtools import debug\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext, format_as_xml",
    "source_url": "https://ai.pydantic.dev/examples/sql-gen/",
    "header": "Example Code"
  },
  "90b9e82303a9b61aa1b8283c7d4252e2afb2a2c02accef64fc992cd0c4f71eeb": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_asyncpg()\nlogfire.instrument_pydantic_ai()\n\nDB_SCHEMA = \"\"\"\nCREATE TABLE records (\n    created_at timestamptz,\n    start_timestamp timestamptz,\n    end_timestamp timestamptz,\n    trace_id text,\n    span_id text,\n    parent_span_id text,\n    level log_level,\n    span_name text,\n    message text,\n    attributes_json_schema text,\n    attributes jsonb,\n    tags text[],\n    is_exception boolean,\n    otel_status_message text,\n    service_name text\n);\n\"\"\"\nSQL_EXAMPLES = [\n    {\n        'request': 'show me records where foobar is false',\n        'response': \"SELECT * FROM records WHERE attributes->>'foobar' = false\",\n    },\n    {\n        'request': 'show me records where attributes include the key \"foobar\"',\n        'response': \"SELECT * FROM records WHERE attributes ? 'foobar'\",\n    },\n    {\n        'request': 'show me records from yesterday',\n        'response': \"SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL '1 day'\",\n    },\n    {\n        'request': 'show me error records with the tag \"foobar\"',\n        'response': \"SELECT * FROM records WHERE level = 'error' and 'foobar' = ANY(tags)\",\n    },\n]\n\n\n@dataclass\nclass Deps:\n    conn: asyncpg.Connection\n\n\nclass Success(BaseModel):\n    \"\"\"Response when SQL could be successfully generated.\"\"\"\n\n    sql_query: Annotated[str, MinLen(1)]\n    explanation: str = Field(\n        '', description='Explanation of the SQL query, as markdown'\n    )\n\n\nclass InvalidRequest(BaseModel):\n    \"\"\"Response the user input didn't include enough information to generate SQL.\"\"\"\n\n    error_message: str\n\n\nResponse: TypeAlias = Success | InvalidRequest\nagent = Agent[Deps, Response](\n    'google-gla:gemini-1.5-flash',\n    # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else\n    output_type=Response,  # type: ignore\n    deps_type=Deps,\n)\n\n\n@agent.system_prompt\nasync def system_prompt() -> str:\n    return f\"\"\"\\\nGiven the following PostgreSQL table of records, your job is to\nwrite a SQL query that suits the user's request.\n\nDatabase schema:\n\n{DB_SCHEMA}\n\ntoday's date = {date.today()}\n\n{format_as_xml(SQL_EXAMPLES)}\n\"\"\"\n\n\n@agent.output_validator\nasync def validate_output(ctx: RunContext[Deps], output: Response) -> Response:\n    if isinstance(output, InvalidRequest):\n        return output\n\n    # gemini often adds extraneous backslashes to SQL\n    output.sql_query = output.sql_query.replace('\\\\', '')\n    if not output.sql_query.upper().startswith('SELECT'):\n        raise ModelRetry('Please create a SELECT query')\n\n    try:\n        await ctx.deps.conn.execute(f'EXPLAIN {output.sql_query}')\n    except asyncpg.exceptions.PostgresError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\nasync def main():\n    if len(sys.argv) == 1:\n        prompt = 'show me logs from yesterday, with level \"error\"'\n    else:\n        prompt = sys.argv[1]\n\n    async with database_connect(\n        'postgresql://postgres:postgres@localhost:54320', 'pydantic_ai_sql_gen'\n    ) as conn:\n        deps = Deps(conn)\n        result = await agent.run(prompt, deps=deps)\n    debug(result.output)",
    "source_url": "https://ai.pydantic.dev/examples/sql-gen/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  },
  "0d7fc3f764ded5de612bd1cd5956f6107fe68cd9af4a0700df1f940c2a2a1ea8": {
    "text": "@asynccontextmanager\nasync def database_connect(server_dsn: str, database: str) -> AsyncGenerator[Any, None]:\n    with logfire.span('check and create DB'):\n        conn = await asyncpg.connect(server_dsn)\n        try:\n            db_exists = await conn.fetchval(\n                'SELECT 1 FROM pg_database WHERE datname = $1', database\n            )\n            if not db_exists:\n                await conn.execute(f'CREATE DATABASE {database}')\n        finally:\n            await conn.close()\n\n    conn = await asyncpg.connect(f'{server_dsn}/{database}')\n    try:\n        with logfire.span('create schema'):\n            async with conn.transaction():\n                if not db_exists:\n                    await conn.execute(\n                        \"CREATE TYPE log_level AS ENUM ('debug', 'info', 'warning', 'error', 'critical')\"\n                    )\n                    await conn.execute(DB_SCHEMA)\n        yield conn\n    finally:\n        await conn.close()\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```",
    "source_url": "https://ai.pydantic.dev/examples/sql-gen/",
    "header": "pyright: reportUnknownVariableType=false"
  },
  "984874614878bd5497d0a325d94454303dbbf40190ba4cd25af8eba52fee1fbf": {
    "text": "Example of a multi-agent flow where one agent delegates work to another, then hands off control to a third agent.\n\nDemonstrates:\n\nIn this scenario, a group of agents work together to find the best flight for a user.\n\nThe control flow for this example can be summarised as follows:\n\n```\ngraph TD\n  START --> search_agent(\"search agent\")\n  search_agent --> extraction_agent(\"extraction agent\")\n  extraction_agent --> search_agent\n  search_agent --> human_confirm(\"human confirm\")\n  human_confirm --> search_agent\n  search_agent --> FAILED\n  human_confirm --> find_seat_function(\"find seat function\")\n  find_seat_function --> human_seat_choice(\"human seat choice\")\n  human_seat_choice --> find_seat_agent(\"find seat agent\")\n  find_seat_agent --> find_seat_function\n  find_seat_function --> buy_flights(\"buy flights\")\n  buy_flights --> SUCCESS\n```",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "Flight booking"
  },
  "de2303424a1baaca4e420b60559c30435221bed33379c68ae49b499a69bf33ec": {
    "text": "With [dependencies installed and environment variables set](../#usage \"../#usage\"), run:",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "Running the Example"
  },
  "69c74f1c12e0df5891ec0a69cecf49061969cb7f217d815478661c92314a53d2": {
    "text": "[flight\\_booking.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/flight_booking.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/flight_booking.py\")\n\n```\n\"\"\"Example of a multi-agent flow where one agent delegates work to another.\n\nIn this scenario, a group of agents work together to find flights for a user.\n\"\"\"\n\nimport datetime\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nimport logfire\nfrom pydantic import BaseModel, Field\nfrom rich.prompt import Prompt\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext, RunUsage, UsageLimits\nfrom pydantic_ai.messages import ModelMessage",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "Example Code"
  },
  "4e29cc07a77a1a58c0e33a21a6f4e1a6914d14d8033da02f4ae22f58a1157701": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass FlightDetails(BaseModel):\n    \"\"\"Details of the most suitable flight.\"\"\"\n\n    flight_number: str\n    price: int\n    origin: str = Field(description='Three-letter airport code')\n    destination: str = Field(description='Three-letter airport code')\n    date: datetime.date\n\n\nclass NoFlightFound(BaseModel):\n    \"\"\"When no valid flight is found.\"\"\"\n\n\n@dataclass\nclass Deps:\n    web_page_text: str\n    req_origin: str\n    req_destination: str\n    req_date: datetime.date",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  },
  "1f4058f9fa2c1f0eb439de1b8fac6e6f30ab3c6887a38b3669832182a0c605d5": {
    "text": "search_agent = Agent[Deps, FlightDetails | NoFlightFound](\n    'openai:gpt-4o',\n    output_type=FlightDetails | NoFlightFound,  # type: ignore\n    retries=4,\n    system_prompt=(\n        'Your job is to find the cheapest flight for the user on the given date. '\n    ),\n)",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "This agent is responsible for controlling the flow of the conversation."
  },
  "3e39161e06a60ce4c2a025239833d09dc85c5d9f9cb33b3d11bfc208ef8363eb": {
    "text": "extraction_agent = Agent(\n    'openai:gpt-4o',\n    output_type=list[FlightDetails],\n    system_prompt='Extract all the flight details from the given text.',\n)\n\n\n@search_agent.tool\nasync def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:\n    \"\"\"Get details of all flights.\"\"\"\n    # we pass the usage to the search agent so requests within this agent are counted\n    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)\n    logfire.info('found {flight_count} flights', flight_count=len(result.output))\n    return result.output\n\n\n@search_agent.output_validator\nasync def validate_output(\n    ctx: RunContext[Deps], output: FlightDetails | NoFlightFound\n) -> FlightDetails | NoFlightFound:\n    \"\"\"Procedural validation that the flight meets the constraints.\"\"\"\n    if isinstance(output, NoFlightFound):\n        return output\n\n    errors: list[str] = []\n    if output.origin != ctx.deps.req_origin:\n        errors.append(\n            f'Flight should have origin {ctx.deps.req_origin}, not {output.origin}'\n        )\n    if output.destination != ctx.deps.req_destination:\n        errors.append(\n            f'Flight should have destination {ctx.deps.req_destination}, not {output.destination}'\n        )\n    if output.date != ctx.deps.req_date:\n        errors.append(f'Flight should be on {ctx.deps.req_date}, not {output.date}')\n\n    if errors:\n        raise ModelRetry('\\n'.join(errors))\n    else:\n        return output\n\n\nclass SeatPreference(BaseModel):\n    row: int = Field(ge=1, le=30)\n    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']\n\n\nclass Failed(BaseModel):\n    \"\"\"Unable to extract a seat selection.\"\"\"",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "This agent is responsible for extracting flight details from web page text."
  },
  "0288bcd2bf7cf2e8f35a63deeb487e06a3551e936faa2f116ac648153004339f": {
    "text": "seat_preference_agent = Agent[None, SeatPreference | Failed](\n    'openai:gpt-4o',\n    output_type=SeatPreference | Failed,\n    system_prompt=(\n        \"Extract the user's seat preference. \"\n        'Seats A and F are window seats. '\n        'Row 1 is the front row and has extra leg room. '\n        'Rows 14, and 20 also have extra leg room. '\n    ),\n)",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "This agent is responsible for extracting the user's seat selection"
  },
  "cb3bdef1267b93cc4536364faae84736758ee875c6fea198e32834b6d4063c5a": {
    "text": "flights_web_page = \"\"\"\n1. Flight SFO-AK123\n- Price: $350\n- Origin: San Francisco International Airport (SFO)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 10, 2025\n\n2. Flight SFO-AK456\n- Price: $370\n- Origin: San Francisco International Airport (SFO)\n- Destination: Fairbanks International Airport (FAI)\n- Date: January 10, 2025\n\n3. Flight SFO-AK789\n- Price: $400\n- Origin: San Francisco International Airport (SFO)\n- Destination: Juneau International Airport (JNU)\n- Date: January 20, 2025\n\n4. Flight NYC-LA101\n- Price: $250\n- Origin: San Francisco International Airport (SFO)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 10, 2025\n\n5. Flight CHI-MIA202\n- Price: $200\n- Origin: Chicago O'Hare International Airport (ORD)\n- Destination: Miami International Airport (MIA)\n- Date: January 12, 2025\n\n6. Flight BOS-SEA303\n- Price: $120\n- Origin: Boston Logan International Airport (BOS)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 12, 2025\n\n7. Flight DFW-DEN404\n- Price: $150\n- Origin: Dallas/Fort Worth International Airport (DFW)\n- Destination: Denver International Airport (DEN)\n- Date: January 10, 2025\n\n8. Flight ATL-HOU505\n- Price: $180\n- Origin: Hartsfield-Jackson Atlanta International Airport (ATL)\n- Destination: George Bush Intercontinental Airport (IAH)\n- Date: January 10, 2025\n\"\"\"",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "potentially using another agent to navigate the site"
  },
  "5f5c40c9c68f72a90c6b2050aa0d255219be23a962844abc81e51064a333f9e1": {
    "text": "usage_limits = UsageLimits(request_limit=15)\n\n\nasync def main():\n    deps = Deps(\n        web_page_text=flights_web_page,\n        req_origin='SFO',\n        req_destination='ANC',\n        req_date=datetime.date(2025, 1, 10),\n    )\n    message_history: list[ModelMessage] | None = None\n    usage: RunUsage = RunUsage()\n    # run the agent until a satisfactory flight is found\n    while True:\n        result = await search_agent.run(\n            f'Find me a flight from {deps.req_origin} to {deps.req_destination} on {deps.req_date}',\n            deps=deps,\n            usage=usage,\n            message_history=message_history,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, NoFlightFound):\n            print('No flight found')\n            break\n        else:\n            flight = result.output\n            print(f'Flight found: {flight}')\n            answer = Prompt.ask(\n                'Do you want to buy this flight, or keep searching? (buy/*search)',\n                choices=['buy', 'search', ''],\n                show_choices=False,\n            )\n            if answer == 'buy':\n                seat = await find_seat(usage)\n                await buy_tickets(flight, seat)\n                break\n            else:\n                message_history = result.all_messages(\n                    output_tool_return_content='Please suggest another flight'\n                )\n\n\nasync def find_seat(usage: RunUsage) -> SeatPreference:\n    message_history: list[ModelMessage] | None = None\n    while True:\n        answer = Prompt.ask('What seat would you like?')\n\n        result = await seat_preference_agent.run(\n            answer,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, SeatPreference):\n            return result.output\n        else:\n            print('Could not understand seat preference. Please try again.')\n            message_history = result.all_messages()\n\n\nasync def buy_tickets(flight_details: FlightDetails, seat: SeatPreference):\n    print(f'Purchasing flight {flight_details=!r} {seat=!r}...')\n\n\nif __name__ == '__main__':\n    import asyncio\n\n    asyncio.run(main())\n```",
    "source_url": "https://ai.pydantic.dev/examples/flight-booking/",
    "header": "restrict how many requests this app can make to the LLM"
  },
  "6d68fd1a1d52e3a003330d1a334d75fe38e1fa55d71a4128a14067fea8c6ccbf": {
    "text": "RAG search example. This demo allows you to ask question of the [logfire](https://pydantic.dev/logfire \"https://pydantic.dev/logfire\") documentation.\n\nDemonstrates:\n\nThis is done by creating a database containing each section of the markdown documentation, then registering\nthe search tool with the Pydantic AI agent.\n\nLogic for extracting sections from markdown files and a JSON file with that data is available in\n[this gist](https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992 \"https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992\").\n\n[PostgreSQL with pgvector](https://github.com/pgvector/pgvector \"https://github.com/pgvector/pgvector\") is used as the search database, the easiest way to download and run pgvector is using Docker:\n\n```\nmkdir postgres-data\ndocker run --rm \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -p 54320:5432 \\\n  -v `pwd`/postgres-data:/var/lib/postgresql/data \\\n  pgvector/pgvector:pg17\n```\n\nAs with the [SQL gen](../sql-gen/ \"../sql-gen/\") example, we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running.\nWe also mount the PostgreSQL `data` directory locally to persist the data if you need to stop and restart the container.\n\nWith that running and [dependencies installed and environment variables set](../#usage \"../#usage\"), we can build the search database with (**WARNING**: this requires the `OPENAI_API_KEY` env variable and will calling the OpenAI embedding API around 300 times to generate embeddings for each section of the documentation):\n\n(Note building the database doesn't use Pydantic AI right now, instead it uses the OpenAI SDK directly.)\n\nYou can then ask the agent a question with:",
    "source_url": "https://ai.pydantic.dev/examples/rag/",
    "header": "RAG"
  },
  "68e252c9d89b60c73d566dbb037294f864c29ed766ffd189df0f44d5f438c30b": {
    "text": "[rag.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/rag.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/rag.py\")\n\n```\n\"\"\"RAG example with pydantic-ai — using vector search to augment a chat agent.\n\nRun pgvector with:\n\n    mkdir postgres-data\n    docker run --rm -e POSTGRES_PASSWORD=postgres \\\n        -p 54320:5432 \\\n        -v `pwd`/postgres-data:/var/lib/postgresql/data \\\n        pgvector/pgvector:pg17\n\nBuild the search DB with:\n\n    uv run -m pydantic_ai_examples.rag build\n\nAsk the agent a question with:\n\n    uv run -m pydantic_ai_examples.rag search \"How do I configure logfire to work with FastAPI?\"\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport re\nimport sys\nimport unicodedata\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\n\nimport asyncpg\nimport httpx\nimport logfire\nimport pydantic_core\nfrom openai import AsyncOpenAI\nfrom pydantic import TypeAdapter\nfrom typing_extensions import AsyncGenerator\n\nfrom pydantic_ai import Agent, RunContext",
    "source_url": "https://ai.pydantic.dev/examples/rag/",
    "header": "Example Code"
  },
  "80b748aaacfcede096fd860df05cdd251f0361eed454c7a523d9f955984b87ea": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_asyncpg()\nlogfire.instrument_pydantic_ai()\n\n\n@dataclass\nclass Deps:\n    openai: AsyncOpenAI\n    pool: asyncpg.Pool\n\n\nagent = Agent('openai:gpt-4o', deps_type=Deps)\n\n\n@agent.tool\nasync def retrieve(context: RunContext[Deps], search_query: str) -> str:\n    \"\"\"Retrieve documentation sections based on a search query.\n\n    Args:\n        context: The call context.\n        search_query: The search query.\n    \"\"\"\n    with logfire.span(\n        'create embedding for {search_query=}', search_query=search_query\n    ):\n        embedding = await context.deps.openai.embeddings.create(\n            input=search_query,\n            model='text-embedding-3-small',\n        )\n\n    assert len(embedding.data) == 1, (\n        f'Expected 1 embedding, got {len(embedding.data)}, doc query: {search_query!r}'\n    )\n    embedding = embedding.data[0].embedding\n    embedding_json = pydantic_core.to_json(embedding).decode()\n    rows = await context.deps.pool.fetch(\n        'SELECT url, title, content FROM doc_sections ORDER BY embedding <-> $1 LIMIT 8',\n        embedding_json,\n    )\n    return '\\n\\n'.join(\n        f'# {row[\"title\"]}\\nDocumentation URL:{row[\"url\"]}\\n\\n{row[\"content\"]}\\n'\n        for row in rows\n    )\n\n\nasync def run_agent(question: str):\n    \"\"\"Entry point to run the agent and perform RAG based question answering.\"\"\"\n    openai = AsyncOpenAI()\n    logfire.instrument_openai(openai)\n\n    logfire.info('Asking \"{question}\"', question=question)\n\n    async with database_connect(False) as pool:\n        deps = Deps(openai=openai, pool=pool)\n        answer = await agent.run(question, deps=deps)\n    print(answer.output)\n\n\n#######################################################",
    "source_url": "https://ai.pydantic.dev/examples/rag/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  },
  "c1bc0009f5c28898d9473778f6d2b5b9b5370a8dd865bfe4fd73380e09c3e2c8": {
    "text": "#######################################################",
    "source_url": "https://ai.pydantic.dev/examples/rag/",
    "header": "search database, and some utilities.                #"
  },
  "57acfe074117d46c1b087d1a4116af388ad77c2506b2224fb0619dd0b01c0e2a": {
    "text": "DOCS_JSON = (\n    'https://gist.githubusercontent.com/'\n    'samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992/raw/'\n    '80c5925c42f1442c24963aaf5eb1a324d47afe95/logfire_docs.json'\n)\n\n\nasync def build_search_db():\n    \"\"\"Build the search database.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(DOCS_JSON)\n        response.raise_for_status()\n    sections = sessions_ta.validate_json(response.content)\n\n    openai = AsyncOpenAI()\n    logfire.instrument_openai(openai)\n\n    async with database_connect(True) as pool:\n        with logfire.span('create schema'):\n            async with pool.acquire() as conn:\n                async with conn.transaction():\n                    await conn.execute(DB_SCHEMA)\n\n        sem = asyncio.Semaphore(10)\n        async with asyncio.TaskGroup() as tg:\n            for section in sections:\n                tg.create_task(insert_doc_section(sem, openai, pool, section))\n\n\nasync def insert_doc_section(\n    sem: asyncio.Semaphore,\n    openai: AsyncOpenAI,\n    pool: asyncpg.Pool,\n    section: DocsSection,\n) -> None:\n    async with sem:\n        url = section.url()\n        exists = await pool.fetchval('SELECT 1 FROM doc_sections WHERE url = $1', url)\n        if exists:\n            logfire.info('Skipping {url=}', url=url)\n            return\n\n        with logfire.span('create embedding for {url=}', url=url):\n            embedding = await openai.embeddings.create(\n                input=section.embedding_content(),\n                model='text-embedding-3-small',\n            )\n        assert len(embedding.data) == 1, (\n            f'Expected 1 embedding, got {len(embedding.data)}, doc section: {section}'\n        )\n        embedding = embedding.data[0].embedding\n        embedding_json = pydantic_core.to_json(embedding).decode()\n        await pool.execute(\n            'INSERT INTO doc_sections (url, title, content, embedding) VALUES ($1, $2, $3, $4)',\n            url,\n            section.title,\n            section.content,\n            embedding_json,\n        )\n\n\n@dataclass\nclass DocsSection:\n    id: int\n    parent: int | None\n    path: str\n    level: int\n    title: str\n    content: str\n\n    def url(self) -> str:\n        url_path = re.sub(r'\\.md$', '', self.path)\n        return (\n            f'https://logfire.pydantic.dev/docs/{url_path}/#{slugify(self.title, \"-\")}'\n        )\n\n    def embedding_content(self) -> str:\n        return '\\n\\n'.join((f'path: {self.path}', f'title: {self.title}', self.content))\n\n\nsessions_ta = TypeAdapter(list[DocsSection])",
    "source_url": "https://ai.pydantic.dev/examples/rag/",
    "header": "https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992"
  },
  "9d19a00fb40b85f9f7969b9929c4a87d47bef7085e730368a024d904994f3a5a": {
    "text": "@asynccontextmanager\nasync def database_connect(\n    create_db: bool = False,\n) -> AsyncGenerator[asyncpg.Pool, None]:\n    server_dsn, database = (\n        'postgresql://postgres:postgres@localhost:54320',\n        'pydantic_ai_rag',\n    )\n    if create_db:\n        with logfire.span('check and create DB'):\n            conn = await asyncpg.connect(server_dsn)\n            try:\n                db_exists = await conn.fetchval(\n                    'SELECT 1 FROM pg_database WHERE datname = $1', database\n                )\n                if not db_exists:\n                    await conn.execute(f'CREATE DATABASE {database}')\n            finally:\n                await conn.close()\n\n    pool = await asyncpg.create_pool(f'{server_dsn}/{database}')\n    try:\n        yield pool\n    finally:\n        await pool.close()\n\n\nDB_SCHEMA = \"\"\"\nCREATE EXTENSION IF NOT EXISTS vector;\n\nCREATE TABLE IF NOT EXISTS doc_sections (\n    id serial PRIMARY KEY,\n    url text NOT NULL UNIQUE,\n    title text NOT NULL,\n    content text NOT NULL,\n    -- text-embedding-3-small returns a vector of 1536 floats\n    embedding vector(1536) NOT NULL\n);\nCREATE INDEX IF NOT EXISTS idx_doc_sections_embedding ON doc_sections USING hnsw (embedding vector_l2_ops);\n\"\"\"\n\n\ndef slugify(value: str, separator: str, unicode: bool = False) -> str:\n    \"\"\"Slugify a string, to make it URL friendly.\"\"\"\n    # Taken unchanged from https://github.com/Python-Markdown/markdown/blob/3.7/markdown/extensions/toc.py#L38\n    if not unicode:\n        # Replace Extended Latin characters with ASCII, i.e. `žlutý` => `zluty`\n        value = unicodedata.normalize('NFKD', value)\n        value = value.encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value).strip().lower()\n    return re.sub(rf'[{separator}\\s]+', separator, value)\n\n\nif __name__ == '__main__':\n    action = sys.argv[1] if len(sys.argv) > 1 else None\n    if action == 'build':\n        asyncio.run(build_search_db())\n    elif action == 'search':\n        if len(sys.argv) == 3:\n            q = sys.argv[2]\n        else:\n            q = 'How do I configure logfire to work with FastAPI?'\n        asyncio.run(run_agent(q))\n    else:\n        print(\n            'uv run --extra examples -m pydantic_ai_examples.rag build|search',\n            file=sys.stderr,\n        )\n        sys.exit(1)\n```",
    "source_url": "https://ai.pydantic.dev/examples/rag/",
    "header": "pyright: reportUnknownVariableType=false"
  },
  "46f654aa1cfa131d812e69c4043f1c3de3a92fe7654c01be34fc52832ef9aafa": {
    "text": "This example shows how to stream markdown from an agent, using the [`rich`](https://github.com/Textualize/rich \"https://github.com/Textualize/rich\") library to highlight the output in the terminal.\n\nIt'll run the example with both OpenAI and Google Gemini models if the required environment variables are set.\n\nDemonstrates:",
    "source_url": "https://ai.pydantic.dev/examples/stream-markdown/",
    "header": "Stream markdown"
  },
  "919494d9a4a4960699f974b90db942e34376e7794c0f2b92e5087b9218480a8c": {
    "text": "With [dependencies installed and environment variables set](../#usage \"../#usage\"), run:",
    "source_url": "https://ai.pydantic.dev/examples/stream-markdown/",
    "header": "Running the Example"
  },
  "29d3499ca2f01fe9185bc9fdf3d8671cf3ff1ad83c17db7365a85eb682870abe": {
    "text": "[stream\\_markdown.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_markdown.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_markdown.py\")\n\n```\n\"\"\"This example shows how to stream markdown from an agent, using the `rich` library to display the markdown.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_markdown\n\"\"\"\n\nimport asyncio\nimport os\n\nimport logfire\nfrom rich.console import Console, ConsoleOptions, RenderResult\nfrom rich.live import Live\nfrom rich.markdown import CodeBlock, Markdown\nfrom rich.syntax import Syntax\nfrom rich.text import Text\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models import KnownModelName",
    "source_url": "https://ai.pydantic.dev/examples/stream-markdown/",
    "header": "Example Code"
  },
  "9b33ac222c4919a4c7d8ba0375ff9ad9e9bbd6b272234e91727fe117cf497b68": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nagent = Agent()",
    "source_url": "https://ai.pydantic.dev/examples/stream-markdown/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  },
  "dad7d88ce3eb6161f6f2f5216af859c510f4fac3d54e181f53d9ee967244be2a": {
    "text": "models: list[tuple[KnownModelName, str]] = [\n    ('google-gla:gemini-2.0-flash', 'GEMINI_API_KEY'),\n    ('openai:gpt-4o-mini', 'OPENAI_API_KEY'),\n    ('groq:llama-3.3-70b-versatile', 'GROQ_API_KEY'),\n]\n\n\nasync def main():\n    prettier_code_blocks()\n    console = Console()\n    prompt = 'Show me a short example of using Pydantic.'\n    console.log(f'Asking: {prompt}...', style='cyan')\n    for model, env_var in models:\n        if env_var in os.environ:\n            console.log(f'Using model: {model}')\n            with Live('', console=console, vertical_overflow='visible') as live:\n                async with agent.run_stream(prompt, model=model) as result:\n                    async for message in result.stream_output():\n                        live.update(Markdown(message))\n            console.log(result.usage())\n        else:\n            console.log(f'{model} requires {env_var} to be set.')\n\n\ndef prettier_code_blocks():\n    \"\"\"Make rich code blocks prettier and easier to copy.\n\n    From https://github.com/samuelcolvin/aicli/blob/v0.8.0/samuelcolvin_aicli.py#L22\n    \"\"\"\n\n    class SimpleCodeBlock(CodeBlock):\n        def __rich_console__(\n            self, console: Console, options: ConsoleOptions\n        ) -> RenderResult:\n            code = str(self.text).rstrip()\n            yield Text(self.lexer_name, style='dim')\n            yield Syntax(\n                code,\n                self.lexer_name,\n                theme=self.theme,\n                background_color='default',\n                word_wrap=True,\n            )\n            yield Text(f'/{self.lexer_name}', style='dim')\n\n    Markdown.elements['fence'] = SimpleCodeBlock\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```",
    "source_url": "https://ai.pydantic.dev/examples/stream-markdown/",
    "header": "models to try, and the appropriate env var"
  },
  "003ba7424da4ff054f29250009a9dcfb3f005f6d5200b04edf6300b25daf760b": {
    "text": "Information about whales — an example of streamed structured response validation.\n\nDemonstrates:\n\nThis script streams structured responses from GPT-4 about whales, validates the data\nand displays it as a dynamic table using [`rich`](https://github.com/Textualize/rich \"https://github.com/Textualize/rich\") as the data is received.",
    "source_url": "https://ai.pydantic.dev/examples/stream-whales/",
    "header": "Stream whales"
  },
  "56cb4c1e2cefc63997c812f697a81c567145d5227c365df93272a3f4b6cae18f": {
    "text": "With [dependencies installed and environment variables set](../#usage \"../#usage\"), run:\n\nShould give an output like this:",
    "source_url": "https://ai.pydantic.dev/examples/stream-whales/",
    "header": "Running the Example"
  },
  "9f1c5e4c44da8992b86fcdbc28947dab167a184d130878db31d498f308c5fbe9": {
    "text": "[stream\\_whales.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_whales.py \"https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_whales.py\")\n\n```\n\"\"\"Information about whales — an example of streamed structured response validation.\n\nThis script streams structured responses from GPT-4 about whales, validates the data\nand displays it as a dynamic table using Rich as the data is received.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_whales\n\"\"\"\n\nfrom typing import Annotated\n\nimport logfire\nfrom pydantic import Field\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.table import Table\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic_ai import Agent",
    "source_url": "https://ai.pydantic.dev/examples/stream-whales/",
    "header": "Example Code"
  },
  "be95456c42a5110a7ad510f79906096d08f5c3fbeab24217bd0f3b4c43d37132": {
    "text": "logfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass Whale(TypedDict):\n    name: str\n    length: Annotated[\n        float, Field(description='Average length of an adult whale in meters.')\n    ]\n    weight: NotRequired[\n        Annotated[\n            float,\n            Field(description='Average weight of an adult whale in kilograms.', ge=50),\n        ]\n    ]\n    ocean: NotRequired[str]\n    description: NotRequired[Annotated[str, Field(description='Short Description')]]\n\n\nagent = Agent('openai:gpt-4', output_type=list[Whale])\n\n\nasync def main():\n    console = Console()\n    with Live('\\n' * 36, console=console) as live:\n        console.print('Requesting data...', style='cyan')\n        async with agent.run_stream(\n            'Generate me details of 5 species of Whale.'\n        ) as result:\n            console.print('Response:', style='green')\n\n            async for whales in result.stream_output(debounce_by=0.01):\n                table = Table(\n                    title='Species of Whale',\n                    caption='Streaming Structured responses from GPT-4',\n                    width=120,\n                )\n                table.add_column('ID', justify='right')\n                table.add_column('Name')\n                table.add_column('Avg. Length (m)', justify='right')\n                table.add_column('Avg. Weight (kg)', justify='right')\n                table.add_column('Ocean')\n                table.add_column('Description', justify='right')\n\n                for wid, whale in enumerate(whales, start=1):\n                    table.add_row(\n                        str(wid),\n                        whale['name'],\n                        f'{whale[\"length\"]:0.0f}',\n                        f'{w:0.0f}' if (w := whale.get('weight')) else '…',\n                        whale.get('ocean') or '…',\n                        whale.get('description') or '…',\n                    )\n                live.update(table)\n\n\nif __name__ == '__main__':\n    import asyncio\n\n    asyncio.run(main())\n```",
    "source_url": "https://ai.pydantic.dev/examples/stream-whales/",
    "header": "'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured"
  }
}